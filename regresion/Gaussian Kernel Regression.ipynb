{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gaussian Kernel Regression.ipynb","provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyN7E8K1JW0v0hM5HJWx1x3x"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zSFv3radiW0Q"},"source":["# Gaussian Kernel Regression\n","\n","La regresión kernel gausiana se comporta de manera análoga al regresor mediante la técnica de los $k$ vecinos pero incluyendo las siguientes diferencias:\n","\n","1. La regresión kernel gausiana toma una media ponderada de las muestras cercanas en base a una distribución gausiana.\n","2. La regresión kernel usa todas las muestras del conjunto como vecinos ($k$ = número de muestras).\n","\n","Conceptualmente lo que hace la regresión kernel gausiana es determinar el peso de cada muestra del conjunto de datos ($x^\\prime$) en la predicción en función de su valor asociado a una distribución gausiana de media igual al punto a predecir ($x$) y desviación típica $\\alpha$. Dicho $\\alpha$ se identificará como un hiper-parámetro del regresor.\n","\n","Ilustremos esto gráficamente. Supongamos la siguiente distribución normal de media 0 y desviación típica 1:"]},{"cell_type":"code","metadata":{"id":"YVJ-6Atslref"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.stats as stats\n","import math\n","\n","mu = 0\n","variance = 1\n","sigma = math.sqrt(variance)\n","x = np.linspace(-3, +3, 100)\n","plt.plot(x, stats.norm.pdf(x, mu, sigma))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lrwi0r41lygb"},"source":["Si estamos prediciendo el valor para $x=0$ entonces los puntos cuyo valor $x$ sea cercano a 1 tendrán un peso en la predicción de aproximadamente 0.2, los puntos cuyo valor $x$ tendrán un peso de aproximadamente 0.05 y así sucesivamente.\n","\n","Nótese la enorme influencia del hiper-parámetro $\\alpha$ en este proceso. Si hacemos el valor de $\\alpha$ muy cercano a 0 obtenemos la siguiente distribución normal:"]},{"cell_type":"code","metadata":{"id":"ERTFP1YcmkhB"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.stats as stats\n","import math\n","\n","mu = 0\n","variance = 0.01\n","sigma = math.sqrt(variance)\n","x = np.linspace(-3, +3, 100)\n","plt.plot(x, stats.norm.pdf(x, mu, sigma))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlEBfb4amwbn"},"source":["Lo que provoca que únicamente los puntos muy cercanos a 0 tengan influencia sobre sus predicciones.\n","\n","Por el contrario, para valores grandes de $\\alpha$ obtenemos esta distribución normal:"]},{"cell_type":"code","metadata":{"id":"EaIpPgZRm9tl"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.stats as stats\n","import math\n","\n","mu = 0\n","variance = 10\n","sigma = math.sqrt(variance)\n","x = np.linspace(-3, +3, 100)\n","plt.plot(x, stats.norm.pdf(x, mu, sigma))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDSH0c-NnIpl"},"source":["Que atendiendo a la escala del eje *y* puede verse que todos los puntos tendrían, aproximadamente, la misma influencia en la predicción."]},{"cell_type":"markdown","metadata":{"id":"HRHvMQg8nR2z"},"source":["Este método de regresión se encuentra implementado en la clase [`sklearn.gaussian_process.GaussianProcessRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html) de la librería `sklearn`. Su constructor dispone del parámetro `alpha` cuya utilidad se ha descrito anteriormente.\n","\n","Veamos su funcionamiento sobre un conjunto de datos sintéticos."]},{"cell_type":"code","metadata":{"id":"8rYmEOYdiSve"},"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import random\n","\n","from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.gaussian_process.kernels import RBF\n","\n","y = []\n","X = np.array(list(range(80)))[:, np.newaxis]\n","for i in range(len(X)):\n","    y.append(20+X[i]+random.random()*50)\n","\n","kernel = RBF(10, (0.01, 1e2))\n","\n","fig, axs = plt.subplots(1,3, figsize=(17,3))\n","for i, alpha in zip([0,1,2], [0.01, 0.05, 0.08]):\n","    gp = GaussianProcessRegressor(alpha=alpha, kernel=kernel)\n","    gp.fit(X, y)\n","    y_pred, sigma = gp.predict(X, return_std=True)\n","\n","    axs[i].plot(X, y, 'y.', markersize=11, label='Samples')\n","    axs[i].plot(X, y_pred, 'k-', label='Prediction')\n","    axs[i].legend()\n","    axs[i].set_title('Alpha = ' + str(alpha))\n","    axs[i].grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqCAg_wdonB-"},"source":["Nótese que hemos necesitado también definir el parámetro `kernel` que indica la función de convarianzas empleada por el método. En este caso hemos usado [`sklearn.gaussian_process.kernels.RBF`](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html) y ajustado sus valores a los datos del ejemplo."]},{"cell_type":"markdown","metadata":{"id":"RFrSQQ_TQ9fw"},"source":["---\n","\n","Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]}]}