{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Tal y como hemos visto, la regresión lineal tiene un problema de escalabilidad derivado de cómo se calculan sus parámetros $\\beta$. El uso de operaciones matriciales para obtener la recta de regresión que minimiza los errores implica operaciones computacionalmente pesadas. El **descenso de gradiente estocástico**, o *Stochastic Gradient Descent (SGD)*, es un método de regresión (aunque puede aplicarse también a clasificación como veremos más adelante) que permite obtener resultados similares a los de la regresión lineal, pero de forma escalable.\n",
    "\n",
    "Se fundamenta en la técnica de **descenso de gradiente** un método de optimización que permite obtener los valores de los parámetros de una función que la hacen mínima. Recordamos que la regresión lineal podía plantearse como el siguiente problema de optimización (nótese el cambio de nomenclatura de $\\beta$ a $w$ para coincidir con el resto de literatura sobre el tema):\n",
    "\n",
    "$$\n",
    "\\min_w \\sum_{i=1}^n \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)^2\n",
    "$$\n",
    "\n",
    "Para poder optimizar la función mediante el descenso de gradiente debemos encontrar las derivadas parciales respecto de $w_0, w_1, \\dots, w_m$. Por simplificar, estas ecuaciones se han obtenido para la muestra $x_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_0} = - 2 \\cdot \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_1} = - 2 \\cdot x_{i,1} \\cdot \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_2} = - 2 \\cdot x_{i,2} \\cdot \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\cdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_m} = - 2 \\cdot x_{i,m} \\cdot \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)\n",
    "$$\n",
    "\n",
    "En general, para cualquier $k \\neq 0$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_k} = - 2 \\cdot x_{i,k} \\cdot \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)\n",
    "$$\n",
    "\n",
    "El algoritmo del descenso de gradiente recorrerá el conjunto de datos un número predeterminado de iteraciones y, para cada muestra $x_i$ procederá a actualizar los parametros $w$ de acuerdo con las siguientes experesiones:\n",
    "\n",
    "$$\n",
    "w_0 \\leftarrow w_0 + \\gamma \\cdot \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k + \\gamma \\cdot x_{i,k} \\cdot \\left( y_i - \\left( w_0 + w_1 \\cdot x_{i,1} + \\cdots + w_m \\cdot x_{i,m} \\right) \\right)\n",
    "$$\n",
    "\n",
    "Resaltar que se ha incluido un híper-parámetro $\\gamma$ conocido como *learning rate* o **factor de aprendizaje** que controla la velocidad del aprendizaje. Un valor alto en dicho parámetro implicará aprendizajes rápidos, pero poco precisos, mientras que un valor bajo implicará un aprendizaje lento y la posibilidad de quedar atrapado en un mínimo local.\n",
    "\n",
    "El método anterior puede ser también aplicado a la regresión línea con regularización (Ridge, Lasso o ElasticNet) añadiendo un coeficiente de regularización y actualizando derivadas y ecuaciones de actualización.\n",
    "\n",
    "Como hemos dicho, el descenso de gradiente estocástico funciona como un descenso de gradiente básico con la única salvedad de que en *SGD* se calcula dicho gradiente con un único punto de datos en lugar de hacerlo con todos los puntos del conjunto de datos. Es decir, en lugar de recorrer todo el conjunto de datos en cada iteración, el algoritmo elige un único punto y actualiza los parámetros $w$ como si ese punto representase a todo el conjunto de datos. Con esto conseguimos que, si bien el método necesita de más iteraciones para converger a un óptimo, el coste computacional de cada una de estas iteraciones es considerablemente inferior a una iteración del descenso de gradiente básico.\n",
    "\n",
    "Es por ello por lo que se recomienda este método de regresión lineal cuando se tienen conjunto de datos con muchas observaciones (generalmente más de 10k puntos de entrenamiento).\n",
    "\n",
    "La siguiente figura muestra el funcionamiento del algoritmo representado la función de coste, el valor del parámetro $w$ del modelo (se ha omitido $w_0$ para poder representarlo) y la recta de regresión vinculada. Las sucesivas filas de la imagen muestran las diferentes iteraciones del algoritmo en las que se selecciona una muestra aleatoria (punto amarillo) para realizar la actualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T12:49:58.965688Z",
     "start_time": "2021-10-21T12:49:50.656864Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from math import ceil, floor\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "# parámetros del experimento\n",
    "\n",
    "n_iters = 12\n",
    "initial_w = -200\n",
    "lr = 1\n",
    "\n",
    "# definimos el conjunto de datos y la función de perdida\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=20.0, random_state=42)\n",
    "\n",
    "loss_function = lambda w : np.sum(np.square(y - w * X[:,0]))\n",
    "\n",
    "ws = np.linspace(-250, 300)\n",
    "\n",
    "loss = np.array([])\n",
    "for w in ws:\n",
    "    loss = np.append(loss, loss_function(w))\n",
    "\n",
    "# pintamos la evolución del gradiente   \n",
    "\n",
    "ncols = 4\n",
    "nrows = 2 * ceil(n_iters / ncols)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(3*ncols, 3*nrows), constrained_layout=True)\n",
    "\n",
    "w = initial_w\n",
    "\n",
    "for i in range(n_iters):\n",
    "    \n",
    "    row = 2 * floor(i/ncols)\n",
    "    col = i % ncols\n",
    "    \n",
    "    # función de perdida con el w actual\n",
    "    \n",
    "    axs[row,col].set_title('loss function (iter ' + str(i+1) +')')\n",
    "    axs[row,col].set_xlabel('w')\n",
    "    axs[row,col].set_ylabel('loss(w)')\n",
    "    \n",
    "    axs[row,col].plot(ws, loss)\n",
    "    axs[row,col].scatter(w, loss_function(w), c='red', s=70)\n",
    "\n",
    "    # recta de regresión asociada\n",
    "\n",
    "    axs[row+1,col].set_title('SGD Regressor (iter ' + str(i+1) +')')\n",
    "    axs[row+1,col].set_xlabel('X')\n",
    "    axs[row+1,col].set_ylabel('y')\n",
    "    \n",
    "    min = np.amin(X)\n",
    "    max = np.amax(X)\n",
    "\n",
    "    diff = max-min\n",
    "\n",
    "    min = min - 0.1 * diff\n",
    "    max = max + 0.1 * diff\n",
    "\n",
    "    axs[row+1,col].set_xlim(min, max)\n",
    "\n",
    "    axs[row+1,col].scatter(X, y)\n",
    "\n",
    "    line = np.linspace(min, max)\n",
    "    axs[row+1,col].plot(line, line * w, c='red')\n",
    "    \n",
    "    # actualizacion de w\n",
    "    \n",
    "    sample = np.random.randint(0, len(X))\n",
    "    gradient = X[sample,0] * (y[sample] - w * X[sample,0])\n",
    "    w = w + lr * gradient\n",
    "    \n",
    "    axs[row+1,col].scatter(X[sample,0], y[sample], c='yellow', s=70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, este método es iterativo y podemos ilustrar el proceso de aprendizaje de la regresión sobre el siguiente conjunto de puntos haciendo uso del modelo `linear_model.SGDRegressor` de la librería `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T12:49:59.136174Z",
     "start_time": "2021-10-21T12:49:58.973223Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 769,
     "status": "ok",
     "timestamp": 1603660529848,
     "user": {
      "displayName": "Fernando Ortega Requena",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64",
      "userId": "02003917424124170753"
     },
     "user_tz": -60
    },
    "id": "yDwAaVsRt5wy",
    "outputId": "87391da9-c79e-4249-80da-5643552d7a99"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=20.0, random_state=42)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T12:50:01.037028Z",
     "start_time": "2021-10-21T12:49:59.138382Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "executionInfo": {
     "elapsed": 3219,
     "status": "ok",
     "timestamp": 1603661685487,
     "user": {
      "displayName": "Fernando Ortega Requena",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64",
      "userId": "02003917424124170753"
     },
     "user_tz": -60
    },
    "id": "Inzi_f0enc29",
    "outputId": "ba76f298-7662-401d-a496-1a17fb1eb7b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from math import floor\n",
    "\n",
    "def plot_regression(X, y, axs, max_iter):\n",
    "  row = floor((max_iter-1) / 3)\n",
    "  col = (max_iter-1) % 3\n",
    "\n",
    "  min = np.amin(X)\n",
    "  max = np.amax(X)\n",
    "\n",
    "  diff = max-min\n",
    "\n",
    "  min = min - 0.1 * diff\n",
    "  max = max + 0.1 * diff\n",
    "\n",
    "  reg = SGDRegressor(max_iter=max_iter, random_state=43)\n",
    "  reg.fit(X, y)\n",
    "\n",
    "  axs[row,col].set_title('SGD iteración ' + str(max_iter))\n",
    "  axs[row,col].set_xlabel('x')\n",
    "  axs[row,col].set_ylabel('y')\n",
    "\n",
    "  axs[row,col].set_xlim(min, max)\n",
    "\n",
    "  axs[row,col].scatter(X,y)\n",
    "\n",
    "  line = np.linspace(min, max)\n",
    "  axs[row,col].plot(line, reg.intercept_ + line * reg.coef_, c='red')\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12,9), constrained_layout=True)\n",
    "\n",
    "for max_iter in range(1,10):\n",
    "  plot_regression(X, y, axs, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `SGDRegressor` también es compatible con la regresión polinómica al poder hacer uso de `PolynomialFeatures` como paso previo al cómputo de la regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6L3RKblrCIC"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es) y **Raúl Lara Cabrera** (raul.lara@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKsW9KBa+JkvVvYBCBdyW0",
   "name": "SGD Regressor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
