{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrI8DaVhYuqD"
   },
   "source": [
    "# k-Nearest Neighbors Regressor\n",
    "\n",
    "El regresor mediante la técnica de los $k$ vecinos realiza un sencillo e intuitivo planteamiento para resolver el problema de la regresión. Su idea más básica consiste en determinar el valor $y$ asociado a una muestra $X$ en función de los valores $y$ de las $k$ muestras $X^\\prime$ más cercanas a $X$ (*i.e.* sus $k$ vecinos).\n",
    "\n",
    "Ilustremos esto de forma gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2txagENYp9Q"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Create the samples\n",
    "X = list(range(40))\n",
    "y = []\n",
    "for i in range(len(X)):\n",
    "    y.append(20+X[i]+random.random()*20)\n",
    "\n",
    "# linear regression evolution \n",
    "fig, axs = plt.subplots(1,4, figsize=(15,4))\n",
    "\n",
    "# return the prediction of x data, by using k neighbours\n",
    "def KNN_prediction(k, X, y, x, figure):\n",
    "    distance = []\n",
    "    for i in range(len(X)):  \n",
    "        distance.append((X[i],y[i],(x-X[i])**2))  # create pairs (X[i], y[i], distance from x to X[i]) \n",
    "  # sorts distance vector attending to the set of distances from x to X[i]\n",
    "    sorted_list = sort(distance)  \n",
    "    \n",
    "    x_neighbourhood, y_neighbourhood = [], []; \n",
    "    y_prediction = 0.0\n",
    "    for i in range(k):  # k-neighbours\n",
    "        y_prediction += sorted_list[i][1]\n",
    "        x_neighbourhood.append(sorted_list[i][0])\n",
    "        y_neighbourhood.append(sorted_list[i][1])\n",
    "    y_prediction /= k    \n",
    "\n",
    "    plot(figure,X,y,'y.', \"samples\")    # plot samples\n",
    "    # plot the k neighbours     \n",
    "    plot(figure,x_neighbourhood,y_neighbourhood,'k.', \"neighbourhood\") \n",
    "    # plot the (x,y_prediction) prediction   \n",
    "    plot(figure,x,y_prediction,\"k+\", 'prediction')\n",
    "    axs[figure].set_xlabel('{:1.0f}'.format(k) + ' neighbors')\n",
    "    return y_prediction\n",
    "\n",
    "def sort(unsorted_list):\n",
    "    return (sorted(unsorted_list, key = lambda x: x[2]))\n",
    "\n",
    "def plot(fig, X, y, parameters, label):\n",
    "    axs[fig].plot(X, y, parameters, label = label)\n",
    "    axs[fig].legend(); axs[fig].grid(); \n",
    "    return\n",
    "\n",
    "KNN_prediction(2,X, y, 7.6, 0)\n",
    "KNN_prediction(4,X, y, 20.5, 1)\n",
    "KNN_prediction(7,X, y, 36.2, 2)\n",
    "KNN_prediction(15,X, y, 24.3, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4EbJrAzalLq"
   },
   "source": [
    "Podemos observar como la predicción (eje *y*) se realiza mediante el valor medio de los $k$ valores (eje *x*) más cercanos al punto dado (representado mediante una cruz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uA6Z2puka-QF"
   },
   "source": [
    "Este sencillo ejemplo muestra el funcionamiento del método de forma gráfica para una sola variable ($x$). Conceptualmente, podemos realizar el mismo procedimiento cuando se disponga de múltiples variables de entrada ($x_1. \\cdots, x_n$), aunque no sea posible representarlo de manera gráfica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxJZvLtlbfAE"
   },
   "source": [
    "Este método de regresión se encuentra implementado en la clase [`sklearn.neighbors.KNeighborsRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) de la librería `sklearn`. Su constructor dispone dos parámetros que debemos definir:\n",
    "\n",
    "- `n_neighbors` indica el número de vecinos a utilizar para ponderar el valor de la predicción.\n",
    "- `weights` define la influencia de cada uno de los vecinos en el valor de la predicción. Si se establece como `\"uniform\"` (por defecto) todos los vecinos tienen el mismo peso en la predicción, mientras que si se establece como `\"distance\"` cada vecino tiene un peso inversamente proporcional a su distancia (más cercanos mayor y menos cercanos menor) en la predicción.\n",
    "\n",
    "El constructor dispone de otro parámetro interesante que es `metric` el cual permite establecer cómo se determina la distancia entre dos puntos. Por defecto, se emplea la distancia euclídea, pero esto puede no ser conveniente en algunos conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afIEJeVdeIJ1"
   },
   "source": [
    "El siguiente ejemplo muestra el funcionamiento del regresor sobre un conjunto de datos sintéticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZNOoyLUeSTU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "X = np.array(list(range(100)))[:, np.newaxis]\n",
    "y = []\n",
    "for i in range(len(X)):\n",
    "    y.append(20+X[i]+random.random()*60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "X_test.sort(0)\n",
    "\n",
    "fig, axs = plt.subplots(4,2, figsize=(15,14))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "for i,n_neighbors in zip([0,1,2,3], [2,10,18,26]):\n",
    "    knn_regressor = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "    knn_regressor.fit(X_train, y_train)\n",
    "    y_hat = knn_regressor.predict(X_test)\n",
    "    \n",
    "    axs[i, 0].scatter(X, y, c='y')\n",
    "    axs[i, 0].plot(X_test, y_hat, c='k')\n",
    "    axs[i, 0].set_title('SciKit KNeighborsRegressor, k = '+  str(n_neighbors) + ', weights=\"uniform\"' )\n",
    "    \n",
    "    knn_regressor_dist = neighbors.KNeighborsRegressor(n_neighbors, 'distance')\n",
    "    knn_regressor_dist.fit(X_train, y_train)\n",
    "    y_hat = knn_regressor_dist.predict(X_test)\n",
    "    \n",
    "    axs[i, 1].scatter(X, y, c='y')\n",
    "    axs[i, 1].plot(X_test, y_hat, c='k')\n",
    "    axs[i, 1].set_title('SciKit KNeighborsRegressor, k = '+  str(n_neighbors) + ', weights=\"distance\"' )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyeodA6iRBeX"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMrjCDwj2ToPrM4YL7TrGgS",
   "collapsed_sections": [],
   "name": "k-Nearest Neighbors Regressor.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
