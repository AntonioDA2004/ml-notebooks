{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Regresión lineal\n",
    "\n",
    "La regresión es uno de los modelos de aprendizaje más supervisados que existen en el ámbito del *machine learning*. Su objetivo principal es estimar un valor continuo a partir de un vector de características. Aunque a priori hablar de \"*modelos de aprendizaje automático para regresión lineal*\" pueda parecer un concepto vanguardista e innovador, no lo es, puesto que en las primeras etapas educativas ya se enseña a hacer una regresión simple. Ilustremos esto con un ejemplo.\n",
    "\n",
    "Supongamos que tenemos los siguientes dos puntos en el espacio:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"kYsFSCSd9Rwr3iC9g5CQKl",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"TJkZcnCc4VOru83Q4o2MXB"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 3])\n",
    "y = np.array([1, 2])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 3)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x, y, s=100)"
   ],
   "execution_count":1,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"HIiVUzNtpEBeBnBh4PF02Q",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"SOoBhdsXeKAtt6B8nGFii4"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Datos los puntos $p_1=(1,1)$ y $p_2=(3,2)$ la ecuación de la recta que pasa por ellos viene definida por:\n",
    "\n",
    "$$\n",
    "\\frac{x - 1}{3 - 1} = \\frac{y - 1}{2 - 1}\n",
    "$$\n",
    "\n",
    "Lo que despejando deja:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{2} \\cdot x + \\frac{1}{2} = 0.5 \\cdot x + 0.5\n",
    "$$"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"HsVkgFH53qM6wTWbIjatZ3",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"lwI6ehn2y9K8aIY6oxsLq3"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 3)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# points\n",
    "x = np.array([1, 3])\n",
    "y = np.array([1, 2])\n",
    "plt.scatter(x, y, s=100, zorder=10)\n",
    "\n",
    "# line\n",
    "line = np.linspace(0, 4)\n",
    "plt.plot(line, 0.5 * line + 0.5, c='red')"
   ],
   "execution_count":2,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"amvrVZ4tppNmFoWbjPsIDm",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"S7DnZsR3f4dfzt218dhZX8"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Esta recta es lo que denominamos **recta de regresión** y nos permite, conocido un valor de *x* determinar el valor real de *y*. Por ejemplo, sabemos que para $x=2$ el valor de $y=0.5\\cdot2+0.5=1.5$. Dicho con otras palabras, estamos estimando el valor *y* a partir de *x*.\n",
    "\n",
    "Si trasladamos este concepto al ámbito del *machine learning* diremos que hemos aprendido la ecuación de la recta, o lo que es lo mismo, el **modelo de regresión**, a partir de los puntos de un cojunto de datos (en el ejemplo anterior los puntos $p_1=(1,1)$ y $p_2=(3,2)$). El proceso de aprendizaje ha consistido en estimar los párametros de la ecuación de la recta en función de los valores de los puntos que conforman el modelo de datos. Recordamos que la ecuación general de la recta tiene la forma:\n",
    "\n",
    "$$\n",
    "y = m \\cdot x + b\n",
    "$$\n",
    "\n",
    "Que comparándolo con el modelo aprendido $y = 0.5 \\cdot x + 0.5$ nos indica que se ha aprendido que para estimar correctamente los puntos $p_1=(1,1)$ y $p_2=(3,2)$ la pendiente de la recta debe ser $m=0.5$ y la interescción (o *bias*) debe ser $b=0.5$.\n",
    "\n",
    "El ejemplo anterior tiene fines didácticos, trabajar en el mundo del *machine learning* con conjuntos de datos implica una dimensionalidad mayor: habrá muchos más puntos (filas del conjunto de datos) y características (columnas del conjunto de datos). Por ello, los modelos de regresión se suelen generalizar del siguiente modo:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_m \\cdot X_m + \\epsilon\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\beta_0$ se corresponde con el *bias* de la recta.\n",
    "- $\\beta_1, \\dots, \\beta_m$ son los parámetros de la pendiente de la recta en las diferentes dimensiones\n",
    "- $\\epsilon$ representa el error que cometemos al no poder aproximar perfectamente todos los puntos. Nótese que cuando tenemos más de 2 puntos es prácticamente imposible encontrar una **recta** que pase por todos ellos.\n",
    "\n",
    "Podemos escribir la expresión anterior usando una notación más compacta de matrices:\n",
    "\n",
    "$$\n",
    "Y = X \\cdot \\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Siendo:\n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\n",
    "\\end{equation} \n",
    "\n",
    "La matriz que contiene los valores de la variable objetivo (i.e. la variable que queremos predecir) para la *n* muestras del conjunto de datos.\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\begin{pmatrix} 1 & x_{1,1} & \\cdots & x_{1,m} \\\\ 1 & x_{2,1} & \\cdots & x_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,m} \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "La matriz que contiene los valores de las *m* carácterísticas ($x_{i,j}$) del conjunto de datos.\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_m \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "La matriz de coeficientes del modelo de regresión **que se van a aprender**.\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "La matriz con los residuos.\n",
    "\n",
    "A partir de estas matrices podemos obtener un sistema de ecuaciones formado por *n* ecuaciones y *m* incógnitas. Normalmente, en *machine learning* disponemos de muchas más muestras que características, por lo que $n >> m$ y el sistema es *sobredimensionado* y la solución exacta no existe (tenemos infinitas soluciones posibles). Utilizando el método de los [mínimos cuadrados ordinarios](https:\/\/es.wikipedia.org\/wiki\/M%C3%ADnimos_cuadrados_ordinarios) podemos buscar la solución que obtenga el mejor $\\beta$ y minimize $|\\epsilon|^2$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial |\\epsilon|^2}{\\partial \\beta} &= \\frac{\\partial |Y-X\\beta|^2}{\\partial \\beta} \\\\\n",
    "&= \\frac{\\partial \\left[ (Y-X\\beta)^t (Y-X\\beta) \\right] }{\\partial \\beta} \\\\\n",
    "&= \\frac{\\partial \\left[ Y^tY - \\beta^tX^tY - Y^tX\\beta + \\beta^tX^tX\\beta \\right] }{\\partial \\beta} \\\\\n",
    "&= - 2 X^t Y + 2 X^t X \\beta = 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Por tanto, podemos *aprender* los valores de los coeficientes que minimizan los residuos a partir de los datos del siguiente modo:\n",
    "\n",
    "$$\n",
    "\\beta = \\left[ \\left( X^t X \\right)^{-1} X^t \\right] Y\n",
    "$$\n",
    "\n",
    "Gráficamente tenemos que dados los siguientes puntos:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"HytRutf1zkP7zYbMjiYUwy",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"sK5ciq4PlvRcwyFgd9PkZx"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=50, n_features=1, n_informative=1, n_targets=1, noise=5, random_state=43)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(X[:,0], y)"
   ],
   "execution_count":3,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"FXAe3T424pJGBxigUjhn3P",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"TNa6WrnrHCS1XJUViWTOKg"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Podemos calcular $\\beta$:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"IDDCivV7iuDQLicQbP5FXz",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"9g8osZfAXgtPT6BVHaOoUP"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "n,m = X.shape\n",
    "x0 = np.ones((n,1))\n",
    "stack = np.hstack((x0, X))\n",
    "\n",
    "# beta = ((X^t * X)^-1 * X^t) * y\n",
    "beta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(stack), stack)), np.transpose(stack)), y)\n",
    "beta"
   ],
   "execution_count":4,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"vmYIzIpoEZMXm8wibKDnkH",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"tKGrsbQlTbmZZSTU9wNobK"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Y pintar la recta de regresión del modelo de acuerdo con la ecuación:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 \\cdot x_1\n",
    "$$"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"zEBjgLjQEAZOwPFbqxXOTo",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"mjOq8pE82eyDVNrWl5GL5X"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(X[:,0], y, zorder=10)\n",
    "\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, beta[0] + beta[1] * line, c='red')"
   ],
   "execution_count":5,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"XhIDaQ9rqIEnww6zpFkbkq",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"i4t3NkMQsVcJuT8N0uK85O"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Estos mismo cálculos podemos hacerlo, de forma más simple, usando el modelo `linear_model.LinearRegression` de `sklearn`:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"jgSSBP0ETznOthrWEnDRiS",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"peRddSKETalafbpJGtKQE7"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)"
   ],
   "execution_count":6,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"6UkHnFy9RgeIOVJSJ8byat",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"xCsLG8QQ4QgkVkSfph0SvM"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Es posible acceder a los **parámetros** del modelo (i.e. los $\\beta$) mediante los atributos `coef_` ($\\beta_1$) e `intercept_` ($\\beta_0$):"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"RhxbDMkpNm0KOj71elq19x",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"JbLAsjrkQMxL7zchnGeEzL"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "reg.coef_"
   ],
   "execution_count":7,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"lHyxJrppOIzFJC3cqYUTJI",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"tGVfoHnGTkEUk956wStzJg"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "reg.intercept_"
   ],
   "execution_count":8,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"rNHraYVwW15SN5W9wQz9x3",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"stcOShxuw7eb53SQFgc72W"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Interpretación de la regresión lineal\n",
    "\n",
    "Entendido el modelo de regresión lineal debemos indagar en el significado de los parámetros $\\beta$ aprendidos por el modelo. Para ello, en lugar de usar un modelo tan sencillo, vamos a utilizar el conjunto de datos de [*california_housing*](https:\/\/scikit-learn.org\/stable\/datasets\/real_world.html#california-housing-dataset). En este conjunto de datos la variable objetivo representa el valor medio de las viviendas en los distritos de California, expresado en cientos de miles de dólares ($100.000).\n",
    "\n",
    "Este conjunto de datos proviene del censo de EE. UU. de 1990 y utiliza una fila por cada grupo de bloques censales. Un grupo de bloques es la unidad geográfica más pequeña para la cual la Oficina del Censo de EE. UU. publica datos muestrales, y suele albergar entre 600 y 3.000 personas.\n",
    "\n",
    "Las columnas que contiene son:\n",
    "\n",
    "- **MedInc**: ingreso medio en el grupo de bloques  \n",
    "- **HouseAge**: antigüedad media de las viviendas en el grupo de bloques  \n",
    "- **AveRooms**: número promedio de habitaciones por hogar  \n",
    "- **AveBedrms**: número promedio de dormitorios por hogar  \n",
    "- **Population**: población del grupo de bloques  \n",
    "- **AveOccup**: número promedio de miembros por hogar  \n",
    "- **Latitude**: latitud del grupo de bloques  \n",
    "- **Longitude**: longitud del grupo de bloques\n",
    "\n",
    "Se considera hogar a un grupo de personas que viven en una misma vivienda. Dado que el número promedio de habitaciones y dormitorios en este conjunto de datos se proporciona por hogar, estos valores pueden ser sorprendentemente altos en grupos de bloques con pocos hogares y muchas viviendas desocupadas, como ocurre en zonas turísticas."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"ug5niZXMPOLsgOuoLteWUe",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"CXr2wlJtkyQfkbicIORVu5"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.datasets import fetch_california_housing\n",
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target"
   ],
   "execution_count":10,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"yUN9AJhiiEGEswJLbb8Ju9",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"Ic45Cox1WP3N0zkMoMcIpV"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Aplicamos regresión lineal al conjunto de datos:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"N3nkAoqjwOFdZ2s7x7OWof",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"8HnZiFrfAf7SOerLkVyMyG"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)"
   ],
   "execution_count":11,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"OKYeUyPqq5EkJtn40NWD4W",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"L4d1kgiCbkuWh9T4YrpWL0"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Analizamos el parámetro `intercept_`:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Vet5y1IquMOcPKnr8J2krb",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"cN5Sz0OgZ0dtYP2QPbRSrz"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "reg.intercept_"
   ],
   "execution_count":12,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"ssPHWa9Cjj7ZsvQ4J3lH0X",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"jgbVj7JDiOAwRLG6RSOuv0"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Su valor es -36.94192020718439 que representa el precio medio básico de cualquier vivienda sin contar con el resto de las características que describan la vivienda (al ser un valor negativo, las viviendas sin más datos no valen nada).\n",
    "\n",
    "Ahora analizamos los coeficientes del modelo:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Vrt7WxizdkOSIrmpzeroY9",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"LSbcTIgrRytKPwC7xRGeQb"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "reg.coef_"
   ],
   "execution_count":13,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"fbaYuqKcanoTgbdC0aR6Ac",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"YWZ7Pwdli7xCkZg0rawSRC"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Y para ello vinculamos cada coeficiente a la característica (*feature*) a la que están representado:"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"vdVD1o7OkvCWE9Jgnd641L",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"w8Gaq9i2zYPqInix7kGAAF"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "pd.DataFrame(data=np.expand_dims(reg.coef_, axis=0), columns=california.feature_names)"
   ],
   "execution_count":14,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"2QNosFGXK8f0bp3jaC4INo",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"OeAOHRXvdmzyxV7gjHeyZ1"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Como norma general:\n",
    "\n",
    "- Los valores positivos en estos coeficientes significan una correlación directa entre los valores de las características que representan los coeficientes y el precio de la vivienda, mientras que los valores negativos representan una correlación inversa.\n",
    "- La magnitud de los coeficientes miden el grado de aportación, positiva o negativa, de una determinada característica al precio de la vivienda.\n",
    "\n",
    "Por poner algunos ejemplos:\n",
    "\n",
    "- El valor de la vivienda se incrementa cuando los ingresos de la zona son más altos.\n",
    "- El número de habitaciones tiene una influencia directa positiva en el precio de la vivienda.\n",
    "- Mayor densidad de población implica menor precio de la vivienda."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"CWLap1NjgRNaLPofgSJOLj",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"Kzn5QLv5zN3uNkdyVoZffC"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Problemas de la regresión lineal\n",
    "\n",
    "La regresión linea presenta principalmente 3 problemas: sobreajuste, escalabilidad y linealidad.\n",
    "\n",
    "### Sobreajuste\n",
    "\n",
    "El modelo de regresión lineal tiene un problema sobreajuste (en inglés *overfitting*) puesto que no es capaz de generalizar bien la línea de regresión en determinados contextos. Por ejemplo, cuando nuestro conjunto de datos contiene datos atípicos (*outlayers*), es decir, muestras que no siguen el patrón predominante en el conjunto de datos, la regresión lineal tiende a desviarse de su ideal prestando excesiva atención a estos datos atípicos.\n",
    "\n",
    "El siguiente ejemplo muestra, de forma gráfica, este proceso. Observamos nuestro conjunto de datos (puntos azules) y los *outlayers* (puntos naranjas). En verde aparece la regresión que se produciría si no existieran *outlayers* y en rojo la que se obtiene con los *outlayers*. Como vemos, unas pocas muestras han desviado significativamente la regresión provocando errores de predicción."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"483UOLkPs2zshj1MGYzbsn",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"Y4JdA1VI2ryQXol9cwTESF"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# conjunto de datos\n",
    "n_samples = 20\n",
    "n_outlayers = 2\n",
    "\n",
    "X, y = make_regression(n_samples=n_samples, n_features=1, n_informative=1, n_targets=1, noise=5, random_state=43)\n",
    "\n",
    "X_outlayers, y_outlayers = make_regression(n_samples=n_outlayers, n_features=1, n_informative=1, n_targets=1, noise=5, bias=100, random_state=43)\n",
    "\n",
    "X = np.vstack((X, X_outlayers))\n",
    "y = np.hstack((y, y_outlayers))\n",
    "\n",
    "# configuración del gráfico\n",
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# muestras\n",
    "plt.scatter(X[:n_samples,0], y[:n_samples], zorder=10, label=\"muestras\")\n",
    "plt.scatter(X[n_samples:,0], y[n_samples:], zorder=10, label=\"outlayers\")\n",
    "\n",
    "# regresion sin outlayers\n",
    "reg = LinearRegression().fit(X[:n_samples,:], y[:n_samples])\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, reg.intercept_ + reg.coef_[0] * line, c='green', label=\"sin outlayers\")\n",
    "\n",
    "# regresion con outlayers\n",
    "reg_outlayers = LinearRegression().fit(X, y)\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, reg_outlayers.intercept_ + reg_outlayers.coef_[0] * line, c='red', label=\"con outlayers\")\n",
    "\n",
    "# mostrar el gráfico\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count":15,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"l7CS5sleU4C5hcGQIqUJbb",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"iDsFZ62EfeZaHDV3SwFcfu"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Escalabilidad\n",
    "\n",
    "Como ya hemos comentado, obtener los parámetros de la regresión lineal implica las siguientes operaciones matriciales $\\beta = \\left[ \\left( X^t X \\right)^{-1} X^t \\right] Y$. Es decir, para poder calcular los parámetros del modelo requerimos hacer 3 multiplicaciones de matrices y una matriz inversa. Estas operaciones matemáticas, en su implementación más eficiente (*Optimized CW-like algorithms*) tienen un coste computacional de $O(n^{2.373})$. Por ello, el rendimiento del algoritmo de regresión lineal se ve profundamente mermado con conjuntos de datos grandes lo que hace que el modelo no sea escalable."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"gez7ec7ENjyaPTEgJwMNxL",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"JibH4rpepIYZx0c7buTC12"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Linealidad\n",
    "\n",
    "La regresión lineal, como su propio nombre indica, asume linealidad en la correlación entre la variable objetivo y las características del conjunto de datos. Sin embargo, en los problemas del mundo real, esta linealidad raramente se cumple, lo que provoca que el modelo no estime bien debido al *underfitting*. \n",
    "\n",
    "El siguiente ejemplo muestra una relación no lineal entre la variable objetivo *y* y la característica *x*. Concretamente $y = 3 - 2x + x^3$ (con ruido añadido). Observamos que el modelo (línea roja) no se ajusta nada bien a los datos."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"DXgGoLx8GJMcuPJx2t8n1D",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"SjFNhb9tGvxf89llVkoNSu"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "X = np.expand_dims(np.linspace(-2, 2, n_samples), axis=1) + np.random.rand(n_samples,1)\n",
    "y = 3 - 2 * X + X * X * X + np.random.rand(n_samples,1)\n",
    "\n",
    "# configuración del gráfico\n",
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, reg.intercept_ + reg.coef_[0] * line, c='red')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count":16,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"lt5oZS6Qmsw1GvAIrIlIae",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"Up3wd0XYiTxHGUhvoBwjbB"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es).\n",
    "\n",
    "<img src=\"https:\/\/licensebuttons.net\/l\/by-nc-sa\/3.0\/88x31.png\">"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"u323Dn6tAApTWFZ7n7efw5",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"ELd9Q4uq7EzPbfgo1hQjYl"
     }
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[],
   "report_row_ids":[],
   "report_tabs":[
    {
     "id":"A1HgkXUmilwv56AR6UbM3R",
     "name":"Report tab",
     "rows":[
      "TJkZcnCc4VOru83Q4o2MXB",
      "SOoBhdsXeKAtt6B8nGFii4",
      "lwI6ehn2y9K8aIY6oxsLq3",
      "S7DnZsR3f4dfzt218dhZX8",
      "sK5ciq4PlvRcwyFgd9PkZx",
      "TNa6WrnrHCS1XJUViWTOKg",
      "9g8osZfAXgtPT6BVHaOoUP",
      "tKGrsbQlTbmZZSTU9wNobK",
      "mjOq8pE82eyDVNrWl5GL5X",
      "i4t3NkMQsVcJuT8N0uK85O",
      "peRddSKETalafbpJGtKQE7",
      "xCsLG8QQ4QgkVkSfph0SvM",
      "JbLAsjrkQMxL7zchnGeEzL",
      "tGVfoHnGTkEUk956wStzJg",
      "stcOShxuw7eb53SQFgc72W",
      "CXr2wlJtkyQfkbicIORVu5",
      "Ic45Cox1WP3N0zkMoMcIpV",
      "8HnZiFrfAf7SOerLkVyMyG",
      "L4d1kgiCbkuWh9T4YrpWL0",
      "cN5Sz0OgZ0dtYP2QPbRSrz",
      "jgbVj7JDiOAwRLG6RSOuv0",
      "LSbcTIgrRytKPwC7xRGeQb",
      "YWZ7Pwdli7xCkZg0rawSRC",
      "w8Gaq9i2zYPqInix7kGAAF",
      "OeAOHRXvdmzyxV7gjHeyZ1",
      "Kzn5QLv5zN3uNkdyVoZffC",
      "Y4JdA1VI2ryQXol9cwTESF",
      "iDsFZ62EfeZaHDV3SwFcfu",
      "JibH4rpepIYZx0c7buTC12",
      "SjFNhb9tGvxf89llVkoNSu",
      "Up3wd0XYiTxHGUhvoBwjbB",
      "ELd9Q4uq7EzPbfgo1hQjYl"
     ]
    }
   ],
   "version":4
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}