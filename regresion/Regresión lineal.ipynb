{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal\n",
    "\n",
    "La regresión es uno de los modelos de aprendizaje más supervisados que existen en el ámbito del *machine learning*. Su objetivo principal es estimar un valor continuo a partir de un vector de características. Aunque a priori hablar de \"*modelos de aprendizaje automático para regresión lineal*\" pueda parecer un concepto vanguardista e innovador, no lo es, puesto que en las primeras etapas educativas ya se enseña a hacer una regresión simple. Ilustremos esto con un ejemplo.\n",
    "\n",
    "Supongamos que tenemos los siguientes dos puntos en el espacio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:47.953119Z",
     "start_time": "2021-04-14T10:23:47.766011Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 3])\n",
    "y = np.array([1, 2])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 3)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x, y, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos los puntos $p_1=(1,1)$ y $p_2=(3,2)$ la ecuación de la recta que pasa por ellos viene definida por:\n",
    "\n",
    "$$\n",
    "\\frac{x - 1}{3 - 1} = \\frac{y - 1}{2 - 1}\n",
    "$$\n",
    "\n",
    "Lo que despejando deja:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{2} \\cdot x + \\frac{1}{2} = 0.5 \\cdot x + 0.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.065464Z",
     "start_time": "2021-04-14T10:23:47.954680Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 3)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# points\n",
    "x = np.array([1, 3])\n",
    "y = np.array([1, 2])\n",
    "plt.scatter(x, y, s=100, zorder=10)\n",
    "\n",
    "# line\n",
    "line = np.linspace(0, 4)\n",
    "plt.plot(line, 0.5 * line + 0.5, c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta recta es lo que denominamos **recta de regresión** y nos permite, conocido un valor de *x* determinar el valor real de *y*. Por ejemplo, sabemos que para $x=2$ el valor de $y=0.5\\cdot2+0.5=1.5$. Dicho con otras palabras, estamos estimando el valor *y* a partir de *x*.\n",
    "\n",
    "Si trasladamos este concepto al ámbito del *machine learning* diremos que hemos aprendido la ecuación de la recta, o lo que es lo mismo, el **modelo de regresión**, a partir de los puntos de un cojunto de datos (en el ejemplo anterior los puntos $p_1=(1,1)$ y $p_2=(3,2)$). El proceso de aprendizaje ha consistido en estimar los párametros de la ecuación de la recta en función de los valores de los puntos que conforman el modelo de datos. Recordamos que la ecuación general de la recta tiene la forma:\n",
    "\n",
    "$$\n",
    "y = m \\cdot x + b\n",
    "$$\n",
    "\n",
    "Que comparándolo con el modelo aprendido $y = 0.5 \\cdot x + 0.5$ nos indica que se ha aprendido que para estimar correctamente los puntos $p_1=(1,1)$ y $p_2=(3,2)$ la pendiente de la recta debe ser $m=0.5$ y la interescción (o *bias*) debe ser $b=0.5$.\n",
    "\n",
    "El ejemplo anterior tiene fines didácticos, trabajar en el mundo del *machine learning* con conjuntos de datos implica una dimensionalidad mayor: habrá muchos más puntos (filas del conjunto de datos) y características (columnas del conjunto de datos). Por ello, los modelos de regresión se suelen generalizar del siguiente modo:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_m \\cdot X_m + \\epsilon\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\beta_0$ se corresponde con el *bias* de la recta.\n",
    "- $\\beta_1, \\dots, \\beta_m$ son los parámetros de la pendiente de la recta en las diferentes dimensiones\n",
    "- $\\epsilon$ representa el error que cometemos al no poder aproximar perfectamente todos los puntos. Nótese que cuando tenemos más de 2 puntos es prácticamente imposible encontrar una **recta** que pase por todos ellos.\n",
    "\n",
    "Podemos escribir la expresión anterior usando una notación más compacta de matrices:\n",
    "\n",
    "$$\n",
    "Y = X \\cdot \\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Siendo:\n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\n",
    "\\end{equation} \n",
    "\n",
    "La matriz que contiene los valores de la variable objetivo (i.e. la variable que queremos predecir) para la *n* muestras del conjunto de datos.\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\begin{pmatrix} 1 & x_{1,1} & \\cdots & x_{1,m} \\\\ 1 & x_{2,1} & \\cdots & x_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,m} \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "La matriz que contiene los valores de las *m* carácterísticas ($x_{i,j}$) del conjunto de datos.\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_m \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "La matriz de coeficientes del modelo de regresión **que se van a aprender**.\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "La matriz con los residuos.\n",
    "\n",
    "A partir de estas matrices podemos obtener un sistema de ecuaciones formado por *n* ecuaciones y *m* incógnitas. Normalmente, en *machine learning* disponemos de muchas más muestras que características, por lo que $n >> m$ y el sistema es *sobredimensionado* y la solución exacta no existe (tenemos infinitas soluciones posibles). Utilizando el método de los [mínimos cuadrados ordinarios](https://es.wikipedia.org/wiki/M%C3%ADnimos_cuadrados_ordinarios) podemos buscar la solución que obtenga el mejor $\\beta$ y minimize $|\\epsilon|^2$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial |\\epsilon|^2}{\\partial \\beta} &= \\frac{\\partial |Y-X\\beta|^2}{\\partial \\beta} \\\\\n",
    "&= \\frac{\\partial \\left[ (Y-X\\beta)^t (Y-X\\beta) \\right] }{\\partial \\beta} \\\\\n",
    "&= \\frac{\\partial \\left[ Y^tY - \\beta^tX^tY - Y^tX\\beta + \\beta^tX^tX\\beta \\right] }{\\partial \\beta} \\\\\n",
    "&= - 2 X^t Y + 2 X^t X \\beta = 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Por tanto, podemos *aprender* los valores de los coeficientes que minimizan los residuos a partir de los datos del siguiente modo:\n",
    "\n",
    "$$\n",
    "\\beta = \\left[ \\left( X^t X \\right)^{-1} X^t \\right] Y\n",
    "$$\n",
    "\n",
    "Gráficamente tenemos que dados los siguientes puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.692863Z",
     "start_time": "2021-04-14T10:23:48.067609Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=50, n_features=1, n_informative=1, n_targets=1, noise=5, random_state=43)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(X[:,0], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular $\\beta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.699843Z",
     "start_time": "2021-04-14T10:23:48.694530Z"
    }
   },
   "outputs": [],
   "source": [
    "n,m = X.shape\n",
    "x0 = np.ones((n,1))\n",
    "stack = np.hstack((x0, X))\n",
    "\n",
    "# beta = ((X^t * X)^-1 * X^t) * y\n",
    "beta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(stack), stack)), np.transpose(stack)), y)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y pintar la recta de regresión del modelo de acuerdo con la ecuación:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 \\cdot x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.804842Z",
     "start_time": "2021-04-14T10:23:48.701436Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(X[:,0], y, zorder=10)\n",
    "\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, beta[0] + beta[1] * line, c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos mismo cálculos podemos hacerlo, de forma más simple, usando el modelo `linear_model.LinearRegression` de `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.831692Z",
     "start_time": "2021-04-14T10:23:48.806343Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible acceder a los **parámetros** del modelo (i.e. los $\\beta$) mediante los atributos `coef_` ($\\beta_1$) e `intercept_` ($\\beta_0$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.837181Z",
     "start_time": "2021-04-14T10:23:48.833241Z"
    }
   },
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.844096Z",
     "start_time": "2021-04-14T10:23:48.840173Z"
    }
   },
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de la regresión lineal\n",
    "\n",
    "Entendido el modelo de regresión lineal debemos indagar en el significado de los parámetros $\\beta$ aprendidos por el modelo. Para ello, en lugar de usar un modelo tan sencillo, vamos a utilizar el conjunto de datos de [*boston house-prices*](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset) que contiene las siguientes columnas:\n",
    "\n",
    "- **CRIM**: crimen per cápita por ciudad\n",
    "- **ZN**: proporción de terrenos residenciales divididos en zonas para lotes de más de 25,000 pies cuadrados\n",
    "- **INDUS**: proporción de acres de negocios no minoristas por ciudad\n",
    "- **CHAS**: variable ficticia de Charles River (= 1 si el tramo limita el río, 0 de lo contrario)\n",
    "- **NOX**: concentración de óxidos nítricos (partes por 10 millones)\n",
    "- **RM**: número promedio de habitaciones por vivienda\n",
    "- **AGE**: proporción de unidades ocupadas por sus propietarios construidas antes de 1940\n",
    "- **DIS**: distancias ponderadas a cinco centros de empleo de Boston\n",
    "- **RAD**: índice de accesibilidad a las autopistas radiales\n",
    "- **TAX**: tasa de impuesto a la propiedad de valor completo por 10.000\\$\n",
    "- **PTRATIO**: colegios por localidad\n",
    "- **B**: 1000 (Bk - 0,63)^ 2, donde Bk es la proporción de personas de color por ciudad\n",
    "- **LSTAT**: porcentaje de estado inferior de la población\n",
    "- **MEDV**: valor mediano de las viviendas ocupadas por sus propietarios en 1000\\$ (**variable objetivo**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.852358Z",
     "start_time": "2021-04-14T10:23:48.845626Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos regresión lineal al conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.859490Z",
     "start_time": "2021-04-14T10:23:48.853613Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos el parámetro `intercept_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.864215Z",
     "start_time": "2021-04-14T10:23:48.860883Z"
    }
   },
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su valor es 36.4594883850901 que representa el precio básico de cualquier vivienda sin contar con el resto de las características que describan la vivienda.\n",
    "\n",
    "Ahora analizamos los coeficientes del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:48.869020Z",
     "start_time": "2021-04-14T10:23:48.865719Z"
    }
   },
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para ello vinculamos cada coeficiente a la característica (*feature*) a la que están representado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:49.002720Z",
     "start_time": "2021-04-14T10:23:48.870362Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=np.expand_dims(reg.coef_, axis=0), columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como norma general:\n",
    "\n",
    "- Los valores positivos en estos coeficientes significan una correlación directa entre los valores de las características que representan los coeficientes y el precio de la vivienda, mientras que los valores negativos representan una correlación inversa.\n",
    "- La magnitud de los coeficientes miden el grado de aportación, positiva o negativa, de una determinada característica al precio de la vivienda.\n",
    "\n",
    "Por poner algunos ejemplos:\n",
    "\n",
    "- Un valor -17.766611 en NOX indica que el precio de la vivienda se **decrementa** en aproximadamente 18000\\$ por cada parte por 10 millones de concentración de óxidos nítricos.\n",
    "- Un valor de 3.809865 en RM indica que el precio de la vivienda **incrementa** en aproximadamente 4000\\$ por cada habitación de la vivienda.\n",
    "- Un valor de 0.000692 en AGE indica que el precio de la vivienda se **mantiene** constante (tiene un incremento de 6\\$) tanto si las casas están ocupadas por sus propietarios como si son alquiladas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas de la regresión lineal\n",
    "\n",
    "La regresión linea presenta principalmente 3 problemas: sobreajuste, escalabilidad y linealidad.\n",
    "\n",
    "### Sobreajuste\n",
    "\n",
    "El modelo de regresión lineal tiene un problema sobreajuste (en inglés *overfitting*) puesto que no es capaz de generalizar bien la línea de regresión en determinados contextos. Por ejemplo, cuando nuestro conjunto de datos contiene datos atípicos (*outlayers*), es decir, muestras que no siguen el patrón predominante en el conjunto de datos, la regresión lineal tiende a desviarse de su ideal prestando excesiva atención a estos datos atípicos.\n",
    "\n",
    "El siguiente ejemplo muestra, de forma gráfica, este proceso. Observamos nuestro conjunto de datos (puntos azules) y los *outlayers* (puntos naranjas). En verde aparece la regresión que se produciría si no existieran *outlayers* y en rojo la que se obtiene con los *outlayers*. Como vemos, unas pocas muestras han desviado significativamente la regresión provocando errores de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:49.160436Z",
     "start_time": "2021-04-14T10:23:49.004233Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# conjunto de datos\n",
    "n_samples = 20\n",
    "n_outlayers = 2\n",
    "\n",
    "X, y = make_regression(n_samples=n_samples, n_features=1, n_informative=1, n_targets=1, noise=5, random_state=43)\n",
    "\n",
    "X_outlayers, y_outlayers = make_regression(n_samples=n_outlayers, n_features=1, n_informative=1, n_targets=1, noise=5, bias=100, random_state=43)\n",
    "\n",
    "X = np.vstack((X, X_outlayers))\n",
    "y = np.hstack((y, y_outlayers))\n",
    "\n",
    "# configuración del gráfico\n",
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# muestras\n",
    "plt.scatter(X[:n_samples,0], y[:n_samples], zorder=10, label=\"muestras\")\n",
    "plt.scatter(X[n_samples:,0], y[n_samples:], zorder=10, label=\"outlayers\")\n",
    "\n",
    "# regresion sin outlayers\n",
    "reg = LinearRegression().fit(X[:n_samples,:], y[:n_samples])\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, reg.intercept_ + reg.coef_[0] * line, c='green', label=\"sin outlayers\")\n",
    "\n",
    "# regresion con outlayers\n",
    "reg_outlayers = LinearRegression().fit(X, y)\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, reg_outlayers.intercept_ + reg_outlayers.coef_[0] * line, c='red', label=\"con outlayers\")\n",
    "\n",
    "# mostrar el gráfico\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalabilidad\n",
    "\n",
    "Como ya hemos comentado, obtener los parámetros de la regresión lineal implica las siguientes operaciones matriciales $\\beta = \\left[ \\left( X^t X \\right)^{-1} X^t \\right] Y$. Es decir, para poder calcular los parámetros del modelo requerimos hacer 3 multiplicaciones de matrices y una matriz inversa. Estas operaciones matemáticas, en su implementación más eficiente (*Optimized CW-like algorithms*) tienen un coste computacional de $O(n^{2.373})$. Por ello, el rendimiento del algoritmo de regresión lineal se ve profundamente mermado con conjuntos de datos grandes lo que hace que el modelo no sea escalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linealidad\n",
    "\n",
    "La regresión lineal, como su propio nombre indica, asume linealidad en la correlación entre la variable objetivo y las características del conjunto de datos. Sin embargo, en los problemas del mundo real, esta linealidad raramente se cumple, lo que provoca que el modelo no estime bien debido al *underfitting*. \n",
    "\n",
    "El siguiente ejemplo muestra una relación no lineal entre la variable objetivo *y* y la característica *x*. Concretamente $y = 3 - 2x + x^3$ (con ruido añadido). Observamos que el modelo (línea roja) no se ajusta nada bien a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:23:49.315357Z",
     "start_time": "2021-04-14T10:23:49.162039Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "X = np.expand_dims(np.linspace(-2, 2, n_samples), axis=1) + np.random.rand(n_samples,1)\n",
    "y = 3 - 2 * X + X * X * X + np.random.rand(n_samples,1)\n",
    "\n",
    "# configuración del gráfico\n",
    "plt.figure()\n",
    "\n",
    "min_x = np.min(X)\n",
    "max_x = np.max(X)\n",
    "diff_x = max_x - min_x\n",
    "\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "diff_y = max_y - min_y\n",
    "\n",
    "plt.xlim(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.ylim(min_y - 0.1 * diff_y, max_y + 0.1 * diff_y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "line = np.linspace(min_x - 0.1 * diff_x, max_x + 0.1 * diff_x)\n",
    "plt.plot(line, reg.intercept_ + reg.coef_[0] * line, c='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T10:31:57.806133Z",
     "start_time": "2021-04-14T10:31:57.797319Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es).\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
