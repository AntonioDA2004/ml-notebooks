{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UZV-7aMargx"
   },
   "source": [
    "# Preprocesamiento de texto\n",
    "\n",
    "Cuando los datos de entrada de nuestro algoritmo de *machine learning* son textos, tenemos un problema similar al que sucedía cuando los datos de entrada eran variables discretas: los algoritmos de *machine learning* funcionan a partir de operaciones matemáticas que no pueden operar con textos. No existe defidida ninguna operación matemática que puede combinar las palabras _\"hola\"_ y _\"adiós\"_. Por tanto, para poder emplear textos definidos en lenguaje natural, independientemente del idioma empleado, necesitamos transformar esos textos en vectores numéricos que los representen.\n",
    "\n",
    "La técnica más conocida para hacer esta transformación se denomina ***bag of words***. Veamos cómo funciona con un ejemplo. Supongamos que tenemos el siguiente texto:\n",
    "\n",
    "> \"*El miedo es el camino hacia el lado oscuro, el miedo lleva a la ira, la ira lleva al odio, el odio lleva al sufrimiento, el sufrimiento al lado oscuro.*\"\n",
    "\n",
    "El primer paso que debemos realizar es el que conocemos como ***tokenizacion***, que consiste en trasformar el texto anterior en un array de palabras. Es decir, vamos a separar cada una de las palabras que conforman la frase anterior empleando como separador los espacios y signos de puntuación. Por tanto, obtendríamos la siguiente lista de *tokens*:\n",
    "\n",
    "\n",
    "`['El', 'miedo', 'es', 'el', 'camino', 'hacia', 'el', 'lado', 'oscuro', 'el', 'miedo', 'lleva', 'a', 'la', 'ira', 'la', 'ira', 'lleva', 'al', 'odio', 'el', 'odio', 'lleva', 'al', 'sufrimiento', 'el', 'sufrimiento', 'al', 'lado', 'oscuro']`\n",
    "\n",
    "Ahora vamos a homogeneizar nuestro texto transformando todas las palabras a minúsculas:\n",
    "\n",
    "`['el', 'miedo', 'es', 'el', 'camino', 'hacia', 'el', 'lado', 'oscuro', 'el', 'miedo', 'lleva', 'a', 'la', 'ira', 'la', 'ira', 'lleva', 'al', 'odio', 'el', 'odio', 'lleva', 'al', 'sufrimiento', 'el', 'sufrimiento', 'al', 'lado', 'oscuro']`\n",
    "\n",
    "A partir de la lista anterior podemos construir un diccionario que contiene todas las palabras que están definidas en nuestro vocabulario. Entendemos como \"nuestro vocabulario\" a las palabras que aparecen en los textos que estamos analizando. El algoritmo de *machine learning* no necesita conocer si esa palabra pertenece o no al Diccionario de la Real Academia de la Lengua Española (o su equivalente en otros idiomas). Así pues, analizando los *tokens* anteriores construiremos el siguiente diccionario:\n",
    "\n",
    "`['el', 'miedo', 'es', 'camino', 'hacia', 'lado', 'oscuro', 'lleva', 'a', 'la', 'ira', 'odio', 'sufrimiento']`\n",
    "\n",
    "Por último, transformar el texto original en un vector numérico de tal forma que las posiciones del vector representan las posiciones de las palabras del diccionario y los valores del vector representa el número de apariciones de la palabra del diccionario en el texto analizado. Nuestro texto quedaría, por tanto, definido por el siguiente vector:\n",
    "\n",
    "`[6, 2, 1, 1, 1, 2, 2, 3, 1, 2, 2, 2, 2]`\n",
    "\n",
    "Analizándolo vemos que la palabra *'el'* se repite 6 veces, la palabra *'miedo'* 2, la palabra *'es'* 1, y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:13.391980Z",
     "start_time": "2021-05-06T11:09:12.691850Z"
    },
    "id": "QzzEeRLsFxyS"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIVa22lpUQMj"
   },
   "source": [
    "*sklearn* nos da soporte para transformar textos en su presentanción mediante *bag of words*. Para ello emplearemos el objeto [feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) del siguiente modo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edPYS-tQU1w8"
   },
   "source": [
    "Primero, importamos el módulo y definimos nuestro objeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:13.417441Z",
     "start_time": "2021-05-06T11:09:13.394116Z"
    },
    "id": "O4_vzJSlU68R"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_qdW0KWU_6l"
   },
   "source": [
    "A continuación, aplicamos la transformación a nuestro texto de ejemplo. En este caso, *CountVectorizer* está esperando una secuencia de textos, a la que se denomina ***corpus***, por lo que debemos declarar nuestro texto dentro de un array de 1 elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:13.439064Z",
     "start_time": "2021-05-06T11:09:13.426457Z"
    },
    "id": "rrkeG7G2VHRx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 6 1 1 2 2 2 3 2 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"El miedo es el camino hacia el lado oscuro, el miedo lleva a la ira, la ira lleva al odio, el odio lleva al sufrimiento, el sufrimiento al lado oscuro.\"\n",
    "]\n",
    "\n",
    "X = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2JPvuPCVEs9"
   },
   "source": [
    "Podemos analizar qué palabra corresponden a cada posición de este array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:13.445743Z",
     "start_time": "2021-05-06T11:09:13.441030Z"
    },
    "id": "uUXOKN5eWP9H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['al', 'camino', 'el', 'es', 'hacia', 'ira', 'la', 'lado', 'lleva', 'miedo', 'odio', 'oscuro', 'sufrimiento']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo-CyYypWkP6"
   },
   "source": [
    "Llegados a este punto es importante resaltar que *sklearn* almacena los vectores en una matriz dispersa que proporciona *NumPy*. Esto se debe a que, cuando trabajamos con textos, lo normal es que el *corpus* disponga de cientos o miles textos (llamados ***documentos***) sobre los cuales se construye el *diccionario*. Como es lógico, no todos los *documentos* contienen todas las plabras del *diccionario*, por lo que, habitualmente, los vectores de *bag of words* están repletos de valores 0. Esta es una información irrelevante que desperdicia gran cantidad de espacio en memoria, por lo que se alamcena utilizando otro tipo de estrucutras de datos que sólo guarda qué palabras pertenecen a cada documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lJZX9NtXZZz"
   },
   "source": [
    "Si analizamos con detalle el vector del texto de ejemplo vemos que las palabras con mayor número de repeticiones son *'el'*, con 6 repeticiones, *'al'*, con 3 repeticiones y *'lleva'* con 3 repeticiones. Esto es un gran problema, puesto que las 2 primeras no aportan ningún significado semántico al texto y provocará que nuestro algoritmo de *machine learning* no sea capaz de extraer conocimiento de *corpus* de documentos.\n",
    "\n",
    "Para resolver este problema se filtran estas palabras del *corpus* antes de construir el diccionario. A estas palabras, que en realidad son todos los artículos, preposiciones, etc., se las conoce como ***stop words***, y existen listas en diferentes idiomas para realizar este filtrado. Veamos cómo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esss68akZIBB"
   },
   "source": [
    "Cuando trabajamos con texto es extremadamente útil conocer la librería NLTK ([thttps://www.nltk.org/](https://www.nltk.org/)), ya que incorpora infinidad de herramientas para la manipulación de textos en lenguaje natural. Entre otras funcionalidades, incorpora una lista de *stop words* en diferentes idiomas. Carguemos las *stop words* en español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.510179Z",
     "start_time": "2021-05-06T11:09:13.447596Z"
    },
    "id": "w_1WHVvBZn-T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\") # required to download the stopwords lists\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrRIvlB-bAy5"
   },
   "source": [
    "Las visualizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.517693Z",
     "start_time": "2021-05-06T11:09:45.513306Z"
    },
    "id": "L8DUofULa97f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "print(spanish_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSdQ-npwaDZu"
   },
   "source": [
    "Ahora, podemos crearnos una instancia de nuestro *CountVectorizer* que incluya esta lista de *stop words* utilizando el parámetro *stop_words* de su constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.522271Z",
     "start_time": "2021-05-06T11:09:45.519724Z"
    },
    "id": "nppUdeMOaUaP"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words = spanish_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo2wEvLvbH4U"
   },
   "source": [
    "Y repetimos el proceso anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.532250Z",
     "start_time": "2021-05-06T11:09:45.525492Z"
    },
    "id": "3VPrbGVubLCj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 2 3 2 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"El miedo es el camino hacia el lado oscuro, el miedo lleva a la ira, la ira lleva al odio, el odio lleva al sufrimiento, el sufrimiento al lado oscuro.\"\n",
    "]\n",
    "\n",
    "X = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Liut1FG-bNqt"
   },
   "source": [
    "Si analizamos las palabras del diccionario vemos que no hay ni rastro de las *stop words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.537184Z",
     "start_time": "2021-05-06T11:09:45.534140Z"
    },
    "id": "r6Lcxoi0bUvy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['camino', 'hacia', 'ira', 'lado', 'lleva', 'miedo', 'odio', 'oscuro', 'sufrimiento']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvHN2EQ1beBz"
   },
   "source": [
    "## tf-idf\n",
    "\n",
    "Aunque llegados a este punto ya tenemos una buena representación vectorial de nuestros textos, aún existe un problema: la representación creada no está normalizada. Esta no-normalización plantea básicamente dos problemas:\n",
    "\n",
    "- A nivel de *documento* (las filas de nuestra matriz de datos) cada uno lleva una escala completamente libre y hace que sea imposible compararlos entre si. Un texto más largo tendrá contadores con valores más grandes que un texto más corto. En un ejemplo llevado al extremo podemos comparar un tweet con la noticia que enlaza ese tweet. Ambos documentos versarán sobre el mismo tema, pero no pueden compararse debido a la volumetría de ambos.\n",
    "\n",
    "- A nivel de *palabras* es complicado comparar cuáles son más relevante y cuáles menos para un *corpus* concreto. Ya hemos eliminado las *stop words*, pero, en función del sesgo de nuestro *corpus* existen palabras que no aportan demasiada información y, por lo tanto, su incidencia en nuestro algoritmo de *machine learning* debería ser menor. Por ejemplo, imaginemos que tenemos un *corpus* de documentos hablando únicamente de los equipos de la Liga de Fútbol Profesional. En este corpus la palabra *'fútbol'* es completamente irrelevante, ya que todos los documentos hablan de ella. Por contra, palabras como *'lesión'* o *'fichaje'* son muy relevante porque permiten subclasificar los documentos. Sin embargo, si nuestro *corpus* está formado por noticias de todo tipo, la palabra *'fútbol'* es muy relevante ya que identifica un tipo de noticias.\n",
    "\n",
    "Para resolver este problema se emplea una normalización denominada **tf-idf** (*term-frecuency times inverse document-frecuency*). Ésta viene definida por la siguiente ecuación:\n",
    "\n",
    "$\\textrm{tf-idf}(t, d) = tf(t, d) \\times idf(t)$\n",
    "\n",
    "siendo $tf(t, d)$ el número de veces que aparece el término (palabra) $t$ en el documento $d$ y definiéndose $idf(t)$ como:\n",
    "\n",
    "$idf(t) = log \\frac{1 + n}{1 + df(t)} + 1$\n",
    "\n",
    "siendo $n$ el número de documentos de nuestro *corpus* y $df(t)$ el número de documentos en los que aparece el término $t$.\n",
    "\n",
    "Posteriormente, los vectores son normalizados a nivel de documento (el modulo del vector de cada documento vale 1).\n",
    "\n",
    "Analizando estas ecuaciones *tf-idf* observamos que, aquellas palabras que tengan menos frecuencia de aparición serán más relevante que aquellas que aparezcan en más documentos.\n",
    "\n",
    "Esta transformación puede realizarse mediante el objeto [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n",
    "\n",
    "Para ver su funcionamiento, vamos a construir un *corpus* con varios documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.541413Z",
     "start_time": "2021-05-06T11:09:45.539023Z"
    },
    "id": "V7_qyymMhBKn"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Este es el primer documento.\",\n",
    "    \"Este documento es el segundo documento.\",\n",
    "    \"Y este es el tercero\",\n",
    "    \"¿Es este el primer documento? No.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itkxEwYxhSYe"
   },
   "source": [
    "Aplicando el proceso anterior obtenemos los siguientes vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.551970Z",
     "start_time": "2021-05-06T11:09:45.543214Z"
    },
    "id": "Doff7DYphYrS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [2 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['documento', 'primer', 'segundo', 'tercero']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqyW9Uu4iCRw"
   },
   "source": [
    "Veamos como aplicar ahora *tf-idf*. Cargamos el módulo y creamos el objeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.556286Z",
     "start_time": "2021-05-06T11:09:45.553675Z"
    },
    "id": "K5gY-SmgguQY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtqVR4kiiSe-"
   },
   "source": [
    "Realizamos la transformación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.571378Z",
     "start_time": "2021-05-06T11:09:45.558042Z"
    },
    "id": "_3ELNstaiUOO"
   },
   "outputs": [],
   "source": [
    "counts = X.toarray()\n",
    "X_transformed = tfidf_transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnsUWNcpidic"
   },
   "source": [
    "Analizamos el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.576824Z",
     "start_time": "2021-05-06T11:09:45.573055Z"
    },
    "id": "F5SJFW4Bie5C",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62922751 0.77722116 0.         0.        ]\n",
      " [0.78722298 0.         0.61666846 0.        ]\n",
      " [0.         0.         0.         1.        ]\n",
      " [0.62922751 0.77722116 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_transformed.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ8ySy4Pvoj_"
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Hasta ahora lo que tenemos es que cada uno de los términos o palabras de los documentos se transforman en una variable del conjunto de datos. Esto da lugar a una alta dimensionalidad del problema.\n",
    "\n",
    "Una posible solución pasa por aplicar un filtrado previo conocido como __Stemming__, que no es otra cosa que truncar las palabras y quedarnos únicamente con su raíz. Por ejemplo, las palabras _chocolate_ y _chocolatería_ son dos términos distintos, por lo que cada una de ellas se convertirían en una variable. Sin embargo, si aplicamos __stemming__ ambos términos se transformarían en `chocolate`, por lo que se contarían como dos ocurrencias del mismo término.\n",
    "\n",
    "Este filtrado tiene sentido _a priori_, pues al quedarnos con la raíz de cada palabra nos estamos quedando realmente con el _significado global_.\n",
    "\n",
    "Un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:45.664384Z",
     "start_time": "2021-05-06T11:09:45.578633Z"
    },
    "id": "BSKT5A9qvoj_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  :  the\n",
      "man  :  man\n",
      "passes  :  pass\n",
      "sentence  :  sentenc\n",
      "swing  :  swing\n",
      "sword  :  sword\n",
      ".  :  .\n",
      "If  :  If\n",
      "would  :  would\n",
      "take  :  take\n",
      "man  :  man\n",
      "'s  :  's\n",
      "life  :  life\n",
      ",  :  ,\n",
      "owe  :  owe\n",
      "look  :  look\n",
      "eyes  :  eye\n",
      "hear  :  hear\n",
      "final  :  final\n",
      "words  :  word\n",
      ".  :  .\n",
      "And  :  and\n",
      "bear  :  bear\n",
      ",  :  ,\n",
      "perhaps  :  perhap\n",
      "man  :  man\n",
      "deserve  :  deserv\n",
      "die  :  die\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "frase = \"\"\"The man who passes the sentence should swing the sword. \n",
    "If you would take a man's life, you owe it to him to look into his eyes and hear his final words. \n",
    "And if you cannot bear to do that, then perhaps the man does not deserve to die\"\"\"\n",
    "\n",
    "palabras = word_tokenize(frase)\n",
    "palabras = [p for p in palabras if not p in set(english_stopwords)]\n",
    "for p in palabras:\n",
    "    print(p, \" : \", ps.stem(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2hizSMZvokB"
   },
   "source": [
    "## Lematización\n",
    "\n",
    "La __lematización__ es el proceso de agrupar las diferentes formas inflexionadas de una palabra para que puedan ser analizadas como un solo elemento. La lematización es similar al _stemming_, pero aporta contexto a las palabras. Por lo tanto, vincula las palabras que tienen un significado similar.\n",
    "\n",
    "Generalmente, la lematización es preferible al _stemming_ porque la lematización hace un análisis morfológico de las palabras.\n",
    "\n",
    "Utilizando la frase anterior como ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:47.415532Z",
     "start_time": "2021-05-06T11:09:45.666084Z"
    },
    "id": "iypSM7LtvokB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                 The                 \n",
      "man                 man                 \n",
      "passes              pass                \n",
      "sentence            sentence            \n",
      "swing               swing               \n",
      "sword               sword               \n",
      ".                   .                   \n",
      "If                  If                  \n",
      "would               would               \n",
      "take                take                \n",
      "man                 man                 \n",
      "'s                  's                  \n",
      "life                life                \n",
      ",                   ,                   \n",
      "owe                 owe                 \n",
      "look                look                \n",
      "eyes                eye                 \n",
      "hear                hear                \n",
      "final               final               \n",
      "words               word                \n",
      ".                   .                   \n",
      "And                 And                 \n",
      "bear                bear                \n",
      ",                   ,                   \n",
      "perhaps             perhaps             \n",
      "man                 man                 \n",
      "deserve             deserve             \n",
      "die                 die                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for p in palabras:\n",
    "    print (\"{0:20}{1:20}\".format(p,wordnet_lemmatizer.lemmatize(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu0mYwLlvokI"
   },
   "source": [
    "Se puede observar que no se ha realizado ninguna sustitución de los términos por su _lema_. Esto se debe a que es necesario especificar el rol de cada término dentro de la oración, es decir, si es un verbo, sustantivo, adjetivo, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:47.423605Z",
     "start_time": "2021-05-06T11:09:47.417623Z"
    },
    "id": "41GE47gpvokJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                 The                 \n",
      "man                 man                 \n",
      "passes              pass                \n",
      "sentence            sentence            \n",
      "swing               swing               \n",
      "sword               sword               \n",
      ".                   .                   \n",
      "If                  If                  \n",
      "would               would               \n",
      "take                take                \n",
      "man                 man                 \n",
      "'s                  's                  \n",
      "life                life                \n",
      ",                   ,                   \n",
      "owe                 owe                 \n",
      "look                look                \n",
      "eyes                eye                 \n",
      "hear                hear                \n",
      "final               final               \n",
      "words               word                \n",
      ".                   .                   \n",
      "And                 And                 \n",
      "bear                bear                \n",
      ",                   ,                   \n",
      "perhaps             perhaps             \n",
      "man                 man                 \n",
      "deserve             deserve             \n",
      "die                 die                 \n"
     ]
    }
   ],
   "source": [
    "for p in palabras:\n",
    "    print (\"{0:20}{1:20}\".format(p,wordnet_lemmatizer.lemmatize(p, pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phUwRK4Rvsex"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Raúl Lara** (raul.lara@upm.es) y **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "05. Preprocesamiento de textos.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
