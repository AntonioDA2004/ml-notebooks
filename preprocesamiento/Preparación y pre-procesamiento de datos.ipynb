{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preparación y pre-procesamiento de datos.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ggq3XvB6GJDT","colab_type":"text"},"source":["# Preparación y pre-procesamiento de datos\n","\n","Los datos son la columna vertebral del *machine learning*. No puede existir aprendizaje si no existen **suficientes datos**, puesto que los algoritmos de *machine learning* no son capaces de generalizar los resultados: sólo aprenden un determinado patrón de todos los existentes. Esto se debe a la propia naturaleza del aprendizaje. Imaginemos, por ejemplo, que un niño pequeño ha visto únicamente 5 vehículos a motor en toda su vida. Es altamente probable que el niño sea capaza de identificar qué es un vehículo a motor, pero, por el contrario, le resultaría imposible diferenciar entre los distintos tipos de vehículos a motor que existen (coche, camiones, autobuses, motocicletas, etc.). Disponer de la cantidad de datos adecuada es, por tanto, esencial para los científicos de datos.\n","\n","En cualquier caso, existen situaciones en las que, teniendo una gran cantidad de datos, estos no son adecuados para el *machine learning*. Si los datos no son representativos o si los datos están **sesgados** (*biased*, en inglés) no es posible aprender correctamente a partir de los mismos. Ilustremos este problema con un par de ejemplos:\n","\n","- No es posible predecir de forma precisa qué canción le interesará a una persona mayor en un servicio de música bajo demanda si todos los demás usuarios del servicio son personas jóvenes. En este caso, los datos están sesgados hacia las preferencias de las personas jóvenes. Sin embargo, para este caso particular, los algoritmos de *machine learning* serán capaces de realizar recomendaciones muy certeras a las personas jóvenes. \n","\n","- Si le proporcionamos a nuestro algoritmo suficientes imágenes de perros y gatos, conseguiremos ser capaces de distinguir entre estos dos animales, pero será imposible que ese mismo algoritmo diference un león de un elefante. Del mismo modo, si todas las imágenes de perros que le pasamos son de *pastores alemanes*, el algoritmo tendrá grandes dificultades para reconocer que un *caniche* o un *carlino* son perros. En todos estos casos, los datos están sesgados hacia un tipo concreto de animal, bien sea los perros y gatos frente a otros animales o los *pastores alemanes* frente a otras razas de perro.\n","\n","Del mismo modo, aún teniendo suficientes datos y no estando estos sesgados, es posible encontrar datos que no tienen la **calidad** requerida para que los algoritmos de *machine learning* sean capaces de aprende. Algunos casos típicos de datos con poca calidad son:\n","\n","- Datos incompletos: personas que no rellenan el campo edad, códigos postales incompletos en formularios, etc.\n","\n","- Anomalías (*outlaiers*, en inglés): datos incorrectos provenientes de errores humanos, sensores averiados, fallos informáticos, etc.\n","\n","- Inconsistencias o datos incorrectos: direcciones de correo electrónico sin el símbolo @, direcciones postales sin el número de la calle, etc.\n","\n","Por último, es importante reseñar, que datos irrelevantes pueden arruinar el proceso del *machine learning*. Para lograr unos resultados relevantes, es fundamental disponer de unos datos **relevantes**. Es imposible predecir la venta de pañales basándonos en la climatología, al igual que es imposible predecir una cardiopatía empleando datos psicológicos.\n","\n","Resumiendo, para lograr que un algoritmo de *machine learning* sea capaz de extraer conocimiento de un conjunto de datos es fundamental que los datos sean: suficientes, no-sesgados, de calidad y representativos. La ausencia de cualquiera de estas características desembocará en la imposibilidad de aprender patrones generalistas de los datos analizados. Con frecuencia, al *machine learning* se le conoce como *learning from data* (aprendizaje desde los datos) ya que no se posible extraer conocimiento si este no existe en los propios datos."]},{"cell_type":"markdown","metadata":{"id":"remUZSMPTM9u","colab_type":"text"},"source":["## Tipos de datos\n","\n","Cuando se habla de tipos de datos en el ámbito del *machine learning* es habitual realizar una distinción entre **datos estructurados** y **datos no estructurados**. Los primeros, hacen referencia a aquellos datos en los que es sencillo determinar el significado intrínseco de un dato. Por ejemplo, cuando trabajamos con datos biométricos, si un sensor nos proporcional las pulsaciones por minuto de una persona, se sabe de forma univoca a qué hace referencia ese dato y que si está por encima de 120 o por debajo de 40 es un valor anómalo. Del mismo modo, si un usuario indica que le gusta un video de youtube podemos estar seguro de que al usuario le interesa dicho video. Por el contrario, en los datos no estructurados es extremadamente complejo conocer el significado del dato que se está tratando. En el ejemplo anterior, si el usuario en lugar de indicar que le ha gustado un video escribe una reseña positiva del mismo, para un algoritmo de *machine learning* es sumamente complicado conocer si la reseña es positiva o negativa debido a la naturaleza desestructurada del lenguaje natural. Además del texto, las imágenes, los vídeos o los sonidos son tipos de datos que suelen considerarse como no estructurados.\n","\n","En los inicios del *machine learning*, la inmensa mayoría de los algoritmos eran capaces de trabajar únicamente con datos estructurados. Sin embargo, en los últimos años se han producido enormes progresos en el tratamiento de datos no estructurados, gracias, principalmente, al auge del *Deep Learning*.\n","\n","La siguiente imagen muestra la identificación de objetos a partir de una fotografía de la calle:\n","\n","![Reconocimiento de objetos](https://drive.google.com/uc?export=view&id=1FWA0TmH9yIQSgpdt7KGbN2_sCfIC4yKl)\n","\n","El siguiente fragmento del texto es una reseña de una cerveza escrita por un algoritmo de *machine learning* (que nunca ha probado cerveza):\n","\n","> *The smell is creamy, malty and woody, not much presence. The taste is dark fruits, and floral hops before its a strong destroy from the mouth as it warms up.*\n","\n","El siguiente vídeo muestra una canción completamente generada por un ordenador:\n","\n","[![Música generada por computador](https://img.youtube.com/vi/Ir_AFDKOc-I/0.jpg)](https://www.youtube.com/watch?v=Ir_AFDKOc-I)\n","\n","Independientemente del tipo de datos que empleemos y del algoritmo que les sea aplicado, cuando trabajamos en *machine learning* es fundamente realizar un pre-procesamiento de los datos. Este pre-procesamiento permite preparar los datos para que puedan ser interpretados por los algoritmos de *machine learning*."]},{"cell_type":"markdown","metadata":{"id":"ucYu0RAWF0oD","colab_type":"text"},"source":["En este documento estudiaremos cómo trabajar con los siguientes tipos de datos: datos continuos, datos discretos, textos e imágenes.\n","\n","Durante este documento se mostrarán ejemplos prácticos, en Python, de los conceptos enseñados. Para ello, haremos uso de dos librerías que nos van a facilitar enormemente esta tarea: NumPy ([https://numpy.org/](https://numpy.org/)), una librería fundamental de Python para la computación científica que incorpora, principalmente, un objeto para trabajar de forma sencilla con arrays N-dimensionales; y scikit-learn ([https://scikit-learn.org/](https://scikit-learn.org/stable/)), una librería de Python que incorpora herramientas simples y eficaces para el minado y análisis de datos.\n","\n","Ambas librería pueden importarse al proyecto del siguiente modo:\n"]},{"cell_type":"code","metadata":{"id":"QzzEeRLsFxyS","colab_type":"code","colab":{}},"source":["import sklearn\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afJWL-DLMq6b","colab_type":"text"},"source":["## Conceptos previos\n","\n","Antes de comenzar a analizar cada tipo de datos, debemos entender qué espera recibir como entrada un algoritmo de *machine learning*. Generalmente, los algoritmos de *machine learning* reciben como entrada una matriz (array) de datos en el que las filas representan las muestras tomadas (*samples*, en ingles) y las columnas representan las diferentes características (*features*, en inglés) de cada muestra.\n","\n","Veamos un ejemplo. La siguiente tabla muestra los datos de los salarios de jugadores de fútbol en función de la posición que ocupan y el club en el que jugan:\n","\n","| Club | Posición | Salario |\n","| :------: | :------: | :------: |\n","| Real Madrid | delantero | 14.000.000 € / anuales |\n","| Barcelona | defensa | 9.000.000 € / anuales |\n","| Atlético de Madrid | centrocampista | 7.000.000 € / anuales |\n","| Valencia | portero | 3.000.000 € / anuales |\n","\n","Cada una de las filas, a excepción de la cabecera, son las muestras de las que se dispone. Estas muestras son la cantidad de observaciones que tenemos y van a definir el número de datos del que diponemos, en este caso, 4. El club, la posición y el salario son las características de cada muestra.\n","\n","Un problema típico de *machine learning* sería predecir el salario de un jugador en base a su posición y club. En ese caso, a las columnas de Club y Posición se las suele representar con $X$, siendo $X1$ el Club y $X2$ la Posición. Por el contrario, a la columna Salario, que es la que se quiere predecir, se la representa con $y$.\n","\n","Utilizando *NumPy* podemos almacenar los datos del siguiente modo:"]},{"cell_type":"code","metadata":{"id":"3DjPCtDOP8n4","colab_type":"code","outputId":"1501993a-8305-4f61-9dbb-12d5db5a40d3","executionInfo":{"status":"ok","timestamp":1564395788587,"user_tz":-120,"elapsed":587,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["X = np.array([['Real Madrid',        'delantero'     ],\n","              ['Barcelona',          'defensa'       ],\n","              ['Atlético de Madrid', 'centrocampista'],\n","              ['Valencia',           'portero'       ]])\n","print(X)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[['Real Madrid' 'delantero']\n"," ['Barcelona' 'defensa']\n"," ['Atlético de Madrid' 'centrocampista']\n"," ['Valencia' 'portero']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SrSwPxwIQWXM","colab_type":"code","outputId":"7c0d8633-4321-489b-da99-9778bb6089a7","executionInfo":{"status":"ok","timestamp":1564395829807,"user_tz":-120,"elapsed":571,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["Y = np.array([14000000, 9000000, 7000000, 3000000])\n","print(Y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[14000000  9000000  7000000  3000000]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mclP9DsAal9s","colab_type":"text"},"source":["## Datos continuos\n","\n","Entendemos por datos continuos a aquellos datos numéricos que pueden tomar cualquier valor (real) en un rango preestablecido. Algunos ejemplos de datos contínuos son:\n","\n","- La *altura de una persona* puede ser cualquier valor decimal comprendido entre 0 e $\\infty$.\n","- La *distancia entre dos ciudades* puede ser cualquier valor decimal comprendido entre 0 e $\\infty$.\n","- La *temperatura de una ciudad* puede ser cualqueir valor decimal comprendido entre $-\\infty$ y $\\infty$.\n","\n","Los datos continuos son los más habituales dentro del ecosistema del *machine learning* por lo que el correcto tratamiento de estos es fundamental para alcanzar los resultados esperados. El principal problema de este tipo de datos es que no disponen de una escala homogénea, por lo que, cada dato, se mueve en un rango diferente al del resto, lo que dificulta enormemente el aprendizaje a partir de los mismos.\n","\n","Generalmente, a los datos continuos se les realiza un proceso de estandarización o normalización con el fin de acotarlos a un rango de valores que permita compararlos entre si independientemente de su naturaleza.\n","\n","La **estandarización** es el proceso a partir del cual un conjunto de datos que siguen una distribución normal, hecho que sucede con la mayoría de datos empleados en *machine learning*, son transformados a una distribución normal con media 0 y desviación típica 1. Para ello, se realiza la siguiente operación:\n","\n","$x^\\prime_i = \\frac{x_i - \\mu}{\\sigma}$\n","\n","Donde $x_i$ es el dato que queremos estandarizar, $\\mu$ es el valor medio de todos los datos y $\\sigma$ es la desviación típica de todos los datos.\n","\n","Ilustremos esto con un ejemplo. Asumamos que tenemos la siguiente matriz de datos en la que las filas son las muestras y las columnas las características:\n"]},{"cell_type":"code","metadata":{"id":"-lWMigVYLYCp","colab_type":"code","colab":{}},"source":["X = np.array([[ 1., -1.,  2.],\n","              [ 2.,  0.,  0.],\n","              [ 0.,  1., -1.]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3xqlsxkLeKY","colab_type":"text"},"source":["Podemos emplear la función [preprocessing.scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale) para estandarizar las características (i.e. las columnas):"]},{"cell_type":"code","metadata":{"id":"5fHWL923XV9-","colab_type":"code","colab":{}},"source":["from sklearn import preprocessing"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7pj3DmGlLovW","colab_type":"code","outputId":"03bb5f02-03bf-4256-a58e-cc0113eef13a","executionInfo":{"status":"ok","timestamp":1564395960102,"user_tz":-120,"elapsed":590,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["X_scaled = sklearn.preprocessing.scale(X)\n","print(X_scaled)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.         -1.22474487  1.33630621]\n"," [ 1.22474487  0.         -0.26726124]\n"," [-1.22474487  1.22474487 -1.06904497]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5tjEPyoHMJyk","colab_type":"text"},"source":["Como vemos, los datos han sido transformados a unos nuevos estandarizados. Si analizamos la media y desviación típica de estos datos observamos lo siguiente:"]},{"cell_type":"code","metadata":{"id":"GbfCGTJCMTNx","colab_type":"code","outputId":"fb988c05-f005-42ac-9ce3-f723c5363804","executionInfo":{"status":"ok","timestamp":1564395961739,"user_tz":-120,"elapsed":764,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_scaled.mean(axis=0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"AxSaGr4dMcrs","colab_type":"code","outputId":"1ff8a562-8491-4b28-a967-d0134b2702b5","executionInfo":{"status":"ok","timestamp":1564395963869,"user_tz":-120,"elapsed":589,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_scaled.std(axis=0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"9nU50J3MMeq7","colab_type":"text"},"source":["La media de cada característica ha sido centrada en el 0 y la desviación típica puesta en 1. Si comparamos esto con los datos sin estandarizar vemos la diferencia:"]},{"cell_type":"code","metadata":{"id":"sWVDnWKEQviO","colab_type":"code","outputId":"800c7380-5dd5-4b07-b640-c229647c9610","executionInfo":{"status":"ok","timestamp":1564395965894,"user_tz":-120,"elapsed":542,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.mean(axis=0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.        , 0.        , 0.33333333])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"3trg_H6cRFpS","colab_type":"code","outputId":"db96785e-2a58-426c-ed50-8e5a836207e7","executionInfo":{"status":"ok","timestamp":1564395996135,"user_tz":-120,"elapsed":554,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.std(axis=0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.81649658, 0.81649658, 1.24721913])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"PyUhuEQkRv3n","colab_type":"text"},"source":["Podemos observar cómo se han modificado los datos para que todos ellos sigan una distribución normal de media 0 y desviación típica 1."]},{"cell_type":"markdown","metadata":{"id":"O_nn6rd5Sa-F","colab_type":"text"},"source":["Existe otra alternativa para la estandarización de las características que consiste en ajustarlas en un rango predefinido, generalmente en el rango $[0, 1]$. Usualmente se utiliza cuando se tienen datos con una desviación típica muy pequeña."]},{"cell_type":"markdown","metadata":{"id":"hhVHZjqnS4s0","colab_type":"text"},"source":["Para la realización de este escalado usaremos la función [preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler). Si queremos normalizar en la escala $[0, 1]$ usaremos:\n","\n"]},{"cell_type":"code","metadata":{"id":"dKIoh2YhTI-J","colab_type":"code","outputId":"cbffdefc-7cb4-4707-9a24-82c2d5173921","executionInfo":{"status":"ok","timestamp":1564396641075,"user_tz":-120,"elapsed":571,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n","X_scaled = min_max_scaler.fit_transform(X)\n","print(X_scaled)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.5        0.         1.        ]\n"," [1.         0.5        0.33333333]\n"," [0.         1.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gdr2B_PaTtkx","colab_type":"text"},"source":["Si queremos emplear otro rango simplemente debemos indicar, mediante una dupla, la escala deseada en el constructor del objeto:"]},{"cell_type":"code","metadata":{"id":"bzm_IJa0T0Hn","colab_type":"code","outputId":"37732832-8c9e-4ade-fc19-3294cf95315f","executionInfo":{"status":"ok","timestamp":1564396747328,"user_tz":-120,"elapsed":580,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["min_max_scaler = sklearn.preprocessing.MinMaxScaler((1,5)) # escala [1, 5]\n","X_scaled = min_max_scaler.fit_transform(X)\n","print(X_scaled)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[3.         1.         5.        ]\n"," [5.         3.         2.33333333]\n"," [1.         5.         1.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GLy9k4ToUZyK","colab_type":"text"},"source":["Otra transformación habitual para este tipo de datos es la conocida como **normalización**. La normalización es el proceso de escalar cada muestra individual para que tengan la norma unitaria. Dicho de otro modo, si asumimos cada muestra como un vector *n*-dimensional (*n* es el número de características), mediante la normalización logramos que estos vectores tengan dimensión 1.\n","\n","Para ello, existen tres normas:\n","\n","- *L1*, se normaliza mediante la suma de los valores absolutos de sus componentes.\n","- *L2*, se normaliza mediante la raíz cuadrada de la suma de sus componentes al cuadrado.\n","- *max*, se normaliza mediante elemento mayor de sus componentes.\n","\n","Por ejemplo, si tenemos el vector $X = [-3, 4]$ obtendríamos:\n","\n","- *L1*: la norma se calcula como $|-3| + |4| = 7$ y el vector quedaría $X^\\prime = [-0.43, 0.57]$.\n","- *L2*: la norma se calcula como $\\sqrt{(-3)^2 + 4^2} = \\sqrt{9+16} = \\sqrt{25} = 5$ y el vector quedaría $X^\\prime = [-0.12, 0.16]$.\n","- *max*: la norma sería $4$ y el vector quedaría $X^\\prime = [-0.75, 1]$.\n","\n","La más utilizada es la norma L2 y suele aplicarse cuando el algoritmo seleccionado utiliza una forma cuadrática como, por ejemplo, el producto escalar, para calcular la similaridad entre cada par de muestras."]},{"cell_type":"markdown","metadata":{"id":"kPG5b-fFZ8VR","colab_type":"text"},"source":["Para la normalización utilizaremos [preprocessing.normalize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize):\n","\n"]},{"cell_type":"code","metadata":{"id":"ns94N_UgFkPg","colab_type":"code","outputId":"43124766-47f7-416e-ab16-f841df21f128","executionInfo":{"status":"ok","timestamp":1564398387809,"user_tz":-120,"elapsed":551,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["X_normalized_l1 = sklearn.preprocessing.normalize(X, norm='l1')\n","print(X_normalized_l1)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.25 -0.25  0.5 ]\n"," [ 1.    0.    0.  ]\n"," [ 0.    0.5  -0.5 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yTFOb2DSaQfJ","colab_type":"code","outputId":"3f0123ac-f139-4336-f3b7-c998428e1e2a","executionInfo":{"status":"ok","timestamp":1564398396839,"user_tz":-120,"elapsed":583,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["X_normalized_l2 = sklearn.preprocessing.normalize(X, norm='l2')\n","print(X_normalized_l2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.40824829 -0.40824829  0.81649658]\n"," [ 1.          0.          0.        ]\n"," [ 0.          0.70710678 -0.70710678]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r61sJ0k7aTBl","colab_type":"code","outputId":"688f6b45-1504-407e-bf70-80fedef61a94","executionInfo":{"status":"ok","timestamp":1564398409947,"user_tz":-120,"elapsed":579,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["X_normalized_max = sklearn.preprocessing.normalize(X, norm='max')\n","print(X_normalized_max)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.5 -0.5  1. ]\n"," [ 1.   0.   0. ]\n"," [ 0.   1.  -1. ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NpKbcDdvao7y","colab_type":"text"},"source":["## Datos discretos\n","\n","Entendemos por datos discretos a aquellos tipos de datos que sólo pueden tomar un valor prefijado entre un conjunto finito o infinito de valores. Algunos ejemplos de datos discretos son:\n","\n","- En el *método de pago de una compra online* hay que elegir entre pago contra-reembolso, pago con tarjeta, pago por trasferencia o pago con PayPal.\n","- La *edad de una persona* puede tomar valores 0, 1, 2, 3... hasta infinito. Aunque el conjunto de valores no esté acotado superiormente, estos datos son discreto, ya que la edad de una persona no se mide en número decimales.\n","- La *valoración de un usuario a un video de youtube* se mide en me gusta o no me gusta.\n","\n","Este tipo de datos, especialmente cuando no son numéricos, suelen ser problemáticos para la gran mayoría de algoritmos de *machine learning*. Casi todos los algoritmos se fundamentan en expresiones matemáticas que requieren operar con las características de las muestras. Por lo tanto, cuando se quiere realizar el producto de un datos que representa a un \"*hombre*\"* con uno que representa a una \"*mujer*\" sencillamente es imposible, ya que existen operaciones matemáticas definidas para esos valores.\n","\n","Para solventar este problema se utiliza una técnica denominada como *OneHotEncoding* que consisten en dividir una característica en tantas características como posibles valores pueda tomar la característica original y asignar el valor 0 ó 1 a cada una de estas nuevas características en función del valor de la original.\n","\n","Veamos esto con un ejemplo. Supongamos que tenemos las siguientes muestras:\n","\n","| Nombre | Sexo |\n","| :---: | :---: |\n","| Alice | mujer |\n","| Bob | hombre |\n","| Carl | hombre |\n","| Denna | mujer |\n","\n","La características *Sexo* se divide en las características *Hombre*  y *Mujer* que tomarán los siguientes valores:\n","\n","| Nombre | Hombre | Mujer |\n","| :---: | :---: | :---: |\n","| Alice | 0 | 1 |\n","| Bob | 1 | 0 |\n","| Carl | 1 | 0 |\n","| Denna | 0 | 1 |\n","\n","Como vemos, las nuevas características toman el valor 1 en la columna que coincide con el valor de la característica original."]},{"cell_type":"markdown","metadata":{"id":"ag4WY89Qfb57","colab_type":"text"},"source":["Esta codificación puede llevarse a cabo empleando el objeto [preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder).\n","\n","Cargamos la característica anterior en un array de *NumPy*:"]},{"cell_type":"code","metadata":{"id":"Lg1GLq88f0xs","colab_type":"code","colab":{}},"source":["X = np.array([['mujer' ],\n","              ['hombre'],\n","              ['hombre'],\n","              ['mujer' ]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SXh0Fdk0gTTn","colab_type":"text"},"source":["Y aplicamos la transformación:"]},{"cell_type":"code","metadata":{"id":"CKNtmeONgVUL","colab_type":"code","outputId":"42737b4d-6107-4bf1-c843-05f8283ef3dc","executionInfo":{"status":"ok","timestamp":1564400371936,"user_tz":-120,"elapsed":546,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["one_hot_encoder = sklearn.preprocessing.OneHotEncoder()\n","one_hot_encoder.fit(X)\n","X_transform = one_hot_encoder.transform(X).toarray()\n","print(X_transform)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0. 1.]\n"," [1. 0.]\n"," [1. 0.]\n"," [0. 1.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jnBqrAseiG4Y","colab_type":"text"},"source":["## Conjuntos de datos con tipos mixtos\n","\n","Anteriormente hemos analizado cómo transformar datos continuos y datos discretos para poder ser utilizados por los algoritmos de *machine learning*. Cuando trabajamos en problema reales, normalmente encontramos conjuntos de datos (*datasets*, en inglés) que poseen características tanto discretas como continuas. En los ejemplos visto anteriormente, sólo podemos aplicar la transformación si todas las características del *dataset* son de idéntico tipo y si a todas las características del *datasets* queremos aplicarles el mismo pre-procesamiento.\n","\n","Para poder aplicar una transformación a cada una de las características de nuestro *dataset* debemos utilizar el objeto [compose.ColumTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). Veamos cómo.\n"]},{"cell_type":"markdown","metadata":{"id":"U50y4dfa6eqm","colab_type":"text"},"source":["Lo primero que debemos hacer es importar el modulo *compose* de *sklearn*:"]},{"cell_type":"code","metadata":{"id":"f3m_RlRh4LAx","colab_type":"code","colab":{}},"source":["import sklearn.compose"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FCzoq4hd6pL3","colab_type":"text"},"source":["Supongamos ahora que tenemos el siguiente conjunto de datos:\n","\n","| id | nombre | altura | peso | sexo |\n","| :-: | :-: | :-: | :-: | :-: |\n","| 1 | Alice | 160 | 59 | mujer |\n","| 2 | Bob | 200 | 98 | hombre |\n","| 3 | Carl | 175 | 63 | hombre |\n","| 4 | Denna | 150 | 47 | mujer |\n","\n","Como observamos, tenemos columnas con datos discretos (*nombre* y *sexo*) y otras con datos continuos (*altura* y *peso*). Queremos aplicar la siguiente transformación:\n","\n","- Descartar la columna *id*.\n","- Mantener inalterada la columna *nombre*.\n","- Aplicar estandarización a la columna *altura*.\n","- Re-escalar la columna *peso* a escala 0, 1.\n","- Aplicar *one-hot-encoding* a la columna *sexo*.\n","\n","El parámetro *transformers* permite determinar el tipo de transformación aplicado a cada columna. Para ello, mediante un array de tuplas, indicaremos:\n","\n","- El nombre de la transformación.\n","- La transformación que queremos realizar o *'drop'* si queremos descartar la columna o *'passthroug'* si no queremos alterar la columna.\n","- El índice de la columna o los índices de las columnas que se ven afectadas."]},{"cell_type":"markdown","metadata":{"id":"xGq2BobR8nmj","colab_type":"text"},"source":["Definimos, por tanto, nuestro *dataset*:"]},{"cell_type":"code","metadata":{"id":"EJU3GvTk2NUc","colab_type":"code","colab":{}},"source":["X = np.array([[1, \"Alice\", 160, 59, 'mujer'],\n","              [2, \"Bob\",   200, 98, 'hombre'],\n","              [3, \"Carl\",  175, 63, 'hombre'],\n","              [4, \"Denna\", 150, 47, 'mujer']]);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-iO6XKup8tu7","colab_type":"text"},"source":["Definimos la transformación:"]},{"cell_type":"code","metadata":{"id":"MyE2rW3f8vzP","colab_type":"code","colab":{}},"source":["column_transformer = sklearn.compose.ColumnTransformer(transformers=[\n","    (\"drop\", \"drop\", [0]),\n","    (\"passthrough\", \"passthrough\", [1]),\n","    (\"scale\", sklearn.preprocessing.StandardScaler(), [2]),\n","    (\"min-max\", sklearn.preprocessing.MinMaxScaler(), [3]),\n","    (\"one-hot\", sklearn.preprocessing.OneHotEncoder(), [4])\n","]);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T-mdEHy-8xC0","colab_type":"text"},"source":["Aplicamos la transformación:"]},{"cell_type":"code","metadata":{"id":"PH-ZOEtN8yvS","colab_type":"code","outputId":"b06e1b6f-84f7-4fbb-ce25-ee2a15c7bcf8","executionInfo":{"status":"ok","timestamp":1564407460885,"user_tz":-120,"elapsed":574,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["X_transform = column_transformer.fit_transform(X)\n","print(X_transform)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[['Alice' '-0.5973509804399748' '0.23529411764705876' '0.0' '1.0']\n"," ['Bob' '1.5265636166799355' '1.0' '1.0' '0.0']\n"," ['Carl' '0.1991169934799916' '0.31372549019607854' '1.0' '0.0']\n"," ['Denna' '-1.1283296297199523' '0.0' '0.0' '1.0']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6UZV-7aMargx","colab_type":"text"},"source":["## Textos\n","\n","Cuando los datos de entrada de nuestro algoritmo de *machine learning* son textos, tenemos un problema similar al que sucedía cuando los datos de entrada eran variables discretas: los algoritmos de *machine learning* funcionan a partir de operaciones matemáticas que no pueden operar con textos. No existe denidida ninguna operación matemática que puede combinar las palabras *\"hola\"* y *\"adiós\"*. Por tanto, para poder emplear textos definidos en lenguaje natural, independientemente del idioma empleado, necesitamos transformar esos textos en vectores numéricos que los representen.\n","\n","La técnica más conocida para hacer esta transformación se denomina ***bag of words***. Veamos cómo funciona con un ejemplo. Supongamos que tenemos el siguiente texto:\n","\n","> \"*El miedo es el camino hacia el lado oscuro, el miedo lleva a la ira, la ira lleva al odio, el odio lleva al sufrimiento, el sufrimiento al lado oscuro.*\"\n","\n","El primer paso que debemos realizar es el que conocemos como ***tokenizacion***, que consiste en trasformar el texto anterior en un array de palabras. Es decir, vamos a separar cada una de las palabras que conforman la frase anterior empleando como separador los espacios y signos de puntuación. Por tanto, obtendríamos la siguiente lista de *tokens*:\n","\n","\n","`['El', 'miedo', 'es', 'el', 'camino', 'hacia', 'el', 'lado', 'oscuro', 'el', 'miedo', 'lleva', 'a', 'la', 'ira', 'la', 'ira', 'lleva', 'al', 'odio', 'el', 'odio', 'lleva', 'al', 'sufrimiento', 'el', 'sufrimiento', 'al', 'lado', 'oscuro']`\n","\n","Ahora vamos a homogeneizar nuestro texto transformando todas las palabras a minúsculas:\n","\n","`['el', 'miedo', 'es', 'el', 'camino', 'hacia', 'el', 'lado', 'oscuro', 'el', 'miedo', 'lleva', 'a', 'la', 'ira', 'la', 'ira', 'lleva', 'al', 'odio', 'el', 'odio', 'lleva', 'al', 'sufrimiento', 'el', 'sufrimiento', 'al', 'lado', 'oscuro']`\n","\n","A partir de la lista anterior podemos construir un diccionario que contiene todas las palabras que están definidas en nuestro vocabulario. Entendemos como \"nuestro vocabulario\" a las palabras que aparecen en los textos que estamos analizando. El algoritmo de *machine learning* no necesita conocer si esa palabra pertenece o no al Diccionario de la Real Academia de la Lengua Española (o su equivalente en otros idiomas). Así pues, analizando los *tokens* anteriores construiremos el siguiente diccionario:\n","\n","`['el', 'miedo', 'es', 'camino', 'hacia', 'lado', 'oscuro', 'lleva', 'a', 'la', 'ira', 'odio', 'sufrimiento']`\n","\n","Por último, transformar el texto original en un vector numérico de tal forma que las posiciones del vector representan las posiciones de las palabras del diccionario y los valores del vector representa el número de apariciones de la palabra del diccionario en el texto analizado. Nuestro texto quedaría, por tanto, definido por el siguiente vector:\n","\n","`[6, 2, 1, 1, 1, 2, 2, 3, 1, 2, 2, 2, 2]`\n","\n","Analizándolo vemos que la palabra *'el'* se repite 6 veces, la palabra *'miedo'* 2, la palabra *'es'* 1, y así sucesivamente."]},{"cell_type":"markdown","metadata":{"id":"SIVa22lpUQMj","colab_type":"text"},"source":["*sklearn* nos da soporte para transformar textos en su presentanción mediante *bag of words*. Para ello emplearemos el objeto [feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) del siguiente modo."]},{"cell_type":"markdown","metadata":{"id":"edPYS-tQU1w8","colab_type":"text"},"source":["Primero, importamos el módulo y definimos nuestro objeto:"]},{"cell_type":"code","metadata":{"id":"O4_vzJSlU68R","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vectorizer = CountVectorizer()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_qdW0KWU_6l","colab_type":"text"},"source":["A continuación, aplicamos la transformación a nuestro texto de ejemplo. En este caso, *CountVectorizer* está esperando una secuencia de textos, a la que se denomina ***corpus***, por lo que debemos declarar nuestro texto dentro de un array de 1 elemento."]},{"cell_type":"code","metadata":{"id":"rrkeG7G2VHRx","colab_type":"code","outputId":"6da8dd95-24ea-4398-9f11-0e3fb1e111c8","executionInfo":{"status":"ok","timestamp":1564481162911,"user_tz":-120,"elapsed":540,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["corpus = [\n","    \"El miedo es el camino hacia el lado oscuro, el miedo lleva a la ira, la ira lleva al odio, el odio lleva al sufrimiento, el sufrimiento al lado oscuro.\"\n","]\n","\n","X = count_vectorizer.fit_transform(corpus)\n","\n","print(X.toarray())\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[3 1 6 1 1 2 2 2 3 2 2 2 2]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g2JPvuPCVEs9","colab_type":"text"},"source":["Podemos analizar qué palabra corresponden a cada posición de este array:"]},{"cell_type":"code","metadata":{"id":"uUXOKN5eWP9H","colab_type":"code","outputId":"155769ac-2ae9-4f54-8e45-04309bd52021","executionInfo":{"status":"ok","timestamp":1564481258385,"user_tz":-120,"elapsed":547,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(count_vectorizer.get_feature_names())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['al', 'camino', 'el', 'es', 'hacia', 'ira', 'la', 'lado', 'lleva', 'miedo', 'odio', 'oscuro', 'sufrimiento']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qo-CyYypWkP6","colab_type":"text"},"source":["Llegados a este punto es importante resaltar que *sklearn* almacena los vectores en una matriz dispersa que proporciona *NumPy*. Esto se debe a que, cuando trabajamos con textos, lo normal es que el *corpus* disponga de cientos o miles textos (llamados ***documentos***) sobre los cuales se construye el *diccionario*. Como es lógico, no todos los *documentos* contienen todas las plabras del *diccionario*, por lo que, habitualmente, los vectores de *bag of words* están repletos de valores 0. Esta es una información irrelevante que desperdicia gran cantidad de espacio en memoria, por lo que se alamcena utilizando otro tipo de estrucutras de datos que sólo guarda qué palabras pertenecen a cada documento."]},{"cell_type":"markdown","metadata":{"id":"0lJZX9NtXZZz","colab_type":"text"},"source":["Si analizamos con detalle el vector del texto de ejemplo vemos que las palabras con mayor número de repeticiones son *'el'*, con 6 repeticiones, *'al'*, con 3 repeticiones y *'lleva'* con 3 repeticiones. Esto es un gran problema, puesto que las 2 primeras no aportan ningún significado semántico al texto y provocará que nuestro algoritmo de *machine learning* no sea capaz de extraer conocimiento de *corpus* de documentos.\n","\n","Para resolver este problema se filtran estas palabras del *corpus* antes de construir el diccionario. A estas palabras, que en realidad son todos los artículos, preposiciones, etc., se las conoce como ***stop words***, y existen listas en diferentes idiomas para realizar este filtrado. Veamos cómo."]},{"cell_type":"markdown","metadata":{"id":"Esss68akZIBB","colab_type":"text"},"source":["Cuando trabajamos con texto es extremadamente útil conocer la librería NLTK ([thttps://www.nltk.org/](https://www.nltk.org/)), ya que incorpora infinidad de herramientas para la manipulación de textos en lenguaje natural. Entre otras funcionalidades, incorpora una lista de *stop words* en diferentes idiomas. Carguemos las *stop words* en español:"]},{"cell_type":"code","metadata":{"id":"w_1WHVvBZn-T","colab_type":"code","outputId":"0c89c6ae-5219-4698-8f09-7398fbfc1582","executionInfo":{"status":"ok","timestamp":1564482422606,"user_tz":-120,"elapsed":7665,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":799}},"source":["import nltk\n","nltk.download(\"popular\") # required to download the stopwords lists\n","\n","from nltk.corpus import stopwords\n","\n","spanish_stopwords = stopwords.words('spanish')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UrRIvlB-bAy5","colab_type":"text"},"source":["Las visualizamos:"]},{"cell_type":"code","metadata":{"id":"L8DUofULa97f","colab_type":"code","outputId":"12541bec-ad0c-43aa-b7d5-c3bc1d738208","executionInfo":{"status":"ok","timestamp":1564482470721,"user_tz":-120,"elapsed":594,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(spanish_stopwords)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lSdQ-npwaDZu","colab_type":"text"},"source":["Ahora, podemos crearnos una instancia de nuestro *CountVectorizer* que incluya esta lista de *stop words* utilizando el parámetro *stop_words* de su constructor:"]},{"cell_type":"code","metadata":{"id":"nppUdeMOaUaP","colab_type":"code","colab":{}},"source":["count_vectorizer = CountVectorizer(stop_words = spanish_stopwords)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qo2wEvLvbH4U","colab_type":"text"},"source":["Y repetimos el proceso anterior:"]},{"cell_type":"code","metadata":{"id":"3VPrbGVubLCj","colab_type":"code","outputId":"9119ff6d-ad42-4f63-cf2a-8176dad1a9fd","executionInfo":{"status":"ok","timestamp":1564482521497,"user_tz":-120,"elapsed":541,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["corpus = [\n","    \"El miedo es el camino hacia el lado oscuro, el miedo lleva a la ira, la ira lleva al odio, el odio lleva al sufrimiento, el sufrimiento al lado oscuro.\"\n","]\n","\n","X = count_vectorizer.fit_transform(corpus)\n","\n","print(X.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1 1 2 2 3 2 2 2 2]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Liut1FG-bNqt","colab_type":"text"},"source":["Si analizamos las palabras del diccionario vemos que no hay ni rastro de las *stop words*:"]},{"cell_type":"code","metadata":{"id":"r6Lcxoi0bUvy","colab_type":"code","outputId":"e9acf74a-2538-4463-d872-ecc16c137dd6","executionInfo":{"status":"ok","timestamp":1564482558404,"user_tz":-120,"elapsed":551,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(count_vectorizer.get_feature_names())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['camino', 'hacia', 'ira', 'lado', 'lleva', 'miedo', 'odio', 'oscuro', 'sufrimiento']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vvHN2EQ1beBz","colab_type":"text"},"source":["Aunque llegados a este punto ya tenemos una buena representación vectorial de nuestros textos, aún existe un problema: la representación creada no está normalizada. Esta no-normalización plantea básicamente dos problemas:\n","\n","- A nivel de *documento* (las filas de nuestra matriz de datos) cada uno lleva una escala completamente libre y hace que sea imposible compararlos entre si. Un texto más largo tendrá contadores con valores más grandes que un texto más corto. En un ejemplo llevado al extremo podemos comparar un tweet con la noticia que enlaza ese tweet. Ambos documentos versarán sobre el mismo tema, pero no pueden compararse debido a la volumetría de ambos.\n","\n","- A nivel de *palabras* es complicado comparar cuáles son más relevante y cuáles menos para un *corpus* concreto. Ya hemos eliminado las *stop words*, pero, en función del sesgo de nuestro *corpus* existen palabras que no aportan demasiada información y, por lo tanto, su incidencia en nuestro algoritmo de *machine learning* debería ser menor. Por ejemplo, imaginemos que tenemos un *corpus* de documentos hablando únicamente de los equipos de la Liga de Fútbol Profesional. En este corpus la palabra *'fútbol'* es completamente irrelevante, ya que todos los documentos hablan de ella. Por contra, palabras como *'lesión'* o *'fichaje'* son muy relevante porque permiten subclasificar los documentos. Sin embargo, si nuestro *corpus* está formado por noticias de todo tipo, la palabra *'fútbol'* es muy relevante ya que identifica un tipo de noticias.\n","\n","Para resolver este problema se emplea una normalización denominada **tf-idf** (*term-frecuency times inverse document-frecuency*). Ésta viene definida por la siguiente ecuación:\n","\n","$\\textrm{tf-idf}(t, d) = tf(t, d) \\times idf(t)$\n","\n","siendo $tf(t, d)$ el número de veces que aparece el término (palabra) $t$ en el documento $d$ y definiéndose $idf(t)$ como:\n","\n","$idf(t) = log \\frac{1 + n}{1 + df(t)} + 1$\n","\n","siendo $n$ el número de documentos de nuestro *corpus* y $d(t)$ el número de documentos en los que aparece el término $t$.\n","\n","Posteriormente, los vectores son normalizados a nivel de documento (el modulo del vector de cada documento vale 1).\n","\n","Analizando estas ecuaciones observamos que *tf-idf* observamos que, aquellas palabras que tengan menos frecuencia de aparición serán más relevante que aquellas que aparezcan en más documentos."]},{"cell_type":"markdown","metadata":{"id":"vhzX1CM2fp5c","colab_type":"text"},"source":["Esta transformación puede realizarse mediante el objeto [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)."]},{"cell_type":"markdown","metadata":{"id":"6Sw2diB3hAnw","colab_type":"text"},"source":["Para ver su funcionamiento, vamos a construir un *corpus* con varios documentos:"]},{"cell_type":"code","metadata":{"id":"V7_qyymMhBKn","colab_type":"code","colab":{}},"source":["corpus = [\n","    \"Este es el primer documento.\",\n","    \"Este documento es el segundo documento.\",\n","    \"Y este es el tercero\",\n","    \"¿Es este el primer documento? No.\"\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itkxEwYxhSYe","colab_type":"text"},"source":["Aplicando el proceso anterior obtenemos los siguientes vectores:"]},{"cell_type":"code","metadata":{"id":"Doff7DYphYrS","colab_type":"code","outputId":"b705012f-9946-4e2e-9c74-e9a43de95db5","executionInfo":{"status":"ok","timestamp":1564484303928,"user_tz":-120,"elapsed":526,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["X = count_vectorizer.fit_transform(corpus)\n","\n","print(X.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1 1 0 0]\n"," [2 0 1 0]\n"," [0 0 0 1]\n"," [1 1 0 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FqyW9Uu4iCRw","colab_type":"text"},"source":["Veamos como aplicar ahora *tf-idf*. Cargamos el módulo y creamos el objeto:"]},{"cell_type":"code","metadata":{"id":"K5gY-SmgguQY","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TtqVR4kiiSe-","colab_type":"text"},"source":["Realizamos la transformación:"]},{"cell_type":"code","metadata":{"id":"_3ELNstaiUOO","colab_type":"code","colab":{}},"source":["counts = X.toarray()\n","X_transformed = tfidf_transformer.fit_transform(counts)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rnsUWNcpidic","colab_type":"text"},"source":["Analizamos el resultado:"]},{"cell_type":"code","metadata":{"id":"F5SJFW4Bie5C","colab_type":"code","outputId":"c88841fe-b481-444c-fcc4-9ee99f25d739","executionInfo":{"status":"ok","timestamp":1564484539369,"user_tz":-120,"elapsed":520,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(X_transformed.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.62922751 0.77722116 0.         0.        ]\n"," [0.78722298 0.         0.61666846 0.        ]\n"," [0.         0.         0.         1.        ]\n"," [0.62922751 0.77722116 0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2f__dn2qatM_","colab_type":"text"},"source":["## Imágenes\n","\n","Cuando trabajamos con imágenes vuelva a surgir la necesidad de transformar los datos de la imagen en una interpretación matemática válida a ingerir por los algoritmos de *machine learning*. A diferencia de lo que sucedía con los datos discretos y los textos, en este caso, el tratamiento es más trivial debido a la propia naturaleza de las imágenes en un ordenador.\n","\n","Una imagen, por compleja que sea, está siempre compuesta por pixeles. Los píxeles se agrupan formando una matriz que define el tamaño de la imagen. Cuantas más filas y columnas tenga esa matriz, más resolución tendrá la imagen. A continuación, se muestra una imagen de 55x55 píxeles:\n","\n","![Imagen pixelada](https://drive.google.com/uc?export=view&id=1B6Z7foIjSYXdu2INvBlmxht3ou65h_N2)\n","\n","Cuando se trabaja con imágenes, generalmente, se distinguen dos tipos de imágenes: las imágenes en blanco y negro (o escala de grises) y las imágenes a color (o RGB). Las primeras definen el color de cada uno de sus pixeles mediante un número de 0 a 255 que indica la cantidad de blanco que posee dicho píxel: el valor 0 se corresponden con la ausencia de blanco (negro) y el valor 255 se corresponde con el negro total. Los valores intermedios permiten definir los diferentes tonos de gris existentes. Esta sería la imagen anterior en escala de grises:\n","\n","![Imagen pixelada en escala de grises](https://drive.google.com/uc?export=view&id=1oLBS16EEwI5opnKiXCXkaPfG3vqkWsP0)\n","\n","Por su parte, las segundas consiguen generar el color mediante la adición de tres componentes lumínicos: el componente rojo, el componente azul y el componente verde. Es por esto por lo que a estas imágenes se las conoce como RGB (del inglés, *Red-Green-Blue*). Mediante la combinación de estos tres componentes puede conseguirse representar cualquier color. Por tanto, cuando definamos una imagen en color, cada píxel vendrá definido por tres valores: la cantidad de rojo, la cantidad de verde y la cantidad de azul de píxel. Todos ellos, al igual que las imágenes en escala de grises, vendrán definidos con un valor numérico de 0 a 255, siendo 0 la ausencia del componente y 255 la totalidad del componente. Las siguientes imágenes muestran la imagen original separando cada uno de sus canales de RGB:\n","\n","![Imagen pixelada separa por componentes RGB](https://drive.google.com/uc?export=view&id=1lSeyP8Otwlzv-MdnjVbC-LW_HWBLAGNM)\n","\n","Debido esta representación de las imágenes, para los algoritmos de *machine learning* es bastante sencillo emplearlas como datos de entrada. Aunque, evidentemente, no es sencillo obtener información relevante de las mismas. Cuando queramos emplear imágenes en nuestro algoritmo simplemente debemos transformar las imágenes a su representación matricial."]},{"cell_type":"markdown","metadata":{"id":"9fq_iuNTgaml","colab_type":"text"},"source":["Para facilitar esta tarea disponemos de la librería *sklear-image* ([https://scikit-image.org/](https://scikit-image.org/)). Con ella nos resulta sencillo convertir imágenes en escala de grises o a color en arrays de *NumPy* que pueden ser pasados como datos a un algoritmo de *machine learning*.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3bJSeGL1ZoQF","colab_type":"text"},"source":["Para poder cargar imágenes desde ficheros o direcciones de internet debemos importar el módulo *io* de dicha librería:"]},{"cell_type":"code","metadata":{"id":"lndQYVc1hSar","colab_type":"code","colab":{}},"source":["from skimage import io"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ioo6X4lEjl5O","colab_type":"text"},"source":["A continuación, vamos a cargar la siguiente imagen de 15x15 píxeles y ver su representación:\n","\n","![Número dos](https://drive.google.com/uc?export=view&id=1f5YvGRb6Eoq1czyHWzGc2L86pNa13tWB)\n"]},{"cell_type":"code","metadata":{"id":"1GyDt9zTj3pk","colab_type":"code","outputId":"1dd856d2-24e3-4bcd-c44a-79f3b7791363","executionInfo":{"status":"ok","timestamp":1564501592198,"user_tz":-120,"elapsed":530,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":305}},"source":["image = io.imread('https://drive.google.com/uc?export=view&id=1pB_Ju0xyV7s2usfpiEyGF_bOWSSn9Ze7')\n","\n","print(image.shape)\n","print(image)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(15, 15)\n","[[255 253 255 254 255 255 253 255 255 254 255 253 253 255 254]\n"," [253 255 252 255 255 135  18   2   1  50 186 255 255 252 255]\n"," [255 249 255 255 119   2 135 253 238  69   1 172 253 253 254]\n"," [253 255 254 220   0 134 255 255 255 253  36  85 255 253 255]\n"," [254 255 252 150   3 220 254 255 255 255 100  31 255 255 255]\n"," [255 250 255 254 255 255 254 254 253 255  86  52 253 254 255]\n"," [254 255 255 254 254 255 254 255 255 180   0 141 251 255 253]\n"," [255 255 251 255 253 255 254 255 134   2 104 251 255 255 255]\n"," [254 254 255 252 255 255 201  51   0 135 255 255 253 255 251]\n"," [255 255 252 255 254 134   0  84 236 255 252 254 254 254 255]\n"," [253 255 255 248 142  16 187 255 255 255 254 255 255 253 254]\n"," [254 255 254 189   0 189 254 255 253 255 255 250 253 255 255]\n"," [253 254 255 102  50 255 254 252 254 255 251 255 255 255 252]\n"," [255 254 252  70   1   0   1   3   4   0   0  70 253 255 255]\n"," [254 254 255 254 255 254 254 254 254 255 255 254 255 250 255]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JNw9Re2uj-Fl","colab_type":"text"},"source":["Observamos que se ha construido una matriz de 15x15 píxeles es escala de grises y que, los píxeles que conforman el número 2, toman valores más bajos por ser de color negro."]},{"cell_type":"markdown","metadata":{"id":"OMdZz2TEkyCz","colab_type":"text"},"source":["Ahora bien, si en lugar de cargar la imagen anterior cargamos la siguiente imagen RGB de 15x15 píxeles:\n","\n","![Número ocho](https://drive.google.com/uc?export=view&id=1Wk5rLjzPmFlcvzFVfe0hT502mhboJnj1)\n"]},{"cell_type":"code","metadata":{"id":"2wXKfJDilIlJ","colab_type":"code","outputId":"27384f8f-f87e-466f-b2c3-755cf5d2f3de","executionInfo":{"status":"ok","timestamp":1564502240136,"user_tz":-120,"elapsed":728,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"}},"colab":{"base_uri":"https://localhost:8080/","height":845}},"source":["image = io.imread('https://drive.google.com/uc?export=view&id=10o0eMYMCVLRQT4ZLnNKW9gTzhylbA1hP')\n","\n","print(image.shape)\n","\n","print(image[:,:,0]) # red\n","\n","print(image[:,:,1]) # green\n","\n","print(image[:,:,2]) # blue"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(15, 15, 3)\n","[[  4   4   2   0   0   7   0   0   1   0   2   0   0  11   0]\n"," [  3   0   0   3   4 179 244 255 255 215 109   1   2   0   5]\n"," [  0   6   0   3 205 246 112   2   0 179 255  87   0   0   0]\n"," [  0   0   8  66 250 119   0   0   4   4 228 150   2   0   4]\n"," [  2   0   4  51 255 124   0  10   0   3 242 158   1   0   0]\n"," [  0  10   0   0 182 232 107   9   0 179 240  75   0   3   0]\n"," [  1   0   2   6  38 224 253 255 253 255 132   2   0   1   2]\n"," [  0   1   0   0 224 216  82   0   0 185 250 119  15   0   0]\n"," [ 11   0   0 140 255  67   2   0   0   0 190 231   1   0   0]\n"," [  4   0   1 166 213   8   0   4   7   0 145 255  64   2   0]\n"," [  4   0   0 171 215   0   7   0   0   0 139 255  39   8   0]\n"," [  0   0  10 135 255  88   1   0   1   8 203 226   0   0   8]\n"," [  0   3   1   0 205 225 145   0   1 183 248 104   5   0   0]\n"," [  5   0   0   0   4 171 241 255 254 209 110   4   0   6   0]\n"," [  5   0   0   0   0   3   0   0   7   0   0  10   0   4   0]]\n","[[250 255 253 255 255 253 255 255 255 255 255 255 249 251 255]\n"," [255 252 255 251 255 131   1   3   1  90 203 255 255 255 255]\n"," [255 254 254 248 111  66 206 252 255 153   1 220 255 254 254]\n"," [255 255 252 237   0 198 255 255 255 248  61 172 252 255 255]\n"," [255 255 249 244   0 196 255 251 253 254  53 166 251 255 255]\n"," [255 253 250 255 139  69 210 249 255 140  42 232 255 252 255]\n"," [255 251 255 255 238  83   0   3   0   5 180 255 255 249 255]\n"," [251 255 255 248  95  92 218 255 255 134   6 193 248 255 255]\n"," [247 255 255 179   6 232 252 255 255 255 135  63 255 255 252]\n"," [253 255 255 155  84 254 255 254 250 254 180   4 225 255 255]\n"," [255 254 255 154  86 253 253 253 255 254 179   4 241 254 255]\n"," [255 254 255 185   4 221 251 255 254 250 106  87 255 252 252]\n"," [255 255 251 253 110  67 184 254 255 124  10 212 255 253 255]\n"," [250 254 255 255 255 150   6   2   1 111 204 244 251 255 255]\n"," [252 255 255 250 254 255 254 252 250 255 255 254 255 255 254]]\n","[[ 0  0  0  0  0  4  0  0  0  0  2  0  8  6  0]\n"," [ 1  0  1  4  0  3  0  5  3 10  0  0  4 13  0]\n"," [ 0 10  0  0  0  0  0  4  0  0  5  0  5  0  0]\n"," [ 4  0 16  0  1 11  6  1  9  0  8  0  4  0  0]\n"," [ 4  0  6  0  4 11  2  0  0  0  0  0  0  6  0]\n"," [ 0  3  0  0  8  0  0  0  0  0  5  1  0 12  0]\n"," [ 0  0  0  9  5  3  3  0  0  5  0  7  0  3 11]\n"," [ 6  2  2  0  0  0 14  0  7  6  0  0  6  5  0]\n"," [17  0  0  8  0  8  0  0  0  3  6  0  0  0  2]\n"," [11  0  2  3  1  0  6  0  0  3  0  0  8  0  2]\n"," [ 2  0  0 12  2  0  2  0  0 10  0  0  5  3  5]\n"," [ 0  4  2  0  2 10  3  3  5  6  1  0  4  1  0]\n"," [ 0  1  0  0  4  0  0  2  0  0  0  0  3  5  6]\n"," [ 7 10  0  0  0  5 13  0  0  4  2 11  0  4  0]\n"," [ 0  0  6  0  0  6 12  9 10  0  0  0  1  0  0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9b-MQWyVmgbY","colab_type":"text"},"source":["Se observan perfectamente los tres componentes de la imagen que definen el número 8 y el fondo de esta."]},{"cell_type":"markdown","metadata":{"id":"11gXRi0LmsFB","colab_type":"text"},"source":["Aunque en este bloque hemos visto una muestra muy simple de cómo podemos trabajar con imágenes, el uso de imágenes en *machine learning* requiere de mucho más procesamiento. Cuando tratamos con imágenes debemos intentar que todas ellas tengan el mismo tamaño para poder ser comparadas entre sí. Además, es importante que propiedades de las imágenes como la saturación, el balance de blancos o el contraste sean homogéneos para evitar sesgos en el conjunto de datos. Todos estos ajustes pueden realizarse con la librería *sklearn-images*."]},{"cell_type":"markdown","metadata":{"id":"zUjp2iXcaukX","colab_type":"text"},"source":["---\n","\n","*Este documento ha sido desarrollado por **Fernando Ortega**. Dpto. Sistemas Informáticos, ETSI de Sistemas Informáticos, Universidad Politécnica de Madrid.*\n","\n","*Última actualización: Julio de 2019*"]},{"cell_type":"markdown","metadata":{"id":"BGu3wCH2a4lI","colab_type":"text"},"source":["<img src=\"https://drive.google.com/uc?export=view&id=1QuQDHyH_yrRbNt6sGzoZ8YcvFGEGlnWZ\" alt=\"CC BY-NC\">"]}]}