{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggq3XvB6GJDT"
   },
   "source": [
    "# Preparación y pre-procesamiento de datos\n",
    "\n",
    "Los datos son la columna vertebral del *machine learning*. No puede existir aprendizaje si no existen **suficientes datos**, puesto que los algoritmos de *machine learning* no son capaces de generalizar los resultados: sólo aprenden un determinado patrón de todos los existentes. Esto se debe a la propia naturaleza del aprendizaje. Imaginemos, por ejemplo, que un niño pequeño ha visto únicamente 5 vehículos a motor en toda su vida. Es altamente probable que el niño sea capaza de identificar qué es un vehículo a motor, pero, por el contrario, le resultaría imposible diferenciar entre los distintos tipos de vehículos a motor que existen (coche, camiones, autobuses, motocicletas, etc.). Disponer de la cantidad de datos adecuada es, por tanto, esencial para los científicos de datos.\n",
    "\n",
    "En cualquier caso, existen situaciones en las que, teniendo una gran cantidad de datos, estos no son adecuados para el *machine learning*. Si los datos no son representativos o si los datos están **sesgados** (*biased*, en inglés) no es posible aprender correctamente a partir de los mismos. Ilustremos este problema con un par de ejemplos:\n",
    "\n",
    "- No es posible predecir de forma precisa qué canción le interesará a una persona mayor en un servicio de música bajo demanda si todos los demás usuarios del servicio son personas jóvenes. En este caso, los datos están sesgados hacia las preferencias de las personas jóvenes. Sin embargo, para este caso particular, los algoritmos de *machine learning* serán capaces de realizar recomendaciones muy certeras a las personas jóvenes. \n",
    "\n",
    "- Si le proporcionamos a nuestro algoritmo suficientes imágenes de perros y gatos, conseguiremos ser capaces de distinguir entre estos dos animales, pero será imposible que ese mismo algoritmo diference un león de un elefante. Del mismo modo, si todas las imágenes de perros que le pasamos son de *pastores alemanes*, el algoritmo tendrá grandes dificultades para reconocer que un *caniche* o un *carlino* son perros. En todos estos casos, los datos están sesgados hacia un tipo concreto de animal, bien sea los perros y gatos frente a otros animales o los *pastores alemanes* frente a otras razas de perro.\n",
    "\n",
    "Del mismo modo, aún teniendo suficientes datos y no estando estos sesgados, es posible encontrar datos que no tienen la **calidad** requerida para que los algoritmos de *machine learning* sean capaces de aprende. Algunos casos típicos de datos con poca calidad son:\n",
    "\n",
    "- Datos incompletos: personas que no rellenan el campo edad, códigos postales incompletos en formularios, etc.\n",
    "\n",
    "- Anomalías (*outlaiers*, en inglés): datos incorrectos provenientes de errores humanos, sensores averiados, fallos informáticos, etc.\n",
    "\n",
    "- Inconsistencias o datos incorrectos: direcciones de correo electrónico sin el símbolo @, direcciones postales sin el número de la calle, etc.\n",
    "\n",
    "Por último, es importante reseñar, que datos irrelevantes pueden arruinar el proceso del *machine learning*. Para lograr unos resultados relevantes, es fundamental disponer de unos datos **relevantes**. Es imposible predecir la venta de pañales basándonos en la climatología, al igual que es imposible predecir una cardiopatía empleando datos psicológicos.\n",
    "\n",
    "Resumiendo, para lograr que un algoritmo de *machine learning* sea capaz de extraer conocimiento de un conjunto de datos es fundamental que los datos sean: suficientes, no-sesgados, de calidad y representativos. La ausencia de cualquiera de estas características desembocará en la imposibilidad de aprender patrones generalistas de los datos analizados. Con frecuencia, al *machine learning* se le conoce como *learning from data* (aprendizaje desde los datos) ya que no se posible extraer conocimiento si este no existe en los propios datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "remUZSMPTM9u"
   },
   "source": [
    "## Tipos de datos\n",
    "\n",
    "Cuando se habla de tipos de datos en el ámbito del *machine learning* es habitual realizar una distinción entre **datos estructurados** y **datos no estructurados**. Los primeros, hacen referencia a aquellos datos en los que es sencillo determinar el significado intrínseco de un dato. Por ejemplo, cuando trabajamos con datos biométricos, si un sensor nos proporcional las pulsaciones por minuto de una persona, se sabe de forma univoca a qué hace referencia ese dato y que si está por encima de 120 o por debajo de 40 es un valor anómalo. Del mismo modo, si un usuario indica que le gusta un video de YouTube podemos estar seguro de que al usuario le interesa dicho video. Por el contrario, en los datos no estructurados es extremadamente complejo conocer el significado del dato que se está tratando. En el ejemplo anterior, si el usuario en lugar de indicar que le ha gustado un video escribe una reseña positiva del mismo, para un algoritmo de *machine learning* es sumamente complicado conocer si la reseña es positiva o negativa debido a la naturaleza desestructurada del lenguaje natural. Además del texto, las imágenes, los vídeos o los sonidos son tipos de datos que suelen considerarse como no estructurados.\n",
    "\n",
    "En los inicios del *machine learning*, la inmensa mayoría de los algoritmos eran capaces de trabajar únicamente con datos estructurados. Sin embargo, en los últimos años se han producido enormes progresos en el tratamiento de datos no estructurados, gracias, principalmente, al auge del *Deep Learning*.\n",
    "\n",
    "La siguiente imagen muestra la identificación de objetos a partir de una fotografía de la calle:\n",
    "\n",
    "![Reconocimiento de objetos](https://drive.google.com/uc?export=view&id=1FWA0TmH9yIQSgpdt7KGbN2_sCfIC4yKl)\n",
    "\n",
    "El siguiente fragmento del texto es una reseña de una cerveza escrita por un algoritmo de *machine learning* (que nunca ha probado cerveza):\n",
    "\n",
    "> *The smell is creamy, malty and woody, not much presence. The taste is dark fruits, and floral hops before its a strong destroy from the mouth as it warms up.*\n",
    "\n",
    "El siguiente vídeo muestra una canción completamente generada por un ordenador:\n",
    "\n",
    "[![Música generada por computador](https://img.youtube.com/vi/Ir_AFDKOc-I/0.jpg)](https://www.youtube.com/watch?v=Ir_AFDKOc-I)\n",
    "\n",
    "Independientemente del tipo de datos que empleemos y del algoritmo que les sea aplicado, cuando trabajamos en *machine learning* es fundamente realizar un pre-procesamiento de los datos. Este pre-procesamiento permite preparar los datos para que puedan ser interpretados por los algoritmos de *machine learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afJWL-DLMq6b"
   },
   "source": [
    "## Conceptos previos\n",
    "\n",
    "Antes de comenzar a analizar cada tipo de datos, debemos entender qué espera recibir como entrada un algoritmo de *machine learning*. Generalmente, los algoritmos de *machine learning* reciben como entrada una matriz (array) de datos en el que las filas representan las muestras tomadas (*samples*, en inglés) y las columnas representan las diferentes características (*features*, en inglés) de cada muestra.\n",
    "\n",
    "Veamos un ejemplo. La siguiente tabla muestra los datos de los salarios de jugadores de fútbol en función de la posición que ocupan y el club en el que juegan:\n",
    "\n",
    "| Club | Posición | Salario |\n",
    "| :------: | :------: | :------: |\n",
    "| Real Madrid | delantero | 14.000.000 € / anuales |\n",
    "| Barcelona | defensa | 9.000.000 € / anuales |\n",
    "| Atlético de Madrid | centrocampista | 7.000.000 € / anuales |\n",
    "| Valencia | portero | 3.000.000 € / anuales |\n",
    "\n",
    "Cada una de las filas, a excepción de la cabecera, son las muestras de las que se dispone. Estas muestras son la cantidad de observaciones que tenemos y van a definir el número de datos del que disponemos, en este caso, 4. El club, la posición y el salario son las características de cada muestra.\n",
    "\n",
    "Un problema típico de *machine learning* sería predecir el salario de un jugador en base a su posición y club. En ese caso, a las columnas de Club y Posición se las suele representar con $X$, siendo $x_1$ el Club y $x_2$ la Posición. Por el contrario, a la columna Salario, que es la que se quiere predecir, se la representa con $y$.\n",
    "\n",
    "Utilizando *NumPy* podemos almacenar los datos del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 1850,
     "status": "ok",
     "timestamp": 1600939564215,
     "user": {
      "displayName": "Fernando Ortega Requena",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64",
      "userId": "02003917424124170753"
     },
     "user_tz": -120
    },
    "id": "3DjPCtDOP8n4",
    "outputId": "0ede22db-7d32-4978-f645-df3f4ead2bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Real Madrid' 'delantero']\n",
      " ['Barcelona' 'defensa']\n",
      " ['Atlético de Madrid' 'centrocampista']\n",
      " ['Valencia' 'portero']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([['Real Madrid',        'delantero'     ],\n",
    "              ['Barcelona',          'defensa'       ],\n",
    "              ['Atlético de Madrid', 'centrocampista'],\n",
    "              ['Valencia',           'portero'       ]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1600939567091,
     "user": {
      "displayName": "Fernando Ortega Requena",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64",
      "userId": "02003917424124170753"
     },
     "user_tz": -120
    },
    "id": "SrSwPxwIQWXM",
    "outputId": "e9325066-a7bb-43f5-ccbe-ac172c923940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14000000  9000000  7000000  3000000]\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([14000000, 9000000, 7000000, 3000000])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dylb4v9vT9o"
   },
   "source": [
    "<hr>\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01. Introducción al preprocesamiento de datos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
