{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:06:35.705365Z",
     "start_time": "2021-05-30T11:06:13.204725Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "!apt install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Mediante la librería **Tensorflow** se pueden definir, entrenar y validar modelos de *Machine Learning*. Para ello hay que definir las entradas del modelo, sus variables (*parámetros*), el algoritmo de optimización y la función de coste. Si bien estas tareas no entrañan una dificultad elevada, el proceso es tedioso.\n",
    "\n",
    "**Keras**, un API de alto nivel para construir y entrenar modelos en Tensorflow nos ofrece las mismas posibilidades aunque a un nivel de abstracción mayor, facilitando la tarea.\n",
    "\n",
    "Para comprobar su funcionamiento, vamos a construir y entrenar algunos modelos para resolver problemas clásicos de aprendizaje supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de estudio: precio de las casas de Boston\n",
    "\n",
    "Este conjunto de datos es bien conocido entre la comunidad. Se encuentra disponible en la librería *Scikit-Learn*. El conjunto de datos cuenta con 13 *features* numéricas sin estandarizar. La variable de salida es el precio en miles de dólares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:06:36.855674Z",
     "start_time": "2021-05-30T11:06:35.709031Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle\n",
    "boston = load_boston()\n",
    "X, y = shuffle(boston.data, boston.target, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras** nos permite crear un modelo de aprendizaje de tipo red neuronal artificial. La red más sencilla es aquella compuesta por una capa de entrada, una de salida y varias capas ocultas. Para construir una red de estas características con **Keras** podemos usar el tipo `Sequential` e ir añadiendo las distintas capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:32:45.185719Z",
     "start_time": "2021-05-30T11:32:45.118883Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def generate_model_boston():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(X.shape[1],)),\n",
    "        tf.keras.layers.Dense(13, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='relu'),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "model_boston = generate_model_boston()\n",
    "print(model_boston.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademas de la función `.summary()` que nos devuelve una representación textual de la red para poder imprimirla por pantalla, tenemos la posibilidad de representar la red gráficamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:06:42.384648Z",
     "start_time": "2021-05-30T11:06:42.156643Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model_boston, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definido el modelo hay que compilarlo para poder usarlo. Además, en el momento de la compilación del modelo tenemos que definir la **función de coste**, el **optimizador** y las **métricas** que queremos calcular. A partir de este momento ya podemos entrenar el modelo mediante `fit()` alimentándolo con el conjunto de datos de entrenamiento. Observa que el propio método de ajuste `fit()` tiene un parámetro para realizar el split en entrenamiento y test. Date cuenta también que no estamos utilizando *batches*, sino el conjunto de datos completo en cada *epoch* del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:35:31.073393Z",
     "start_time": "2021-05-30T11:35:28.335091Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_boston = generate_model_boston()\n",
    "    model_boston.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.mean_squared_error,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "            tf.keras.metrics.RootMeanSquaredError()\n",
    "        ]\n",
    "    )\n",
    "    history = model_boston.fit(X, y, epochs=25, verbose=0, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería **Keras** también nos permite validar el modelo de forma muy sencilla, ejecutando `evaluate()`. Si bien en este ejemplo estamos usando el dataset completo, esta función sería útil si nos hubiésemos guardado un conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:14:36.607739Z",
     "start_time": "2021-05-30T11:14:36.461373Z"
    }
   },
   "outputs": [],
   "source": [
    "model_boston.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos usar la información histórica que devuelve el proceso de ajuste (método `fit()`) para representar gráficamente cómo evoluciona el valor de la función de pérdida así como otras métricas tanto para el conjunto de datos de entrenamiento como para el de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:36:26.075694Z",
     "start_time": "2021-05-30T11:36:25.097189Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='MSE (train)')\n",
    "plt.plot(history.history['val_loss'], label='MSE (validation)')\n",
    "plt.title('MSE for Boston houses (log-scale)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer lo mismo con el resto de métricas de evaluación que configuramos en el momento de compilación del modelo. En este caso representamos la evolución del MAE tanto para entrenamiento como para validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:35:42.250141Z",
     "start_time": "2021-05-30T11:35:41.775761Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_absolute_error'], label='MAE (train)')\n",
    "plt.plot(history.history['val_mean_absolute_error'], label='MAE (val)')\n",
    "plt.title('MAE for Boston houses (Log-scale)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento por lotes para reducir el *overfitting*\n",
    "Podemos observar que las curvas de entrenamiento y validación se cruzan alrededor del *epoch* 12, indicando un sobreajuste del modelo. Esto es normal con las **redes neuronales artificiales** y, además, no hemos utilizado entrenamiento por lotes. Esto provoca que en cada *epoch* se utilice el dataset completo para ajustar los pesos, dando lugar al **overfitting**. Vamos a estudiar cómo afecta el tamaño del lote al sobreajuste del modelo\n",
    "\n",
    "En primer lugar vamos a crear una función para dibujar la gráfica de la evolución de las métricas de calidad, así es más cómodo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:06:47.079342Z",
     "start_time": "2021-05-30T11:06:47.070831Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_val(history, metric='loss', y_scale='linear'):\n",
    "    plt.plot(history.history[metric], label=metric + ' (train)')\n",
    "    plt.plot(history.history['val_' + metric], label=metric + ' (val)')\n",
    "    plt.title(f'{metric} ({y_scale}-scale)')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.yscale(y_scale)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a reajustar el modelo utilizando entrenamiento por lotes. Para ello proporcionaremos el tamaño del lote al parámetro `batch_size` del método `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:38:34.311488Z",
     "start_time": "2021-05-30T11:38:30.273488Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_boston = generate_model_boston()\n",
    "    model_boston.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.mean_squared_error,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "            tf.keras.metrics.RootMeanSquaredError()\n",
    "        ]\n",
    "    )\n",
    "    history = model_boston.fit(X, y, epochs=25, batch_size=25, verbose=0, validation_split=0.2)\n",
    "plot_train_val(history, metric='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de estudio: MNIST\n",
    "\n",
    "El siguiente dataset que vamos a trabajar es el clásico MNIST de dígitos manuscritos. Es un problema de clasificación que consiste en identificar el dígito de 0 a 9 analizando la imagen escaneada.\n",
    "\n",
    "### API funcional vs. secuencial de Keras\n",
    "En el ejemplo anterior hemos utilizado la *API secuencial* de Keras para definir el modelo. Esta API obtiene su nombre de la forma de construir los modelos añadiendo capas, una detrás de otra. Aunque muy sencilla de utilizar, esta API tiene un gran inconveniente, y es que los modelos definidas con ella **siempre son modelos lineales**, es decir, una secuencia interconectada de capas desde la entrada a la salida.\n",
    "\n",
    "*Keras* nos ofrece una alternativa mucho más flexible que permite diseñar redes como un grafo acíclico dirigido: la **API funcional**. Vamos a ver cómo se usa esta API definiendo un modelo para resolver el problema de clasificación que se nos plantea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T12:05:58.024476Z",
     "start_time": "2021-05-30T12:05:57.686395Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def generate_model_mnist():\n",
    "    inputs = tf.keras.Input(shape=(784,)) # Las imágenes de MNIST son de 28x28 píxeles\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(inputs) # Primera capa densa conectada a 'inputs'\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x) # Segunda capa densa conectada a la primera 'x'\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(x) # Capa de salida con 10 neuronas, una por clase\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"mnist\")\n",
    "\n",
    "plot_model(generate_model_mnist(), show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, la forma de construirse que tiene el modelo nos permite topologías no lineales, varias entradas, etc. Procedemos ahora a cargar el *dataset*. Hacemos una normalización de los valores de las variables para pasar de un nivel de escala de grises de 0 a 255 a un valor continuo entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T12:19:16.587759Z",
     "start_time": "2021-05-30T12:19:15.142827Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya solo nos queda compilar y entrenar el modelo. Es interesante recalcar que, dado que el problema es de clasificación, tenemos que usar una función de pérdida adecuada para este tipo de problemas. En nuestro caso usaremos **entropía cruzada categórica**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T12:40:49.664447Z",
     "start_time": "2021-05-30T12:40:23.400777Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_mnist = generate_model_mnist()\n",
    "    model_mnist.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model_mnist.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T12:42:36.498512Z",
     "start_time": "2021-05-30T12:42:36.263107Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_val(history, metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el modelo con el conjunto de datos de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T12:42:23.405238Z",
     "start_time": "2021-05-30T12:42:23.128102Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    y_pred = np.argmax(model_mnist.predict(x_test, batch_size=64, verbose=0), axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de estudio: calidad de los vinos\n",
    "En esta ocasión tenemos que predecir la nota (0 a 10) que un experto catador otorgará a un vino en función de sus propiedades fisicoquímicas. El conjunto de datos está incluido en **Tensorflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:04:38.886938Z",
     "start_time": "2021-05-31T11:04:38.805179Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds, ds_info = tfds.load(\n",
    "    'wine_quality',\n",
    "    split='train',\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos a Pandas Dataframe para que sea más cómodo de procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:24:35.370435Z",
     "start_time": "2021-05-31T11:24:33.371470Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "raw_data = tfds.as_dataframe(ds, ds_info)\n",
    "raw_data = shuffle(raw_data)\n",
    "Y = raw_data['quality']\n",
    "X = raw_data.drop('quality', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar tendríamos que normalizar/estandarizar las variables de entrada. Vamos a utilizar una capa de **Keras** que hace precisamente eso, por lo cual integraremos la normalización directamente en el modelo, justo a continuación de la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:40:44.494241Z",
     "start_time": "2021-05-31T11:40:44.154903Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def generate_model_wines():\n",
    "    inputs = tf.keras.layers.Input((11,))\n",
    "    #normalize = tf.keras.layers.experimental.preprocessing.Normalization()(inputs)\n",
    "    x = tf.keras.layers.Dense(16, activation='tanh')(inputs)\n",
    "    x = tf.keras.layers.Dense(16, activation='tanh')(x)\n",
    "    outputs = tf.keras.layers.Dense(11, activation='softmax')(x) # Notas de 0 a 10, ambas incluidas\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"wine_quality\")\n",
    "\n",
    "plot_model(generate_model_wines(), show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:46:47.646253Z",
     "start_time": "2021-05-31T11:46:28.943406Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_wines = generate_model_wines()\n",
    "    model_wines.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model_wines.fit(X.values, Y.values, batch_size=16, epochs=25, verbose=0, validation_split=0.2)\n",
    "plot_train_val(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:44:32.563791Z",
     "start_time": "2021-05-31T11:44:32.341254Z"
    }
   },
   "source": [
    "### Pipelines y variables de entrada\n",
    "\n",
    "Podemos observar que planteando el problema como clasificación no obtenemos buenos resultados. Hay que pensar que la nota que se asigna a un vino tiene un orden, ya que no es lo mismo equivocarse prediciendo una nota de 2 cuando la real era un 10, que predecir un 9. Es por ello que vamos a plantear el problema como si de una regresión se tratase, devolviendo una salida continua que representa la votación predicha por el modelo.\n",
    "\n",
    "Además, vamos a aprovechar este cambio para plantear el uso de *pipelines* para conectar los *datasets* al modelo, además de utilizar una entrada individual por cada una de las *features* del *dataset*.\n",
    "\n",
    "En primer lugar, cargamos el conjunto de datos `wine_quality` utilizando tfds.load(), y convertimos la variable objetivo en `float`, ya que ahora el problema será de regresión y no de clasificación. A continuación, barajamos el conjunto de datos y lo dividimos en conjuntos de entrenamiento y de test. Tomamos los primeros `test_size` observaciones como la división de entrenamiento, y el resto como la división de test. Todo ello lo hacemos utilizando un **pipeline** de *Tensorflow*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T12:07:01.028131Z",
     "start_time": "2021-05-31T12:07:01.018234Z"
    }
   },
   "outputs": [],
   "source": [
    "to_regression = lambda x, y: (x, tf.cast(y, tf.float32))\n",
    "\n",
    "def get_train_and_test_splits(train_size, batch_size=1):\n",
    "    dataset = (\n",
    "        tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n",
    "        .map(to_regression)\n",
    "        .prefetch(buffer_size=dataset_size)  # El dataset es pequeño y cabe entero en memoria\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    train_dataset = (\n",
    "        dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
    "    )\n",
    "    test_dataset = dataset.skip(train_size).batch(batch_size)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos dicho antes, vamos a construir un modelo en el cual cada *feature* es una entrada distinta del mismo. Para ello tenemos que darle nombres a las entradas. El resto del modelo será similar al que hemos definido antes, solo que ahora la capa de salida constará de una única neurona con una función de activación continua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T12:19:51.476830Z",
     "start_time": "2021-05-31T12:19:50.789507Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"fixed_acidity\",\n",
    "    \"volatile_acidity\",\n",
    "    \"citric_acid\",\n",
    "    \"residual_sugar\",\n",
    "    \"chlorides\",\n",
    "    \"free_sulfur_dioxide\",\n",
    "    \"total_sulfur_dioxide\",\n",
    "    \"density\",\n",
    "    \"pH\",\n",
    "    \"sulphates\",\n",
    "    \"alcohol\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "            name=feature_name, shape=(1,), dtype=tf.float32\n",
    "        )\n",
    "    return inputs\n",
    "\n",
    "def regression_wine():\n",
    "    inputs = create_model_inputs()\n",
    "    input_values = [value for _, value in sorted(inputs.items())]\n",
    "    features = keras.layers.concatenate(input_values)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    for units in [16, 16]:\n",
    "        features = layers.Dense(units, activation=\"sigmoid\")(features)\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "plot_model(regression_wine(), show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer una partición de entrenamiento y test en 80% y 20%, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T12:07:05.371995Z",
     "start_time": "2021-05-31T12:07:05.225721Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_size = 4898\n",
    "batch_size = 256\n",
    "train_size = int(dataset_size * 0.80)\n",
    "train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y procedemos al entrenamiento y validación del modelo construido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T12:21:13.058380Z",
     "start_time": "2021-05-31T12:21:00.720805Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    tf.keras.backend.clear_session()\n",
    "    reg_wines = regression_wine()\n",
    "    reg_wines.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    history = reg_wines.fit(train_dataset, epochs=50, validation_data=test_dataset, verbose=0)\n",
    "plot_train_val(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T12:21:24.257428Z",
     "start_time": "2021-05-31T12:21:24.032655Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_val(history, metric='root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Creado por **Raúl Lara Cabrera** (raul.lara@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
