{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de hiper-parámetros con Nevergrad\n",
    "\n",
    "[Nevergrad](https://facebookresearch.github.io/nevergrad/index.html) es una librería de optimización numérica de código abierto desarrollada por **Facebook**. Aunque puede utilizarse para resolver cualquier problema de optimización, está diseñada para realizar una optimización de hiper-parámetros cuando estamos ajustando un modelo.\n",
    "\n",
    "A diferencia de `GridSearch`, **Nevergrad** no realiza una búsqueda exhaustiva sobre todas las combinaciones posibles de valores, sino que utiliza metaheurísticas tales como los algoritmos evolutivos y sus variantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:09:48.915940Z",
     "start_time": "2021-06-06T15:09:39.076267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nevergrad in /opt/conda/lib/python3.8/site-packages (0.4.2.post2)\n",
      "Requirement already satisfied: cma>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from nevergrad) (3.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from nevergrad) (3.7.4.3)\n",
      "Requirement already satisfied: bayesian-optimization>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from nevergrad) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from nevergrad) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /opt/conda/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad) (2.1.0)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nevergrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo rápido del funcionamiento de la librería sería optimizar una función bidimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:11:23.558069Z",
     "start_time": "2021-06-06T15:11:06.372259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3_w,6)-aCMA-ES (mu_w=2.0,w_1=63%) in dimension 2 (seed=<module 'time' (built-in)>, Sun Jun  6 15:11:08 2021)\n",
      "[0.50000003 0.49999993]\n"
     ]
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "\n",
    "def square(x):\n",
    "    return sum((x - 0.5) ** 2)\n",
    "\n",
    "# optimization on x as an array of shape (2,)\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=2, budget=100)\n",
    "recommendation = optimizer.minimize(square)  # best value\n",
    "print(recommendation.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir un modelo de red neuronal para ver cómo podemos optimizar los hiperparámetros para obtener el mejor ajuste del modelo. Utilizaremos un conjunto de datos sencillo para no dilatar mucho el tiempo de cómputo de la optimización de los hiperparámetros: **Fashion MNIST**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T16:23:13.818205Z",
     "start_time": "2021-06-06T16:23:12.992213Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "num_train_images = X_train.shape[0]\n",
    "num_test_images = X_test.shape[0]\n",
    "image_height = X_train.shape[1]\n",
    "image_width = X_train.shape [2]\n",
    "X_train = X_train.reshape(X_train.shape[0], image_height*image_width)\n",
    "X_test = X_test.reshape(X_test.shape[0], image_height*image_width)\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "y_train = K.utils.to_categorical(y_train, 10)\n",
    "y_test = K.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que funcione **Nevergrad** necesitamos una función de evaluación para medir la calidad de un conjunto determinado de hiper-parámetros. Para ello encapsulamos la construcción de la red neuronal junto con la evaluación de la misma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T16:26:50.297397Z",
     "start_time": "2021-06-06T16:26:50.285494Z"
    }
   },
   "outputs": [],
   "source": [
    "def network(d_1, d_2, dr, if_dense3, d_3, act='relu', x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_test):\n",
    "    model = K.Sequential()\n",
    "    model.add(K.layers.Dense(d_1, activation=act, input_shape=(784,)))\n",
    "    model.add(K.layers.Dropout(dr))\n",
    "    model.add(K.layers.Dense(d_2, activation=act))\n",
    "    if if_dense3: # la tercera capa es opcional\n",
    "        model.add(K.layers.Dense(d_3, activation=act))\n",
    "    model.add(K.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=256,\n",
    "                        epochs=10,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    return float(score[0])    # el valor de 'fitness' será el valor de evaluación de la función de coste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, los hiper-parámetros de la red son los siguientes:\n",
    "\n",
    "- `d_1`: número de neuronas de la primera capa densa.\n",
    "- `d_2`: número de neuronas de la segunda capa densa.\n",
    "- `dr`: tasa de *Dropout* entre la primera y segunda capa.\n",
    "- `if_dense3`: ¿añadimos tercera capa?\n",
    "- `d_3`: número de neuronas de la tercera capa densa (opcional).\n",
    "\n",
    "**Nevergrad** denomina **intrumentalización** a la definición de los hiper-parámetros. Observa que los nombres de los parámetros de la instrumentalización coinciden con los parámetros de la función de evaluación. También es necesario elegir uno de los [optimizadores](https://facebookresearch.github.io/nevergrad/optimizers_ref.html#optimizers) que vienen incluidos en **Nevergrad**.\n",
    "\n",
    "Por último, bastará con ejecutar la meta-optimización consistente en minimizar la función de evaluación que hemos definido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T17:16:39.037638Z",
     "start_time": "2021-06-06T17:00:59.329344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.41100960969924927\n",
      "Test accuracy: 0.8508999943733215\n",
      "Test loss: 0.4905564486980438\n",
      "Test accuracy: 0.8219000101089478\n",
      "Test loss: 0.8029636740684509\n",
      "Test accuracy: 0.680899977684021\n",
      "Test loss: 0.3949854373931885\n",
      "Test accuracy: 0.8555999994277954\n",
      "Test loss: 0.8093854188919067\n",
      "Test accuracy: 0.676800012588501\n",
      "Test loss: 0.5914907455444336\n",
      "Test accuracy: 0.7678999900817871\n",
      "Test loss: 0.41467413306236267\n",
      "Test accuracy: 0.8550999760627747\n",
      "Test loss: 0.4008086621761322\n",
      "Test accuracy: 0.8578000068664551\n",
      "Test loss: 0.43379488587379456\n",
      "Test accuracy: 0.8472999930381775\n",
      "Test loss: 0.68550705909729\n",
      "Test accuracy: 0.7294999957084656\n",
      "Test loss: 0.427870512008667\n",
      "Test accuracy: 0.8425999879837036\n",
      "Test loss: 0.6401051878929138\n",
      "Test accuracy: 0.7678999900817871\n",
      "Test loss: 0.3965599238872528\n",
      "Test accuracy: 0.858299970626831\n",
      "Test loss: 0.42326122522354126\n",
      "Test accuracy: 0.8483999967575073\n",
      "Test loss: 0.7196761965751648\n",
      "Test accuracy: 0.7835000157356262\n",
      "Test loss: 0.3942267596721649\n",
      "Test accuracy: 0.8549000024795532\n",
      "Test loss: 0.4233396053314209\n",
      "Test accuracy: 0.8537999987602234\n",
      "Test loss: 0.4113691747188568\n",
      "Test accuracy: 0.8503999710083008\n",
      "Test loss: 0.38619813323020935\n",
      "Test accuracy: 0.8633999824523926\n",
      "Test loss: 0.4425128400325775\n",
      "Test accuracy: 0.840399980545044\n",
      "Test loss: 1.1690880060195923\n",
      "Test accuracy: 0.5619999766349792\n",
      "Test loss: 0.4204395115375519\n",
      "Test accuracy: 0.849399983882904\n",
      "Test loss: 0.4771518409252167\n",
      "Test accuracy: 0.8284000158309937\n",
      "Test loss: 0.4388698637485504\n",
      "Test accuracy: 0.8447999954223633\n",
      "Test loss: 0.4125013053417206\n",
      "Test accuracy: 0.8536999821662903\n",
      "Test loss: 0.44797030091285706\n",
      "Test accuracy: 0.8393999934196472\n",
      "Test loss: 0.5301666259765625\n",
      "Test accuracy: 0.7932000160217285\n",
      "Test loss: 1.046357274055481\n",
      "Test accuracy: 0.5745000243186951\n",
      "Test loss: 0.5588021278381348\n",
      "Test accuracy: 0.8043000102043152\n",
      "Test loss: 0.4802161157131195\n",
      "Test accuracy: 0.8337000012397766\n",
      "Test loss: 0.3962453007698059\n",
      "Test accuracy: 0.859499990940094\n",
      "Test loss: 0.40313124656677246\n",
      "Test accuracy: 0.8543000221252441\n",
      "Test loss: 0.5017237067222595\n",
      "Test accuracy: 0.8220999836921692\n",
      "Test loss: 0.3822101950645447\n",
      "Test accuracy: 0.8622000217437744\n",
      "Test loss: 0.7310312390327454\n",
      "Test accuracy: 0.7451000213623047\n",
      "Test loss: 0.5708498954772949\n",
      "Test accuracy: 0.785099983215332\n",
      "Test loss: 0.4295459985733032\n",
      "Test accuracy: 0.8482000231742859\n",
      "Test loss: 0.4024766981601715\n",
      "Test accuracy: 0.8547000288963318\n",
      "Test loss: 0.39735734462738037\n",
      "Test accuracy: 0.8561999797821045\n",
      "Test loss: 0.5452420115470886\n",
      "Test accuracy: 0.7847999930381775\n",
      "Test loss: 0.3785206079483032\n",
      "Test accuracy: 0.8644000291824341\n",
      "Test loss: 0.3821905851364136\n",
      "Test accuracy: 0.8652999997138977\n",
      "Test loss: 0.4173818826675415\n",
      "Test accuracy: 0.8519999980926514\n",
      "Test loss: 0.4316413998603821\n",
      "Test accuracy: 0.8482000231742859\n",
      "Test loss: 0.6681765913963318\n",
      "Test accuracy: 0.7706999778747559\n",
      "Test loss: 0.38943496346473694\n",
      "Test accuracy: 0.8579000234603882\n",
      "Test loss: 0.36798518896102905\n",
      "Test accuracy: 0.8697999715805054\n",
      "Test loss: 0.3799903690814972\n",
      "Test accuracy: 0.8637999892234802\n",
      "Test loss: 0.37936192750930786\n",
      "Test accuracy: 0.8639000058174133\n",
      "Test loss: 0.37991583347320557\n",
      "Test accuracy: 0.863099992275238\n",
      "Test loss: 0.3619385063648224\n",
      "Test accuracy: 0.8700000047683716\n",
      "Test loss: 0.3950231373310089\n",
      "Test accuracy: 0.8603000044822693\n",
      "Test loss: 0.4242055118083954\n",
      "Test accuracy: 0.8507000207901001\n",
      "Test loss: 0.37553107738494873\n",
      "Test accuracy: 0.8616999983787537\n",
      "Test loss: 0.3725634515285492\n",
      "Test accuracy: 0.8644999861717224\n",
      "Test loss: 0.38015857338905334\n",
      "Test accuracy: 0.8618999719619751\n",
      "Test loss: 0.4103538691997528\n",
      "Test accuracy: 0.858299970626831\n",
      "Test loss: 0.6277950406074524\n",
      "Test accuracy: 0.7741000056266785\n",
      "Test loss: 0.4800904393196106\n",
      "Test accuracy: 0.8267999887466431\n",
      "Test loss: 0.38363027572631836\n",
      "Test accuracy: 0.8618000149726868\n",
      "Test loss: 0.38765355944633484\n",
      "Test accuracy: 0.8603000044822693\n",
      "Test loss: 0.3827953636646271\n",
      "Test accuracy: 0.8644999861717224\n",
      "Test loss: 0.5425417423248291\n",
      "Test accuracy: 0.8062000274658203\n",
      "Test loss: 0.37177643179893494\n",
      "Test accuracy: 0.8648999929428101\n",
      "Test loss: 0.388630747795105\n",
      "Test accuracy: 0.8593999743461609\n",
      "Test loss: 0.5068684220314026\n",
      "Test accuracy: 0.8223999738693237\n",
      "Test loss: 0.46759799122810364\n",
      "Test accuracy: 0.8314999938011169\n",
      "Test loss: 0.37417808175086975\n",
      "Test accuracy: 0.8669999837875366\n",
      "Test loss: 0.38716769218444824\n",
      "Test accuracy: 0.8569999933242798\n",
      "Test loss: 0.3874589204788208\n",
      "Test accuracy: 0.8604000210762024\n",
      "Test loss: 0.3773351311683655\n",
      "Test accuracy: 0.8644999861717224\n",
      "Test loss: 0.38746607303619385\n",
      "Test accuracy: 0.8586999773979187\n",
      "Test loss: 0.40463921427726746\n",
      "Test accuracy: 0.8579000234603882\n",
      "Test loss: 0.5824893712997437\n",
      "Test accuracy: 0.8061000108718872\n",
      "Test loss: 0.6458771228790283\n",
      "Test accuracy: 0.7695000171661377\n",
      "Test loss: 0.37299981713294983\n",
      "Test accuracy: 0.868399977684021\n",
      "Test loss: 0.3704226315021515\n",
      "Test accuracy: 0.8691999912261963\n",
      "Test loss: 0.38307520747184753\n",
      "Test accuracy: 0.8644000291824341\n",
      "Test loss: 0.37629246711730957\n",
      "Test accuracy: 0.8633999824523926\n",
      "Test loss: 0.39766380190849304\n",
      "Test accuracy: 0.8568000197410583\n",
      "Test loss: 0.38128817081451416\n",
      "Test accuracy: 0.8650000095367432\n",
      "Test loss: 0.3993072211742401\n",
      "Test accuracy: 0.855400025844574\n",
      "Test loss: 0.39576151967048645\n",
      "Test accuracy: 0.8549000024795532\n",
      "Test loss: 0.4088176488876343\n",
      "Test accuracy: 0.8503000140190125\n",
      "Test loss: 0.3926919102668762\n",
      "Test accuracy: 0.8549000024795532\n",
      "Test loss: 0.40355315804481506\n",
      "Test accuracy: 0.8565000295639038\n",
      "Test loss: 0.42025190591812134\n",
      "Test accuracy: 0.8501999974250793\n",
      "Test loss: 0.6407576203346252\n",
      "Test accuracy: 0.7476000189781189\n",
      "Test loss: 0.4995935559272766\n",
      "Test accuracy: 0.8220999836921692\n",
      "Test loss: 0.382704496383667\n",
      "Test accuracy: 0.8628000020980835\n",
      "Test loss: 0.36915698647499084\n",
      "Test accuracy: 0.8675000071525574\n",
      "Test loss: 0.3869357109069824\n",
      "Test accuracy: 0.8614000082015991\n",
      "Test loss: 0.489673376083374\n",
      "Test accuracy: 0.833299994468689\n",
      "Test loss: 0.3720618486404419\n",
      "Test accuracy: 0.8665000200271606\n",
      "Test loss: 0.39821505546569824\n",
      "Test accuracy: 0.8550000190734863\n",
      "Test loss: 0.38948437571525574\n",
      "Test accuracy: 0.8629999756813049\n",
      "Test loss: 0.37387916445732117\n",
      "Test accuracy: 0.8637999892234802\n",
      "Test loss: 0.37384480237960815\n",
      "Test accuracy: 0.8669000267982483\n",
      "Test loss: 0.3781200647354126\n",
      "Test accuracy: 0.8639000058174133\n",
      "Test loss: 0.38079991936683655\n",
      "Test accuracy: 0.8622999787330627\n"
     ]
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "\n",
    "d_1 = ng.p.TransitionChoice(range(10, 60, 10))\n",
    "d_2 = ng.p.Choice(range(10, 60, 10))\n",
    "dr = ng.p.Scalar(lower=0.2, upper=0.7)\n",
    "if_dense3 = ng.p.Choice([True, False])\n",
    "d_3 = ng.p.Choice(range(10, 60, 10))\n",
    "#act = ng.p.Choice(['sigmoid', 'relu'])\n",
    "\n",
    "params = ng.p.Instrumentation(d_1, d_2, dr, if_dense3, d_3)\n",
    "optimizer = ng.optimizers.TwoPointsDE(parametrization=params, budget=10000)\n",
    "best = optimizer.minimize(network, batch_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T17:16:56.256515Z",
     "start_time": "2021-06-06T17:16:56.249683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 50, 0.26367643514925654, True, 50), {})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Creado por **Raúl Lara Cabrera** (raul.lara@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
