{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spectral Clustering.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yG7tfuESchvg"},"source":["# Clustering Espectral\n","\n","Esta técnica es un algoritmo de clustering que ha conseguido obtener un mejor rendimiento que otros algoritmos para el mismo cometido. La principal característica del **clustering espectral** es que trata cada punto de datos como si fuese un nodo en un grafo. Por tanto, el problema de clustering se transforma en un problema de [partición de grafos](https://en.wikipedia.org/wiki/Graph_partition).\n","\n","La idea principal es construir un grafo con los puntos de datos de manera que haya una arista con peso entre dos puntos modelando la similaridad entre ellos."]},{"cell_type":"markdown","metadata":{"id":"DbOmo2RzekWy"},"source":["## Características\n","\n","Las principales ventajas del **clustering espectral** se pueden resumir en:\n","\n","1. No es necesario asumir ninguna propiedad subyacente de los datos para que la técnica funcione, a diferencia de otros algoritmos que, por ejemplo, asumen distribuciones gaussianas de los valores. Por lo tanto esta técnica a priori es capaz de dar solución a problemas de clustering más genéricos.\n","2. Facilidad de implementación y velocidad: este algoritmo es bastante sencillo de implementar y, además, suele funcionar bastante rápido, pues consiste básicamente en operaciones matemáticas de álgebra lineal.\n","\n","\n","Sin embargo, tiene una **gran desventaja**, y es que el algoritmo es poco escalable, ya que basa su funcionamiento en el cálculo de autovalores y autovectores. De este modo, este algoritmo tiende a tardar mucho cuando el conjunto de datos de entrada es denso. De hecho, cuando mejor funciona es cuando la matriz de afinidad es dispersa.\n","\n","Según recomiendan en la propia [documentación](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering) de _sklearn_ del método:\n","\n","> SpectralClustering requiere que se especifique el número de clusters. Funciona bien para un pequeño número de clusters pero no es aconsejable cuando se utilizan muchos clusters. Además, se recomienda un tamaño medio de conjunto de datos."]},{"cell_type":"markdown","metadata":{"id":"SR3CMaGYik3d"},"source":["## El algoritmo\n","\n","Los pasos del clustering espectral son los siguientes:\n","\n","1. **Construir el grafo de similitud**: En este paso se construye el grafo de similitud entre los puntos de entrada codificado como una matriz de adyacencia $A$. Dicha matriz puede construirse de distintas maneras:\n","\n","    * **$\\varepsilon$-vecindad**: para contruir este grafo hay que fijar inicialmente un umbral $\\varepsilon$. De esa forma, todos los puntos que se encuentren en un radio $\\varepsilon$ de similitud estarán conectados por una arista.\n","\n","    ![ejemplo de epsilon-vecindad](https://i.imgur.com/6et5vqt.png)\n","\n","    * **kNN**: Se fija un parámetro $k$ de antemano. Entonces, para cada par $u$ y $v$ de vértices se añade una arista dirigida $u\\rightarrow v$ solo si $v$ está entre los $k$ vecinos más cercanos de $u$. Esto hace que el grafo sea dirigido y con las aristas ponderadas. Si queremos convertirlo en un grafo no dirigido podemos quitar la direccionalidad de las aristas o bien unir dos vértices solo si están recíprocamente conectados.\n","\n","    ![ejemplo de vecindad kNN](https://i.imgur.com/2BSkklH.png)\n","    \n","    * **Grafo completamente conectado**: La tercera opción consiste en conectar todos y cada uno de los vértices para obtener un grafo completamente conectado, donde cada arista entre dos vértices tiene como peso la medida de similaridad entre dichos puntos.\n","\n","2. **Proyectar los datos en un espacio dimensional inferior**: En este paso se reduce el espacio dimensional de los puntos de entrada para intentar que los que están relacionados queden más cerca en este espacio reducido. De esta forma se reduce la complejidad del problema de clustering y puede ser resuelto por algún algoritmo tradicional. Para ello se calcula la [Matriz Laplaciana del Grafo](https://es.wikipedia.org/wiki/Matriz_laplaciana), que se define como:\n","\n","$$L=D-A$$\n","\n","donde $A$ es la matriz de adyacencia anterior, mientras que $D$ es la matriz de grado del grafo, que se define como:\n","\n","$$D_{ij}=\\left\\{\\begin{matrix}\n","\\sum_{j=1|(i,j)\\in E}^n w_{ij}, i=j\\\\ \n","0, i \\neq j\n","\\end{matrix}\\right.$$\n","\n","3. **Hacer clustering con los datos proyectados**: En este paso se aplica un algoritmo tradicional de clustering sobre los datos proyectados en un espacio dimensional inferior. De esta forma se termina asignando un grupo (cluster) a cada punto de entrada.\n"]},{"cell_type":"markdown","metadata":{"id":"cG1T0nVx9IwD"},"source":["## Caso de estudio\n","\n","Vamos a aplicar **Clustering Espectral** a un problema de aprendizaje supervisado, pero tratándolo como si no tuviésemos información sobre el _ground truth_.\n","\n","El conjunto de datos que se va a utilizar es [credit-g](https://www.openml.org/d/31), del repositorio OpenML. Este conjunto de datos está formado por 20 características relacionadas con los préstamos solicitados por los clientes de un banco alemán. El problema radica en clasificar dichos préstamos según su riesgo crediticio:\n","\n","* Riesgo alto\n","* Riesgo bajo\n","\n","Hay varias variables del conjunto de datos que son nominales:\n","\n","- checking_status\n","- credit_history\n","- purpose\n","- savings_status\n","- employment\n","- personal_status\n","- other_parties\n","- property_magnitude\n","- other_payment_plans\n","- housing\n","- job\n","- own_telephone\n","- foreign_worker"]},{"cell_type":"code","metadata":{"id":"KBClJs0ncdV9"},"source":["# Librerías necesarias\n","\n","import pandas as pd\n","from sklearn.datasets import fetch_openml\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","import matplotlib.pyplot as plt \n","from sklearn.cluster import SpectralClustering \n","from sklearn.decomposition import PCA \n","from sklearn.metrics import silhouette_score, adjusted_rand_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lQGF7RC-wZU"},"source":["raw_credito = fetch_openml(data_id=31)\n","credito = pd.DataFrame(data=raw_credito.data, columns=raw_credito.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FowcCJu_Aef"},"source":["credito.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6Pxacce_K34"},"source":["# Un poco de pre-procesado para las columnas nominales. Se usará one-hot-encoder\n","numeric_features = ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']\n","nominal_features = list(set(raw_credito.feature_names) - set(numeric_features))\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numeric_features),\n","        ('cat', OneHotEncoder(categories='auto'), nominal_features)])\n","\n","credito = preprocessor.fit_transform(credito)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BzoYHlhFHNv"},"source":["Vamos a utilizar el análisis de componentes principales (PCA) para proyectar el conjunto de datos sobre dos características, de forma que podamos dibujarlo."]},{"cell_type":"code","metadata":{"id":"aGx-dNk1DRhG"},"source":["pca = PCA(n_components = 2) \n","X_principal = pca.fit_transform(credito) \n","X_principal = pd.DataFrame(X_principal) \n","X_principal.columns = ['P1', 'P2']\n","X_principal.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrBX-SU6Fdyq"},"source":["X_principal.plot.scatter(x='P1', y='P2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPxxWFNbGM9J"},"source":["Ahora vamos a lanzar el algoritmo de **clustering espectral** para ver cómo se agrupa cada uno de los préstamos solicitados. Fijamos la medida de afinidad a `laplacian` para que utilice la matriz Laplaciana como hemos visto en la descripción del algoritmo. "]},{"cell_type":"code","metadata":{"id":"6QD9N1hcF20_"},"source":["spectral = SpectralClustering(n_clusters=2, random_state=1337, affinity='laplacian')\n","labels = spectral.fit_predict(credito)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IDsxc7dhH1O4"},"source":["Ahora mostramos en una figura el grupo asignado por el algoritmo a cada muestra con un color distinto."]},{"cell_type":"code","metadata":{"id":"c1Im3yLvHgfO"},"source":["# Creamos un vector con el color asignado a cada etiqueta\n","cvec = ['r' if label==0 else 'b' for label in labels] \n","  \n","# Plotting the clustered scatter plot \n","plt.figure(figsize=(9, 9)) \n","plt.scatter(X_principal['P1'], X_principal['P2'], c = cvec) \n","plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ywkDJgbMUN3"},"source":["Vamos a evaluar el rendimiento del modelo con alguna de las métricas que ya hemos visto, así como usando el _ground-truth_."]},{"cell_type":"code","metadata":{"id":"7UYBmiZJIWQc"},"source":["print('Silhouette: ', silhouette_score(credito, labels))\n","print('Rand Index (con ground truth): ', adjusted_rand_score(raw_credito.target, labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7RXB34pr94zd"},"source":["---\n","\n","Creado por **Raúl Lara** (raul.lara@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]}]}