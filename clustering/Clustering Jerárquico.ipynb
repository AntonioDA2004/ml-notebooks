{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clustering Jerárquico.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RpINl4Yv5lkQ"},"source":["# Clustering Jerárquico\n","\n","Dentro de las técnicas de aprendizaje no supervisado nos encontramos con el **Clustering Jerárquico**. Esta técnica toma su nombre del modo de proceder a la hora de agrupar los puntos de datos de entrada, pues construye una jerarquía de clusters de varios niveles.\n","\n","Dicha jerarquía se puede construir según dos enfoques opuestos:\n","\n","- **Aglomerativa** (bottom-up)\n","- **Divisiva** (top-down)\n"]},{"cell_type":"markdown","metadata":{"id":"TTXOaq_gQ4gs"},"source":["## Aglomerativo (bottom-up)\n","\n","Los puntos de datos se agrupan utilizando un enfoque de abajo hacia arriba que comienza con puntos de datos individuales y se van agrupando en clusters más grandes a medida que avanza el proceso.\n","\n","El algoritmo a seguir para realizar el clustering jerárquico aglomerativo se resume en los siguientes pasos:\n","\n","1. Cada punto de datos de entrada pertenece a su propio cluster.\n","2. Se agrupan los dos puntos de datos más cercanos en un mismo cluster.\n","3. Se van construyendo más clusters al unir los dos cluster más cercanos.\n","4. Se repite el paso anterior hasta que solo queda un único cluster al cual pertenecen todos los puntos de entrada.\n","\n","### Medidas de similaridad y distancia entre clusters\n","\n","Observando el algoritmo anterior se aprecia que es necesario utilizar una medida de similaridad entre puntos para determinar cómo de cerca se encuentran. Pero no solo eso, además necesitamos una manera de medir la distancia entre clusters que pueden estar formados por más de un punto. Esta medida se conoce como el **criterio de vinculación** (_linkage criteria_), y en el caso de _sklearn_ puede realizarse de la siguiente manera:\n","\n","- **Ward**: minimiza la suma de las diferencias al cuadrado dentro de todos los clusters. Se trata de un enfoque de minimización de la varianza y, en este sentido, es similar a la función objetivo de `k-means`, pero abordada con un enfoque jerárquico aglomerativo.\n","- **Vinculación máxima** o **completa**: minimiza la distancia máxima entre las observaciones de cada par de clusters.\n","- **Vinculación media**: minimiza la media de las distancias entre las observaciones de cada par de clusters.\n","- **Vinculación única**: minimiza la distancia más corta entre las observaciones de cada par de clusters.\n","\n","En las vinculaciones únicas, medias y máximas es posible configurar la métrica de similaridad que se usa para calcular las distancias:\n","\n","- Distancia euclídea (L2)\n","- Distancia Manhattan (L1), \n","- Distancia coseno\n","- Cualquier matriz de afinidad precalculada.\n","\n","La distancia L1 suele funcionar bien para conjuntos de datos dispersos, donde muchas variables tienen valores nulos para muchas observaciones. Por otro lado,\n","la distancia del coseno es interesante porque es independiente de las escalas.\n","\n","### Dendogramas\n","\n","Estos agrupamientos secuenciales y por niveles forman lo que se conoce como un [Dendograma](https://en.wikipedia.org/wiki/Dendrogram), que es una estructura arbórea.\n","\n","Para entender cómo funciona el clustering jerárquico aglomerativo y cómo se construye el dendograma, comenzamos con un conjunto de datos de entrada de dos variables:"]},{"cell_type":"code","metadata":{"id":"u51qrAnS5d89"},"source":["# Ejemplo obtenido de: https://www.instintoprogramador.com.mx/2019/07/clustering-jerarquico-con-python-y.html\n","import numpy as np\n","\n","X = np.array([[5,3],  \n","    [10,15],\n","    [15,12],\n","    [24,10],\n","    [30,30],\n","    [85,70],\n","    [71,80],\n","    [60,78],\n","    [70,55],\n","    [80,91],])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zb6Jn4D6XCzP"},"source":["Vamos a representar gráficamente los puntos de entrada, indexándolos según la posición que ocupan en la matriz `X`:"]},{"cell_type":"code","metadata":{"id":"prF869KQXBzn"},"source":["import matplotlib.pyplot as plt\n","\n","labels = range(1, 11)  \n","plt.figure(figsize=(10, 7))  \n","plt.subplots_adjust(bottom=0.1)  \n","plt.scatter(X[:,0],X[:,1], label='True Position')\n","\n","for label, x, y in zip(labels, X[:, 0], X[:, 1]):  \n","    plt.annotate(\n","        label,\n","        xy=(x, y), xytext=(-3, 3),\n","        textcoords='offset points', ha='right', va='bottom')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yH6kiMfgXUF9"},"source":["Para representar gráficamente los dendogramas, podemos usar la librería [scipy](https://www.scipy.org/):"]},{"cell_type":"code","metadata":{"id":"qBrCLR_sXPN0"},"source":["from scipy.cluster.hierarchy import dendrogram, linkage  \n","from matplotlib import pyplot as plt\n","\n","linked = linkage(X, 'single')\n","\n","labelList = range(1, 11)\n","\n","plt.figure(figsize=(10, 7))  \n","dendrogram(linked,  \n","            orientation='top',\n","            labels=labelList,\n","            distance_sort='descending',\n","            show_leaf_counts=True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IOt1lHnXX9KD"},"source":["## Divisivo (top-down)\n","\n","Este enfoque, complementario al agregativo, comienza con todos los puntos de entrada agrupados en el mismo cluster. Iterativamente va particionando un cluster que se elige según una estrategia.\n","\n","En otras palabras, el algoritmo sería el siguiente:\n","\n","1. Se agrupan todos los puntos de entrada en el mismo cluster.\n","2. Se decide cuál es el cluster que se va a particionar según la estrategia elegida.\n","3. Se divide ese cluster usando otro algoritmo de clustering (por ejemplo _k-means_).\n","4. Se repiten los pasos 2-3 hasta que cada punto de entrada pertenezca a su propio cluster o hasta que se alcancen el número de clusters deseados.\n","\n","Para elegir qué cluster se va a particionar, se suelen utilizar los siguientes enfoques:\n","\n","- Se particionan todos los clusters que haya.\n","- Se particiona el cluster que esté formado por más elementos\n","- Se elige el cluster que tenga la mayor varianza con respecto a su centroide\n","\n","### Divisivo vs Aglomerativo:\n","\n","- El **clustering divisivo** es más complejo que el **aglomerativo**, pues hay que lanzar un algoritmo adicional de clustering cada vez que se quiere particionar un cluster existente.\n","- Sin embargo, el **divisivo** puede ser más eficiente si no se llegan a particionar todos los clusteres hasta que cada punto de entrada forme uno.\n","- El **clustering divisivo** también es más _preciso_ que el **aglomerativo**, pues éste último toma decisiones de agrupación basada en la información local de los puntos (se van agrupando puntos cercanos) sin tener en cuenta la distribución global de los puntos de entrada, cosa que sí hace el **clustering divisivo**.\n"]},{"cell_type":"markdown","metadata":{"id":"zQZAToQQjgic"},"source":["## Caso de estudio: intención de compra\n","Vamos a cargar un dataset del repositorio [UCI](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset). Es un conjunto de datos de características sobre las sesiones online de compradores en una tienda virtual.\n","\n","Según la propia descripción del conjunto de datos:\n","\n","\n","\n","> The dataset consists of 10 numerical and 8 categorical attributes.\n","The 'Revenue' attribute can be used as the class label.\n",">\n","> \"Administrative\", \"Administrative Duration\", \"Informational\", \"Informational Duration\", \"Product Related\" and \"Product Related Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another.\n",">\n","> The \"Bounce Rate\", \"Exit Rate\" and \"Page Value\" features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. The value of \"Bounce Rate\" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session. The value of \"Exit Rate\" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The \"Page Value\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. The \"Special Day\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date.\n",">\n",">The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.\n","\n","Por tanto, tenemos 7 variables categóricas que habrá que transformar.\n"]},{"cell_type":"code","metadata":{"id":"1tQhlsUQXzFr"},"source":["import pandas as pd\n","\n","raw = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv')\n","target = raw['Revenue']\n","raw.drop('Revenue', axis=1, inplace=True)\n","raw.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ee1LZz3Kj6c0"},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n","\n","nominal_features = ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n","numeric_features = list(set(raw.columns) - set(nominal_features))\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', MinMaxScaler(copy=False), numeric_features),\n","        ('cat', OneHotEncoder(categories='auto'), nominal_features)])\n","\n","datos = preprocessor.fit_transform(raw)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNjp5FhKpm09"},"source":["datos.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYR6bplFpuoT"},"source":["from sklearn.decomposition import TruncatedSVD \n","tsvd = TruncatedSVD(n_components=2) \n","X_principal = tsvd.fit_transform(datos) \n","X_principal = pd.DataFrame(X_principal) \n","X_principal.columns = ['P1', 'P2']\n","X_principal.plot.scatter(x='P1', y='P2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhYDeJT1q7Xv"},"source":["Y ahora aplicamos **clustering jerárquico aglomerativo** para intentar extraer conocimiento del problema:"]},{"cell_type":"code","metadata":{"id":"6vWnG5A8p4ck"},"source":["from sklearn.cluster import AgglomerativeClustering\n","\n","modelo = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='single')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyfd7Anhr3be"},"source":["from sklearn.metrics import silhouette_score, adjusted_rand_score\n","\n","labels = modelo.fit_predict(datos.toarray())\n","\n","print('Silhouette: ', silhouette_score(datos, labels))\n","print('Rand Index (con ground truth): ', adjusted_rand_score(target, labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30MNAHffQgyA"},"source":["## Ejercicio\n","Realiza un grid_search para probar distintas métricas de afinidad y vinculación. Compara los resultados de los modelos según la métrica Silhouette y Rand Index y determina cuál es mejor para este problema"]},{"cell_type":"code","metadata":{"id":"yp4CUMv5RAi7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J984NHMZn_XV"},"source":["<hr>\n","\n","Creado por **Raúl Lara** (raul.lara@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]}]}