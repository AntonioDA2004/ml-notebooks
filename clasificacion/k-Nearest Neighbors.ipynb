{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3.7.6 64-bit ('base': conda)","language":"python","name":"python37664bitbasecondaf773e600c47a4b2ebe48cbbd4129b87f"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":true,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"k-Nearest Neighbors.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"FPocsTEAM7UL"},"source":["# k-Nearest Neighbors Classifier\n","\n","El algoritmo de los **k vecinos más cercanos (kNN)** se puede utilizar como modelo de clasificación.\n","\n","Su funcionamiento es muy intuitivo y análogo a su homónimo *kNN* existente para regresión. Para clasificar un punto cualquiera el algoritmo:\n","\n","1. Calcula la distancia existente entre dicho punto y todos los demás puntos existentes en el conjunto de datos.\n","2. Determina los *k* puntos más cercanos.\n","3. Determina la clase de un punto en función de la clase a la que pertenezcan dichos puntos.\n","\n","Analicemos el funcionamiento de **KNN** como modelo de clasificación usando el siguiente conjunto de datos:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-05-06T15:18:31.480298Z","start_time":"2020-05-06T15:18:31.475268Z"},"id":"rPkcPu7FM7UN"},"source":["import matplotlib.pyplot as plt\n","import math\n","from sklearn.datasets import make_moons"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8z_LCMT6Q9nA"},"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples=150, noise=0.15, random_state=42)\n","\n","plt.figure(figsize=(5,5))\n","plt.xlabel('X1')\n","plt.ylabel('X2')\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFWPp0HioB4g"},"source":["Observamos que las clases siguen una disposición de lunas (*moons*) por lo que su separación es no lineal. Utilizando la implementación [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) podemos observar los $k=5$ vecinos determinados por el algoritmos en diferentes regiones del espacio muestral:"]},{"cell_type":"code","metadata":{"id":"acsrFTcFSZNZ"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.datasets import make_moons\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","def plot_neighbors(X, y, axs, knn, point):\n","  min = np.amin(X, axis=0)\n","  max = np.amax(X, axis=0)\n","\n","  diff = max - min\n","\n","  min = min - 0.1 * diff\n","  max = max + 0.1 * diff\n","\n","  axs.set_xlabel('X1')\n","  axs.set_ylabel('X2')\n","\n","  axs.set_xlim(min[0], max[0])\n","  axs.set_ylim(min[1], max[1])\n","\n","  axs.set_title('neighbors of point ' + str(point))\n","\n","  axs.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr, zorder=20)\n","\n","  axs.plot(point[0], point[1], c='black', marker='x', markersize=15, zorder=15)\n","  kneighbors = knn.kneighbors(np.expand_dims(point, 0), n_neighbors=knn.n_neighbors, return_distance=False)\n","\n","  for i in kneighbors[0]:\n","    axs.plot(X[i,0], X[i,1], c='yellow', marker='o', markersize=15, zorder=10)\n","\n","X, y = make_moons(n_samples=150, noise=0.15, random_state=42)\n","\n","knn = KNeighborsClassifier(n_neighbors=5).fit(X, y)\n","\n","fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n","fig.tight_layout(pad=4.0)\n","\n","plot_neighbors(X, y, axs[0,0], knn, np.array([-0.1, 0.5]))\n","plot_neighbors(X, y, axs[0,1], knn, np.array([-1, 0]))\n","plot_neighbors(X, y, axs[1,0], knn, np.array([1, -0.5]))\n","plot_neighbors(X, y, axs[1,1], knn, np.array([0.4, 0.5]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5QOf09jokcm"},"source":["Como vemos, el algoritmo encuentra siempre los *k* vecinos más cercanos. En algunos casos, la clasificación del nuevo punto será automática: si todos los vecinos pertenecen a la misma clase el nuevo punto pertenecerá a dicha clase; en otros, será necesario ponderar la clase media a la que pertenece el nuevo punto. Esto puede lograrse mediante una \"votación\" en la que cada vecino \"opine\" de la clase a la que debe pertenece el nuevo punto. Esta \"votación\" puede realizarse de forma uniforme (el peso de todos los vecinos es el mismo) o ponderada en función de la distancia del vecino al punto (el peso de los vecinos más cercanos es mayor).\n","\n","Como podemos imaginar, el número vecinos es un híper-parámetro crucial para el desempeño de este clasificador. Es necesario ajustar adecuadamente este híper-parámetro ya que, si el híper-parámetro es demasiado pequeño tendremos *overfittig* y si el híper-parámetro es demasiado grande tendremos *underfitting*. \n","\n","Ilustremos este hecho con cuatro conjuntos de datos (*blobs*, *moons*, *circles* y *spirals*) y distinto número de vecinos (2, 10, 150):"]},{"cell_type":"code","metadata":{"id":"IUpjoC8PAwxG"},"source":["import numpy as np\r\n","import sklearn.utils\r\n","import math\r\n"," \r\n","def make_spirals(n_samples=500, n_classes=2, shuffle=True, noise=1, n_loops=2, margin=.5, random_state=None):\r\n","  np.random.seed(random_state)\r\n"," \r\n","  n_samples_per_class = math.ceil(n_samples / n_classes)\r\n","  theta = np.random.rand(n_samples_per_class,1) * n_loops * 360 * np.pi / 180\r\n"," \r\n","  r = margin * n_classes * theta \r\n"," \r\n","  X = np.array([]).reshape(0, 2)\r\n","  y = np.array([])\r\n"," \r\n","  for i in range(n_classes):\r\n","    rotated_theta = theta + i * 2 * np.pi / n_classes\r\n","    x1 = r * np.cos(rotated_theta) + np.random.rand(n_samples_per_class,1) * noise\r\n","    x2 = r * np.sin(rotated_theta) + np.random.rand(n_samples_per_class,1) * noise\r\n","    class_samples = np.hstack((x1, x2))\r\n","    X = np.vstack((X, class_samples))\r\n","    y = np.hstack((y, np.array([i]*n_samples_per_class)))\r\n"," \r\n","  if shuffle:\r\n","    X, y = sklearn.utils.shuffle(X, y, random_state=random_state)\r\n","\r\n","  X = X[0:n_samples,:]\r\n","  y = y[0:n_samples]\r\n"," \r\n","  return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1aeVIwnU1EO"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.datasets import make_blobs, make_moons, make_circles\n","\n","def plot_clasification(X, y, axs):\n","  min = np.amin(X, axis=0)\n","  max = np.amax(X, axis=0)\n","\n","  diff = max - min\n","\n","  min = min - 0.1 * diff\n","  max = max + 0.1 * diff\n","\n","  axs[0].set_title('Raw data')\n","  axs[0].set_xlabel('X1')\n","  axs[0].set_ylabel('X2')\n","\n","  axs[0].set_xlim(min[0], max[0])\n","  axs[0].set_ylim(min[1], max[1])\n","\n","  axs[0].scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n","\n","  for idx, n_neighbors in enumerate([2, 10, 100]):\n","\n","    knn = KNeighborsClassifier(n_neighbors).fit(X, y)\n","\n","    axs[idx+1].set_title('n_neighbors=' + str(n_neighbors))\n","    axs[idx+1].set_xlabel('X1')\n","    axs[idx+1].set_ylabel('X2')\n","\n","    axs[idx+1].set_xlim(min[0], max[0])\n","    axs[idx+1].set_ylim(min[1], max[1])\n","\n","    xx, yy = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n","    Z = knn.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n","    Z = Z.reshape(xx.shape) \n","\n","    axs[idx+1].pcolormesh(xx, yy, Z, cmap=plt.cm.bwr_r, vmin=0, vmax=1)\n","    axs[idx+1].scatter(X[:,0], X[:,1], c='k', marker='x')\n","\n","\n","fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20,20))\n","fig.tight_layout(pad=4.0)\n","\n","X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=4.0, random_state=42)\n","plot_clasification(X, y, axs[0])\n","\n","X, y = make_moons(n_samples=300, noise=0.20, random_state=42)\n","plot_clasification(X, y, axs[1])\n","\n","X, y = make_circles(n_samples=300, noise=0.20, factor=0.5, random_state=42)\n","plot_clasification(X, y, axs[2])\n","\n","X, y = make_spirals(n_samples=500, random_state=42)\n","plot_clasification(X, y, axs[3])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tp1QhSafqB4N"},"source":["Una gran ventaja de *kNN*, además de su sencillez, es que permite la clasificación multiclase:"]},{"cell_type":"code","metadata":{"id":"XgMv2zU-qKZG"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.datasets import make_blobs\n","\n","def plot_clasification(n_classes, axs):\n","  X, y = make_blobs(n_samples=300, n_features=2, centers=n_classes, cluster_std=2.0, random_state=23)\n","\n","  min = np.amin(X, axis=0)\n","  max = np.amax(X, axis=0)\n","\n","  diff = max - min\n","\n","  min = min - 0.1 * diff\n","  max = max + 0.1 * diff\n","\n","  # raw data\n","\n","  axs[0].set_title('raw data')\n","  axs[0].set_xlabel('X1')\n","  axs[0].set_ylabel('X2')\n","\n","  axs[0].set_xlim(min[0], max[0])\n","  axs[0].set_ylim(min[1], max[1])\n","\n","  axs[0].scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Accent)\n","\n","  # classified data\n","\n","  knn = KNeighborsClassifier(n_neighbors=20).fit(X, y)\n","\n","  axs[1].set_title('n_classes=' + str(n_classes))\n","  axs[1].set_xlabel('X1')\n","  axs[1].set_ylabel('X2')\n","\n","  axs[1].set_xlim(min[0], max[0])\n","  axs[1].set_ylim(min[1], max[1])\n","\n","  x1, x2 = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n","  y_pred = knn.predict(np.c_[x1.ravel(), x2.ravel()])\n","  y_pred = y_pred.reshape(x1.shape) \n","\n","  axs[1].pcolormesh(x1, x2, y_pred, cmap=plt.cm.Accent)\n","  axs[1].scatter(X[:,0], X[:,1], c='k', marker='x')\n","\n","\n","fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(10,20))\n","fig.tight_layout(pad=4.0)\n","\n","plot_clasification(n_classes=3, axs=axs[0])\n","plot_clasification(n_classes=4, axs=axs[1])\n","plot_clasification(n_classes=5, axs=axs[2])\n","plot_clasification(n_classes=6, axs=axs[3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHlgpretBEjw"},"source":["import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","\r\n","from sklearn.neighbors import KNeighborsClassifier\r\n","from sklearn.datasets import make_blobs\r\n","\r\n","def plot_clasification(n_classes, axs):\r\n","  X, y = make_spirals(n_samples=500, n_classes=n_classes, margin=1, random_state=42)\r\n","\r\n","  min = np.amin(X, axis=0)\r\n","  max = np.amax(X, axis=0)\r\n","\r\n","  diff = max - min\r\n","\r\n","  min = min - 0.1 * diff\r\n","  max = max + 0.1 * diff\r\n","\r\n","  x1, x2 = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\r\n","\r\n","  # raw data\r\n","\r\n","  axs[0].set_title('raw data')\r\n","  axs[0].set_xlabel('X1')\r\n","  axs[0].set_ylabel('X2')\r\n","\r\n","  axs[0].set_xlim(min[0], max[0])\r\n","  axs[0].set_ylim(min[1], max[1])\r\n","\r\n","  axs[0].scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Accent)\r\n","\r\n","  # n_neighbors=20\r\n","\r\n","  knn = KNeighborsClassifier(n_neighbors=3).fit(X, y)\r\n","\r\n","  axs[1].set_title('n_classes=' + str(n_classes) + ', n_neighbors=3')\r\n","  axs[1].set_xlabel('X1')\r\n","  axs[1].set_ylabel('X2')\r\n","\r\n","  axs[1].set_xlim(min[0], max[0])\r\n","  axs[1].set_ylim(min[1], max[1])\r\n","\r\n","  y_pred = knn.predict(np.c_[x1.ravel(), x2.ravel()])\r\n","  y_pred = y_pred.reshape(x1.shape) \r\n","\r\n","  axs[1].pcolormesh(x1, x2, y_pred, cmap=plt.cm.Accent)\r\n","  axs[1].scatter(X[:,0], X[:,1], c='k', marker='x')\r\n","\r\n","  # n_neighbors=20\r\n","\r\n","  knn = KNeighborsClassifier(n_neighbors=7).fit(X, y)\r\n","\r\n","  axs[2].set_title('n_classes=' + str(n_classes) + ', n_neighbors=3')\r\n","  axs[2].set_xlabel('X1')\r\n","  axs[2].set_ylabel('X2')\r\n","\r\n","  axs[2].set_xlim(min[0], max[0])\r\n","  axs[2].set_ylim(min[1], max[1])\r\n","\r\n","  y_pred = knn.predict(np.c_[x1.ravel(), x2.ravel()])\r\n","  y_pred = y_pred.reshape(x1.shape) \r\n","\r\n","  axs[2].pcolormesh(x1, x2, y_pred, cmap=plt.cm.Accent)\r\n","  axs[2].scatter(X[:,0], X[:,1], c='k', marker='x')\r\n","\r\n","  # n_neighbors=20\r\n","\r\n","  knn = KNeighborsClassifier(n_neighbors=15).fit(X, y)\r\n","\r\n","  axs[3].set_title('n_classes=' + str(n_classes) + ', n_neighbors=3')\r\n","  axs[3].set_xlabel('X1')\r\n","  axs[3].set_ylabel('X2')\r\n","\r\n","  axs[3].set_xlim(min[0], max[0])\r\n","  axs[3].set_ylim(min[1], max[1])\r\n","\r\n","  y_pred = knn.predict(np.c_[x1.ravel(), x2.ravel()])\r\n","  y_pred = y_pred.reshape(x1.shape) \r\n","\r\n","  axs[3].pcolormesh(x1, x2, y_pred, cmap=plt.cm.Accent)\r\n","  axs[3].scatter(X[:,0], X[:,1], c='k', marker='x')\r\n","\r\n","\r\n","fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(20,15))\r\n","fig.tight_layout(pad=4.0)\r\n","\r\n","plot_clasification(n_classes=3, axs=axs[0])\r\n","plot_clasification(n_classes=4, axs=axs[1])\r\n","plot_clasification(n_classes=5, axs=axs[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hvor-GmBNIAG"},"source":["---\n","\n","Creado por **Raúl Lara** (raul.lara@upm.es) y **Fernando Ortega** (fernando.ortega@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]}]}