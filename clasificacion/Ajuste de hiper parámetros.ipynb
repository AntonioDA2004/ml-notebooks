{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtxjX8fUoewc"
   },
   "source": [
    "# Ajuste de hiper-parámetros\n",
    "\n",
    "Cuando trabajamos con modelos de *machine learning* a menudo nos encontramos con que estos disponen de una serie de hiper-parámetros que es necesario ajustar con el fin de lograr que realicen las predicciones más certeras posibles sobre nuestro conjunto de datos. A este paso se le conoce como **hyper-parameters tunning**.\n",
    "\n",
    "Veamos cómo podemos ajustar los hiper-parámetros de un `RandomForestClassifier`para el conjunto de datos de las caras de Olivetti.\n",
    "\n",
    "Cargamos las caras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYUgPyA6n0yU"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "olivetti = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D7mrxdypLkH"
   },
   "source": [
    "El conjunto de datos de las caras de Olivetti busca identificar a 40 personas mediante imágenes de sus rostros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbzJiJQqpY86"
   },
   "outputs": [],
   "source": [
    "print(olivetti.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6FrWUNusEJZ"
   },
   "source": [
    "Algunas de las caras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OZtAPXHpYbd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_images = 6\n",
    "image_shape = (64, 64)\n",
    "\n",
    "random_indices = np.random.choice(X.shape[0], size=n_images, replace=False)\n",
    "faces = X[random_indices, :]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=n_images, figsize=(3*n_images, 3))\n",
    "\n",
    "for i, comp in enumerate(faces):\n",
    "    axs[i].imshow(comp.reshape(image_shape), cmap=plt.cm.gray, interpolation='nearest', vmin=0, vmax=1)\n",
    "    axs[i].set_xticks(())\n",
    "    axs[i].set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uiOCqr0sFtY"
   },
   "source": [
    "Prepramos el dataset y lo partimos en `train` y `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uom3D70or3Av"
   },
   "outputs": [],
   "source": [
    "X = olivetti.data\n",
    "y = olivetti.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPFEIbnQrxwP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldH6W3HVsLvE"
   },
   "source": [
    "## Optimización de un hiper-parámetro\n",
    "\n",
    "Si quieremos optimizar un único híper-parametro, lo ideal es analizar cómo se comporta una medida de calidad según evoluciona dicho hiper-parámetro.\n",
    "\n",
    "Por ejemplo, veamos como varía el *accuracy* al variar el hiper-parámetro `n_estimators`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1JDQ_UJsGMX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = np.arange(1,50,3)\n",
    "train_scores = np.array([])\n",
    "test_scores = np.array([])\n",
    "\n",
    "for n in n_estimators:\n",
    "  clf = RandomForestClassifier(n_estimators=n, random_state=42).fit(X_train, y_train)\n",
    "  train_scores = np.append(train_scores, clf.score(X_train, y_train))\n",
    "  test_scores = np.append(test_scores, clf.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(n_estimators, train_scores, label=\"train\")\n",
    "plt.plot(n_estimators, test_scores, label=\"test\")\n",
    "\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXZ9m4q6s0u2"
   },
   "source": [
    "## Optimización de dos hiper-parámetros\n",
    "\n",
    "Si queremos comprobar cómo evoluciona el error cuando para las variaciones producidas por dos hiper-parámetros, debemos pintar un mapa de calor en el que se muestre el resultado de evaluar el modelo para las combinaciones de calores de los dos hiper-parámetros.\n",
    "\n",
    "Veamos como evoluciona el *accuracy* cuando variamos `n_estimators` y `max_depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YyrRd0skeHz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_estimators = np.arange(5,51,5)\n",
    "max_depths = np.arange(1,30,3)\n",
    "\n",
    "train_scores = np.array([])\n",
    "test_scores = np.array([])\n",
    "\n",
    "for n in n_estimators:\n",
    "  for d in max_depths:\n",
    "    clf = RandomForestClassifier(n_estimators=n, max_depth=d, random_state=42).fit(X_train, y_train)\n",
    "    train_scores = np.append(train_scores, clf.score(X_train, y_train))\n",
    "    test_scores = np.append(test_scores, clf.score(X_test, y_test))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "# train error \n",
    "\n",
    "im0 = axs[0].imshow(train_scores.reshape((len(n_estimators), len(max_depths))), vmin=0, vmax=1, cmap='Blues')\n",
    "plt.colorbar(im0, ax=axs[0])\n",
    "\n",
    "axs[0].set_xlabel('max_depths')\n",
    "axs[0].set_xticks(np.arange(len(max_depths)))\n",
    "axs[0].set_xticklabels(max_depths)\n",
    "\n",
    "axs[0].set_ylabel('n_estimators')\n",
    "axs[0].set_yticks(np.arange(len(n_estimators)))\n",
    "axs[0].set_yticklabels(n_estimators)\n",
    "\n",
    "axs[0].set_title('Train')\n",
    "\n",
    "# test error\n",
    "\n",
    "im1 = axs[1].imshow(test_scores.reshape((len(n_estimators), len(max_depths))), vmin=0, vmax=1, cmap='Blues')\n",
    "plt.colorbar(im1, ax=axs[1])\n",
    "\n",
    "axs[1].set_xlabel('max_depths')\n",
    "axs[1].set_xticks(np.arange(len(max_depths)))\n",
    "axs[1].set_xticklabels(max_depths)\n",
    "\n",
    "axs[1].set_ylabel('n_estimators')\n",
    "axs[1].set_yticks(np.arange(len(n_estimators)))\n",
    "axs[1].set_yticklabels(n_estimators)\n",
    "\n",
    "axs[1].set_title('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1D4ybmTtbNY"
   },
   "source": [
    "## Optimización de múltiples hiper-parámetros\n",
    "\n",
    "Para analizar qué sucede con más de dos hiper-parámetros, no podemos pintar ningún tipo de gráfico al tener más de tres dimensiona a representar (dos para los hiper-parámetros y una para el error). Por ello, en lugar de representar gráficamente el error lo hacemos de forma tabulada.\n",
    "\n",
    "[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) es una excelente herramienta que prueba de forma exahstiva todas las combinaciones de hièr-parámetros para unos rangos pre-seleccionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWaEqiBIlip9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "  'n_estimators': np.arange(5,51,10),\n",
    "  'max_depth': np.arange(1,30,5),\n",
    "  'criterion': ('gini', 'entropy')\n",
    "}\n",
    "\n",
    "rf =  RandomForestClassifier(random_state=42)\n",
    "gs = GridSearchCV(rf, parameters, cv=3)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYDRKXalu1JU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pjwkg-irvRUu"
   },
   "source": [
    "El problema de *Grid Search* es que es profundamente explorativo, por lo que los tiempos de ejecución suelen dispararse. Como alternativa disponemos de [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) que selecciona aleatoriamente combinaciones de hiper-parámetros dentro del rango elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0PH_hMZvmlS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {\n",
    "  'n_estimators': np.arange(5,51,5),\n",
    "  'max_depth': np.arange(1,30,3),\n",
    "  'criterion': ('gini', 'entropy')\n",
    "}\n",
    "\n",
    "rf =  RandomForestClassifier(random_state=42)\n",
    "rs = RandomizedSearchCV(rf, parameters, n_iter=10)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoV6eHzrv2Et"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(rs.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWHu4w6i0xEh"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ajuste de hiper-parámetros.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
