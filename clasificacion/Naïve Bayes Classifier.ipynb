{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:36:39.236507Z",
     "start_time": "2021-05-17T10:36:34.658672Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install make-spirals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgOw35LtltU-"
   },
   "source": [
    "# Naïve Bayes Classifier\n",
    " \n",
    "El clasificador *Naïve Bayes* es un algoritmo de aprendizaje supervisado basado en el Teorema de Bayes. El adjetivo *naïve* (en español *ingenuo*) se establece por asumir independencia entre cada para de características (*features*) dada la clase a la que pertenece la muestra.\n",
    " \n",
    "El Teorema de Bayes tiene la siguiente formulación:\n",
    " \n",
    "$$\n",
    "P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\n",
    "$$\n",
    " \n",
    "Donde:\n",
    " \n",
    "- $P(A)$ y $P(B)$ son las probabilidades a priori.\n",
    "- $P(B|A)$ es la probabilidad de $B$ condicionada por $A$.\n",
    "- $P(A|B)$ es la probabilidad a posteriori.\n",
    " \n",
    "Cuando trabajamos en un contexto de *machine learning*, y más concretamente en un problema de clasificación, disponemos de un conjunto de datos con $n$ muestras de la forma $x_1,\\dots,x_m$ etiquetadas para una clase $y$. Si le damos un enfoque probabilístico a esta clasificación, nuestro objetivo será calcular la probabilidad a posteriori $P(y|x_1,\\dots,x_m)$, es decir, dime la probabilidad de pertenencia a la clase $y$ dada la muestra $x_1,\\dots,x_m$.\n",
    " \n",
    "Por tanto, podemos reformular el Teorema de Bayes del siguiente modo:\n",
    " \n",
    "$$\n",
    "P(y|x_1,\\dots,x_m)=\\frac{P(y) \\cdot P(x_1,\\dots,x_m|y)}{P(x_1,\\dots,x_m)}\n",
    "$$\n",
    "\n",
    "Observamos que $P(x_1,\\dots,x_m|y)$ puede descomponerse del siguiente modo:\n",
    "\n",
    "$$\n",
    "P(x_1,\\dots,x_m|y) = P(x_1|y) \\cdot P(x_2,\\dots,x_m|y) = P(x_1|y) \\cdot P(x_2|y,x_1) \\cdot P(x_3,\\dots,x_m|y,x_1,x_2) = P(x_1|y) \\cdot P(x_2|y,x_1) \\cdot P(x_3|y,x_1,x_2) \\cdots P(x_m|y,x_1,\\dots,x_{m-1})\n",
    "$$\n",
    "\n",
    "Puesto que *Naïve Bayes* asume independencia condicional entre las variables $x_1,\\dots,x_m$, se verifica que:\n",
    "\n",
    "$$\n",
    "P(x_2|y,x_1) = P(x_2|y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_3|y,x_1,x_2) = P(x_3|y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_m|y,x_1,\\dots,x_{m-1}) = P(x_m|y)\n",
    "$$\n",
    "\n",
    "Y por lo tanto:\n",
    "\n",
    "$$\n",
    "P(x_1,\\dots,x_m|y) = \\prod_{j=1}^{m} P(x_j|y)\n",
    "$$\n",
    "\n",
    "Dejando reformulado el cáclulo de $P(y|x_1,\\dots,x_m)$:\n",
    " \n",
    "$$\n",
    "P(y|x_1,\\dots,x_m) = \\frac{P(y) \\prod_{j=1}^{m} P(x_j|y)}{P(x_1,\\dots,x_m)}\n",
    "$$\n",
    " \n",
    "Observamos que $P(x1,\\dots,x_m)$ únicamente depende de una muestra $x$ y, por lo tanto, adquiere un valor constante una vez prefijado el conjunto de datos de entrada sobre el que aprende el algoritmo. Por ello, podemos, de nuevo, simplificar la ecuación anterior a:\n",
    " \n",
    "$$\n",
    "P(y|x_1,\\dots,x_m) \\propto P(y) \\prod_{j=1}^{m} P(x_j|y)\n",
    "$$\n",
    " \n",
    "En definitiva $P(y|x_1,\\dots,x_m)$ nos devuelve una puntuación de pertenencia de una muestra $x_1,\\dots,x_m$ a la clase $y$ siendo dicha puntuación es proporcional a la probabilidad de pertenecer a esa clase. Por tanto, podemos determinar la predicción del clasificador ($\\hat{y}$) buscando la clase que maximice dicha puntuación:\n",
    " \n",
    "$$\n",
    "\\hat{y} = \\arg \\max_y P(y) \\prod_{j=1}^{m} P(x_j|y)\n",
    "$$\n",
    " \n",
    "Resaltar que $\\hat{y}$ no devuelve la probabilidad de pertenecer a una clase al haber suprimido $P(x_1,\\dots,x_m)$ de la ecuación. Sin embargo, al ser el conjunto de clases finito, podemos recuperar esta probabilidad atendiendo a la siguiente ecuación:\n",
    " \n",
    "$$\n",
    "P(y=c|x_1,\\dots,x_m) = \\frac{P(y=c) \\prod_{j=1}^{m} P(x_j|y=c)}{\\sum_{k=1}^{K} \\left(P(y=k) \\prod_{j=1}^{n} P(x_j|y=k) \\right)}\n",
    "$$\n",
    " \n",
    "Siendo $K$ el número total de clases que se desea predecir.\n",
    " \n",
    "A pesar de la simplicidad del método y de asumir independencia entre cada par de variables, situación que rara vez se da en la vida real, el clasificador Naïve Bayes funciona sorprendentemente bien con conjuntos de datos del mundo real. En especial, ha sido ampliamente utilizado para trabajar con clasificación de textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LtTovaGArTj"
   },
   "source": [
    "Cuando queremos aplicar el clasificador *Naïve Bayes* en un contexto de *machine learning* deberemos aprender $P(y)$ y $P(x_j|y)$. \n",
    "\n",
    "El aprendizaje de $P(y)$ resulta trivial analizando el número de muestras de nuestro conjunto de datos que pertenecen a cada clase. Además, este valor, cuando el conjunto de datos está perfectamente balanceado (i.e. existen exactamente el mismo número de muestras de cada clase) dicho valor es constante para todas las clases y no influye en la etiqueta asignada en la predicción ($\\hat{y}$).\n",
    "\n",
    "Por lo tanto, la esencia del clasificador *Naïve Bayes* radica en el cálculo de $P(x_j|y)$. Analizaremos cada una de las posibles implementaciones en los siguientes apartados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoINhwC2Cc1C"
   },
   "source": [
    "## Categorical Naïve Bayes\n",
    " \n",
    "La implementación de *Categorical Naïve Bayes* es la más simple e intuitiva que podemos encontrar. Esta implementación asume que todas las características del conjunto de datos ($x_1,\\dots,x_n$) son discretas y, por tanto, toman sus valores a partir de un conjunto pre-establecido.\n",
    " \n",
    "Por ello, el cálculo de $P(x_j|y)$ se reduce a \"contar casos\" tal y como se indica en la siguiente ecuación:\n",
    " \n",
    "$$\n",
    "P(x_j=t|y=c) = \\frac{\\#\\lbrace i \\in S | x_{i,j}=t, y_i=c \\rbrace + \\alpha}{\\#\\lbrace i \\in S | y_i=c \\rbrace + \\alpha \\cdot n_j}\n",
    "$$\n",
    " \n",
    "Donde $S={1, 2, 3,\\dots,m}$ denota las $m$ muestras del conjunto de datos y $n_j$ indica el número de valores posibles de la característica $j$.\n",
    " \n",
    "Observamos, además, que se añade un hiper-parámetro $\\alpha$ para evitar  $P(x_j|y)=0$.\n",
    " \n",
    "En `sklearn` este algoritmo se encuentra implementado en la clase [`sklearn.naive_bayes.CategoricalNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB). Es importante destacar que para poder usarse las categorías asociadas a cada característica deben asociarse a un valor numérico (iniciado en 0) haciendo uso, por ejemplo, de [`sklearn.preprocessing.OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html).\n",
    " \n",
    "Veamos un ejemplo con un conjunto de datos sencillo:\n",
    " \n",
    "| x1 | x2 | y |\n",
    "| -- | -- | - |\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 2 |\n",
    "| 2  | 1  | 2 |\n",
    " \n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:36:41.446188Z",
     "start_time": "2021-05-17T10:36:41.438615Z"
    },
    "id": "D185KImQJWp7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 0], [1, 0] , [1, 1], [2, 1]])\n",
    "y = np.array([1, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:36:42.840107Z",
     "start_time": "2021-05-17T10:36:42.296422Z"
    },
    "id": "W6-OpzBnJoVi"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf = CategoricalNB(alpha=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s7orLfL7SDp"
   },
   "source": [
    "El atributo `category_count_`devuelve los contadores obtenidos durante el entrenamiento. Devuelve una lista de matrices en las que: cada elemento de la lista se corresponde con una *feature*, cada fila de las matrices se corresponde con una clase y cada columna de las matrices con uno de los posibles valores de la *feature*. Las celdas contienen los contadores.\n",
    " \n",
    "Por ejemplo, observamos que el número de veces que el valor de `x1` toma el valor `0` para la clase `1` (`clf.category_count_[0][0][0]`) es `1` o que el número de veces que el valor de `x2` toma el valor `1` para la clase `2` (`clf.category_count_[1][1][1]`) es `2`.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:36:51.073794Z",
     "start_time": "2021-05-17T10:36:51.063066Z"
    },
    "id": "BYD3yODGJ0nB"
   },
   "outputs": [],
   "source": [
    "clf.category_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzFWerqXClpO"
   },
   "source": [
    "## Gaussian Naïve Bayes\n",
    " \n",
    "La implementación *Gaussian Naive Bayes* es la más frecuente en contextos de *machine learning*, ya que asume que las características del conjunto de datos ($x_1,\\dots,x_n$) son continuas y, además, siguen una distribución normal. Aunque esto no sea cierto, esta suposición proporciona generalmente buenos resultados de clasificación.\n",
    " \n",
    "Empleando una distribución normal, el cálculo de $P(x_j|y)$ se reduce a emplear la función de densidad de la distribución gausiana:\n",
    " \n",
    "$$\n",
    "P(x_j|y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{j,y}}}{exp \\left( - \\frac{\\left( x_j - \\mu_{j,y} \\right)^2}{2\\sigma^2_{j,y}} \\right)}\n",
    "$$\n",
    " \n",
    "El modelo, por tanto, aprende los parámetros $\\sigma_{j,y}$ y $\\mu_{j,y}$ para cada característica $j$ y clase $y$ existente.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UPXSJsN08wk"
   },
   "source": [
    "En `sklearn` este algoritmo se encuentra implementado en la clase [`sklearn.naive_bayes.GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB). Es conveniente que los datos de entrada hayan sido previamente estandarizados.\n",
    " \n",
    "Veamos su salida sobre el siguiente conjunto de datos de ejemplo:\n",
    " \n",
    "| x1 | x2 | y |\n",
    "| -- | -- | - |\n",
    "| -1 | -1 | 1 |\n",
    "| -2 | -1 | 1 |\n",
    "| -3 | -2 | 1 |\n",
    "|  1 |  1 | 2 |\n",
    "|  2 |  1 | 2 |\n",
    "|  3 |  2 | 2 |\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:36:53.322522Z",
     "start_time": "2021-05-17T10:36:53.314605Z"
    },
    "id": "FtyvoU66zo6-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:36:54.007978Z",
     "start_time": "2021-05-17T10:36:53.995464Z"
    },
    "id": "N2uwEdRs1uQp"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LD_FGmC2pIh"
   },
   "source": [
    "El atributo `theta_` contiene los valores aprendidos de $\\mu_{j,y}$ en una matriz. Cada fila representa una clase y cada columna una de las *features*.  Observamos, por ejemplo, que $\\mu_{2,1}$ (media de la *feature* `x2` para la clase `1`) es `-1.33333333`. Dada la simplicidad del ejemplo, podemos verificar que esto es cierto, puesto que se obtiene como:\n",
    "\n",
    "$$\n",
    "\\mu_{2,1} = \\frac{(-1) + (-1) + (-2)}{3} = - \\frac{4}{3} = -1.33333333\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:38:36.038670Z",
     "start_time": "2021-05-17T10:38:36.029813Z"
    },
    "id": "ndyNUqmA0Fmy"
   },
   "outputs": [],
   "source": [
    "clf.theta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AOsQd7P1woM"
   },
   "source": [
    "El atributo `sigma_` contiene los valores aprendidos de $\\sigma_{j,y}$ en una matriz. Al igual que antes. cada fila representa una clase y cada columna una de las *features*. Observamos, por ejemplo, que $\\sigma_{1,1}$ (desviación típica de la *feature* `x1` para la clase `1`) es `0.66666667`.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:38:36.887610Z",
     "start_time": "2021-05-17T10:38:36.878492Z"
    },
    "id": "qBH7tis5zsOO"
   },
   "outputs": [],
   "source": [
    "clf.sigma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2t0fIw1sn09"
   },
   "source": [
    "Como cualquier clasificador, en esencia, Naïve Bayes busca encontrar el hiper-plano de separación entre las clases a clasificar. Al operar Gaussian Naïve Bayes sobre valores contínuos podemos dibujar dicho hiper-plano y asociarlo a la probabilidad de pertenecer a cada una de las clases a analizar.\n",
    "\n",
    "La siguiente imagen muestra dos nubes de puntos generadas a partir de dos distribuciones guasianas y la correspondiente clasificación realizada por Naïve Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:05.502894Z",
     "start_time": "2021-05-17T10:42:05.268602Z"
    },
    "id": "SUY2uPHPtK0o"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=4, random_state=42)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "# Raw data\n",
    "\n",
    "ax1.set_title('Raw data')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_xlim(min[0], max[0])\n",
    "ax1.set_ylim(min[1], max[1])\n",
    "\n",
    "ax1.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "# Classified data\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape) \n",
    "\n",
    "ax2.set_title('Classification')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_xlim(min[0], max[0])\n",
    "ax2.set_ylim(min[1], max[1])\n",
    "\n",
    "ax2.pcolormesh(xx, yy, Z, cmap='bwr_r')\n",
    "ax2.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zGCRblTxaSC"
   },
   "source": [
    "Naïve Bayes funciona también correctamente para problemas de clasificación multiclase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:14.461472Z",
     "start_time": "2021-05-17T10:42:14.214736Z"
    },
    "id": "y9fl5AUqwIwp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=4, random_state=42)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "# Raw data\n",
    "\n",
    "ax1.set_title('Raw data')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_xlim(min[0], max[0])\n",
    "ax1.set_ylim(min[1], max[1])\n",
    "\n",
    "ax1.scatter(X[:,0], X[:,1], c=y, cmap='rainbow')\n",
    "\n",
    "# Classified data\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape) \n",
    "\n",
    "ax2.set_title('Classification')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_xlim(min[0], max[0])\n",
    "ax2.set_ylim(min[1], max[1])\n",
    "\n",
    "ax2.contourf(xx, yy, Z, cmap='rainbow', alpha=0.7, antialiased=True)\n",
    "ax2.scatter(X[:,0], X[:,1], c=y, cmap='rainbow', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kiw3upCyZOv"
   },
   "source": [
    "Sin embargo, el principio de ingenuidad (i.e. la suposición de independencia entre las variables) complica en gran medida la clasificación no lineal de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:15.306098Z",
     "start_time": "2021-05-17T10:42:15.008579Z"
    },
    "id": "bZiPLNp1xhXF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Raw data\n",
    "\n",
    "ax1.set_title('Raw data')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_xlim(min[0], max[0])\n",
    "ax1.set_ylim(min[1], max[1])\n",
    "\n",
    "ax1.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "# Classified data\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape) \n",
    "\n",
    "ax2.set_title('Classification')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_xlim(min[0], max[0])\n",
    "ax2.set_ylim(min[1], max[1])\n",
    "\n",
    "ax2.pcolormesh(xx, yy, Z, cmap='bwr_r', vmin=0, vmax=1)\n",
    "ax2.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:15.605846Z",
     "start_time": "2021-05-17T10:42:15.405361Z"
    },
    "id": "zLO5zaX7ymVJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "# Raw data\n",
    "\n",
    "ax1.set_title('Raw data')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_xlim(min[0], max[0])\n",
    "ax1.set_ylim(min[1], max[1])\n",
    "\n",
    "ax1.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "# Classified data\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape) \n",
    "\n",
    "ax2.set_title('Classification')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_xlim(min[0], max[0])\n",
    "ax2.set_ylim(min[1], max[1])\n",
    "\n",
    "ax2.pcolormesh(xx, yy, Z, cmap=plt.cm.bwr_r, vmin=0, vmax=1)\n",
    "ax2.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:16.015451Z",
     "start_time": "2021-05-17T10:42:15.794058Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from make_spirals import make_spirals\n",
    "\n",
    "X, y = make_spirals(n_samples=1000, random_state=42)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "# Raw data\n",
    "\n",
    "ax1.set_title('Raw data')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_xlim(min[0], max[0])\n",
    "ax1.set_ylim(min[1], max[1])\n",
    "\n",
    "ax1.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "# Classified data\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape) \n",
    "\n",
    "ax2.set_title('Classification')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_xlim(min[0], max[0])\n",
    "ax2.set_ylim(min[1], max[1])\n",
    "\n",
    "ax2.pcolormesh(xx, yy, Z, cmap=plt.cm.bwr_r, vmin=0, vmax=1)\n",
    "ax2.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjGe8ulsCf92"
   },
   "source": [
    "## Multinominal Naïve Bayes\n",
    " \n",
    "La implementación *Multinomial Naïve Bayes* presupone que las muestras del conjunto de datos siguen una distribución multinomial. Esta situación sucede muy frecuentemente en el contexto de la clasificación de texto, donde, al utilizar la técnica de *bag of words* para representar un documento de texto, las muestras toman una distribución multinomial.\n",
    " \n",
    "Recordemos que *bag of words* se encarga de construir un diccionario con las $n$ palabras que aparecen en los textos evaluados y luego representa cada una de las muestras mediante $x_1,\\dots,x_n$, conteniendo $x_j$ el número de veces que aparece la $j$-ésima palabra en documento.\n",
    " \n",
    "En este caso, $P(x_j|y)$ se calcula en función de la frecuencia relativa de la palabra en el documento:\n",
    " \n",
    "$$\n",
    "P(x_j|y)=\\frac{\\alpha + \\sum_{i \\in S_y} x_{i,j}}{\\alpha \\cdot n + \\sum_{j^\\ast = 1}^{n} \\sum_{i \\in S_y} x_{i,j^\\ast}}\n",
    "$$\n",
    " \n",
    "Donde $S_y$ representa las muestras de la clase $y$ y $\\alpha$ es un hiper-parámetro para evitar las probabilidades con valor 0.\n",
    " \n",
    "En `sklearn` este algoritmo se encuentra implementado en la clase [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB).\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_vVfh1b01B3"
   },
   "source": [
    "Veamos estos con un sencillo ejemplo que consta de los siguientes 4 textos:\n",
    "\n",
    "| Texto                | Clase    |\n",
    "| -------------------- | -------- |\n",
    "| \"fútbol madrid\"      | fútbol   |\n",
    "| \"fútbol barcelona\"   | fútbol   |\n",
    "| \"política madrid\"    | política |\n",
    "| \"política barcelona\" | política |\n",
    "\n",
    "\n",
    "Tras aplicar *bag of words* nos quedaría el siguiente conjunto de datos:\n",
    "\n",
    "| fútbol | madrid | barcelona | política | y |\n",
    "| ------ | ------ | --------- | -------- | - |\n",
    "| 1      | 1      | 0         | 0        | 1 |\n",
    "| 1      | 0      | 1         | 0        | 1 |\n",
    "| 0      | 1      | 0         | 1        | 2 |\n",
    "| 0      | 0      | 1         | 1        | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:17.504412Z",
     "start_time": "2021-05-17T10:42:17.496138Z"
    },
    "id": "FCUwRXD9zjZH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1, 0, 0], [1, 0, 1, 0,] , [0, 1, 0, 1], [0, 0, 1, 1]])\n",
    "y = np.array([1, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:18.036539Z",
     "start_time": "2021-05-17T10:42:18.022809Z"
    },
    "id": "YQmXmgd81zAn"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GKi_Xxi1ztQ"
   },
   "source": [
    "Mediante el atributo `feature_count_` podemos recuperar las frecuencias absolutas de cada palabra en cada clase. El atributo contiene una matriz en la que las filas son las clases, las columnas las palabras y las celdas contienen los contadores. Vemos, por ejemplo, que para la clase `1` la palabra `fútbol` aparece `2` veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:19.198477Z",
     "start_time": "2021-05-17T10:42:19.190170Z"
    },
    "id": "iHT7R2COzuZG"
   },
   "outputs": [],
   "source": [
    "clf.feature_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxCD6Q9E2NrC"
   },
   "source": [
    "El atributo `feature_log_prob` permite recuperar $log \\left(P(x_j|y)\\right)$. Al igual que antes, el atributo contiene una matriz en la que las filas son las clases, las columnas las palabras y las celdas contienen las log. probabilidades. Observamos que las palabras `madrid` y `barcelona` no tienen mucha incidencia en determinar la clase, mientras que `fútbol` y `política` si la tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:20.235874Z",
     "start_time": "2021-05-17T10:42:20.227114Z"
    },
    "id": "6R5dLsQN2hH7"
   },
   "outputs": [],
   "source": [
    "clf.feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6lsWp8-Cqiz"
   },
   "source": [
    "## Bernoulli Naïve Bayes\n",
    " \n",
    "La implementación *Bernoulli Naïve Bayes* asume que los datos siguen distribuciones de Bernoulli multivariantes. Con otras palabras, asumen que todos datos del conjunto de datos son binarios, indicando $1$ que una muestra cumple una determinada *feature* y $0$ que no la cumple.\n",
    " \n",
    "Esta representación es extremadamente frecuente en la clasificación de textos cortos (por ejemplo, *tweets*) donde aplicar *bag of words* proporcionaría muestras mayoritariamente compuestas de valores $0$ y $1$.\n",
    " \n",
    "El cálculo de $P(x_j|y)$ se realiza de acuerdo a la siguiente ecuación:\n",
    " \n",
    "$$\n",
    "P(x_j|y) = x_j \\cdot P(j|y) + (1-x_j) (1 - P(j|y))\n",
    "$$\n",
    " \n",
    "Donde $P(j|y)$ denota la probabilidad de pertenecer a la clase $y$ cuando la característica $j$ es verdadera.\n",
    " \n",
    "En `sklearn` este algoritmo se encuentra implementado en la clase [sklearn.naive_bayes.BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB). Es importante destacar que si los datos han sido binarizados previamente se debe especificar el parámetro `binarize=None`. En caso contrario, el método binarizará los datos en función del umbral especificado en `binarize` (por defecto `0.0`).\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11wSyJND6qQX"
   },
   "source": [
    "Vamos a repetir el ejemplo anterior que ya disponía de datos binarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:21.645612Z",
     "start_time": "2021-05-17T10:42:21.637733Z"
    },
    "id": "osljroIj5_0Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1, 0, 0], [1, 0, 1, 0,] , [0, 1, 0, 1], [0, 0, 1, 1]])\n",
    "y = np.array([1, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:22.113054Z",
     "start_time": "2021-05-17T10:42:22.100126Z"
    },
    "id": "W7bpuJk86GJc"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB(alpha=0, binarize=None)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eICvJU7a6vzI"
   },
   "source": [
    "De nuevo, disponemos de los atributos `feature_count_` y `feature_log_prob_` con idéntico significado a los vistos en `MultinomialNB`. El único cambio se produce en el cálculo de $P(x_j|y)$ y, por tanto, de los valores de `feature_log_prob_`.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:23.081855Z",
     "start_time": "2021-05-17T10:42:23.072820Z"
    },
    "id": "z8w60pjq6X-Z"
   },
   "outputs": [],
   "source": [
    "clf.feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgwroM44CuIs"
   },
   "source": [
    "## Complement Naïve Bayes\n",
    " \n",
    "La implementación *Complement Naïve Bayes* es una evolución de *Multinomial Naïve Bayes* que funciona particularmente bien cuando las clases están desbalanceadas (una clase tiene muchas más muestras que otras). Al igual que *Multinomial Naïve Bayes*, ofrece muy buenos resultados para la clasificación de textos.\n",
    " \n",
    "A pesar de englobarse dentro de la familia de los clasificadores *Naïve Bayes* su funcionamiento difiere de los mismos.\n",
    " \n",
    "A continuación se expresan las ecuaciones que permiten entrenar el modelo.\n",
    " \n",
    "$$\n",
    "\\alpha = \\sum_{j=1}^{n} \\alpha_j\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\hat{\\theta}_{c,j} = \\frac{\\alpha_j + \\sum_{i \\notin S_c} d_{i,j}}{\\alpha + \\sum_{i \\notin S_c} \\sum_{j^\\ast = 1}^{n} d_{i,j^\\ast}}\n",
    "$$\n",
    " \n",
    "$$\n",
    "w_{c,j} = log(\\hat{\\theta}_{c,j})\n",
    "$$\n",
    " \n",
    "$$\n",
    "w_{c,j}^\\prime = \\frac{w_{c,j}}{\\sum_{j^\\ast = 1}^{n} |w_{c,j^\\ast}|}\n",
    "$$\n",
    " \n",
    "Representando $d_{i,j}$ bien la frecuencia absoluta, bien el valor de TF-IDF, de la palabra $j$ en el documento $i$, y siendo $\\alpha$ un hiper-parámetro del modelo para evitar las probabilidades con valor $0$.\n",
    " \n",
    "Obtenidos los valores $w_{c,j}^\\prime$, la predicción $\\hat{y}$ se obtendrá como la clase $c$ que minimice la siguiente expresión para un documento $x_1,\\cdots,x_n$.\n",
    " \n",
    "$$\n",
    "\\hat{y} = \\arg \\min_c \\sum_{j=1}^{n} x_j \\cdot w_{c,j}^\\prime\n",
    "$$\n",
    " \n",
    "En `sklearn` este algoritmo se encuentra implementado en la clase [sklearn.naive_bayes.ComplementNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_ci8V-EQ30z"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOe7/Vi18d0Go8WryE/VY4Z",
   "collapsed_sections": [],
   "name": "Naïve Bayes Classifier.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
