{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:55:30.867308Z",
     "start_time": "2021-05-19T13:55:25.523109Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install make-spirals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfubmTtSvjyx"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "Como hemos comprobado, los *árboles de decisión* tienden a hacer **sobreajuste o overfitting**. Este **sobreajuste** resulta ser una propiedad general de los árboles de decisión: es muy fácil ir demasiado profundo en el árbol, y así ajustar los detalles de los datos particulares en lugar de las propiedades generales de las distribuciones de las que se extraen.\n",
    "\n",
    "Al mismo tiempo hemos podido comprobar que es fácil generar *árboles de decisión* diferentes para un mismo conjunto de datos de entrada. Además, limitando las posibilidades del algoritmo para añadir nodos al árbol podemos generar modelos que, si bien presentan problemas de **sobreajuste**, están focalizados en subconjunto específico del conjunto de datos. Por lo tanto, parece razonable combinar varios de estos modelos sobreajustados para intentar mitigar el efecto de sus **sobreajustes**.\n",
    "\n",
    "Esta idea de que se pueden combinar múltiples estimadores con *overfitting* para reducir el efecto de este sobreajuste es lo que subyace a un método de ensamble llamado \"bagging\". El \"bagging\" hace uso de un conjunto de estimadores paralelos, cada uno de los cuales se ajusta en exceso (con *overfitting*) a los datos, y promedia los resultados para encontrar una mejor clasificación. Un **conjunto de árboles de decisión aleatorios** se conoce como **random forest**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttkNKW9yyF02"
   },
   "source": [
    "## Random Forests 'a mano'\n",
    "\n",
    "Como ya se ha dicho, un **Random Forest**  no es más que un **ensemble _bagging_** de **árboles de decisión aleatorio**. *Scikit-learn* nos proporciona un objeto para instanciar y usar **baggings** de algún otro modelo de aprendizaje computacional. En nuestro caso, vamos a usar este método para definir y entrenar un **Random Forest** construido a partir de árboles aleatorios de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:55:30.885145Z",
     "start_time": "2021-05-19T13:55:30.873542Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Sg842HXDvfx4"
   },
   "outputs": [],
   "source": [
    "## Función auxiliar para pintar las fronteras de decisión de un clasificador\n",
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nmBbx_3zThq"
   },
   "source": [
    "Primero vamos a generar un *dataset* sintético con la función `make_blobs`. Este *dataset* constará de 300 observaciones con 4 categorías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:58:20.264350Z",
     "start_time": "2021-05-19T13:58:19.738654Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OJI5anFpzZsj"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hq3SO4ppzec7"
   },
   "source": [
    "Cuando construimos un *Random Forest* a partir del [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) debemos especificar:\n",
    "\n",
    "- `n_estimators`: el número de árboles de que se compondrá nuestro bosque.\n",
    "- `max_samples`: el porcentaje de muestras del dataset que se usarán para construir cada bosque.\n",
    "\n",
    "Podemos ilustrar gráficamente qué cómo se combinan las árboles comparando las fronteras de decisión de cada árbol que compone el bosque recuperándolas mediante el atributo `estimators_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:55:33.994313Z",
     "start_time": "2021-05-19T13:55:31.863739Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import numpy as np\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=8, max_samples=0.3, random_state=1)\n",
    "bag.fit(X, y)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15,15))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "min, max = np.amin(X, axis=0), np.amax(X, axis=0)\n",
    "diff = max - min\n",
    "min, max = min - 0.1 * diff, max + 0.1 * diff\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        \n",
    "        axs[i,j].set_xlabel('X1')\n",
    "        axs[i,j].set_ylabel('X2')\n",
    "\n",
    "        axs[i,j].set_xlim(min[0], max[0])\n",
    "        axs[i,j].set_ylim(min[1], max[1])\n",
    "                \n",
    "        xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "        \n",
    "        if i == 0 and j == 0:\n",
    "            axs[i,j].set_title('Random Forest')\n",
    "            Z = bag.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            axs[i,j].contourf(xx, yy, Z, alpha=0.4, cmap='rainbow')\n",
    "        \n",
    "        else:\n",
    "            t = 3 * i + j - 1 # tree index\n",
    "    \n",
    "            axs[i,j].set_title('Decision Tree #' + str(t+1))\n",
    "            Z = bag.estimators_[t].predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            axs[i,j].contourf(xx, yy, Z, alpha=0.4, cmap='rainbow')\n",
    "            \n",
    "        axs[i,j].scatter(X[:,0], X[:,1], c=y, edgecolor='black', s=20, cmap='rainbow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-09z-VVL0OYH"
   },
   "source": [
    "En este ejemplo, hemos aleatorizado los datos ajustando cada modelo con un **subconjunto aleatorio del 30%** de los puntos de entrenamiento. En la práctica, los **árboles de decisión** se aleatorizan en mayor medida, inyectando cierta aleatoriedad en la forma en que se eligen las divisiones: de esta manera todos los datos contribuyen al ajuste cada vez, pero los resultados del ajuste siguen teniendo la aleatoriedad deseada. Por ejemplo, al determinar la característica en la que se va a dividir, el árbol aleatorio podría seleccionar entre las varias características principales.\n",
    "\n",
    "También posible aleatoreizar las características que se van a usar para cada uno de los árboles mediante el parámetro `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T21:04:40.919599Z",
     "start_time": "2021-05-20T21:04:39.932597Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import numpy as np\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=8, max_samples=0.3, max_features=1, random_state=1)\n",
    "bag.fit(X, y)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15,15))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "min, max = np.amin(X, axis=0), np.amax(X, axis=0)\n",
    "diff = max - min\n",
    "min, max = min - 0.1 * diff, max + 0.1 * diff\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        \n",
    "        axs[i,j].set_xlabel('X1')\n",
    "        axs[i,j].set_ylabel('X2')\n",
    "\n",
    "        axs[i,j].set_xlim(min[0], max[0])\n",
    "        axs[i,j].set_ylim(min[1], max[1])\n",
    "                \n",
    "        x1, x2 = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "        grid = np.c_[x1.ravel(), x2.ravel()]\n",
    "        \n",
    "        if i == 0 and j == 0:\n",
    "            axs[i,j].set_title('Random Forest')\n",
    "            Z = bag.predict(grid)\n",
    "            Z = Z.reshape(x1.shape)\n",
    "            axs[i,j].contourf(x1, x2, Z, alpha=0.4, cmap='rainbow')\n",
    "        \n",
    "        else:\n",
    "            t = 3 * i + j - 1 # tree index\n",
    "    \n",
    "            axs[i,j].set_title('Decision Tree #' + str(t+1))\n",
    "            xt = grid[:,bag.estimators_features_[t]]\n",
    "            Z = bag.estimators_[t].predict(xt)\n",
    "            Z = Z.reshape(x1.shape)\n",
    "            axs[i,j].contourf(x1, x2, Z, alpha=0.4, cmap='rainbow')\n",
    "            \n",
    "        axs[i,j].scatter(X[:,0], X[:,1], c=y, edgecolor='black', s=20, cmap='rainbow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AP3NQ4xn1mKB"
   },
   "source": [
    "## Random Forest con Scikit-Learn\n",
    "\n",
    "En *Scikit-Learn*, este conjunto optimizado de árboles de decisión aleatorios se implementa en el modelo `RandomForestClassifier`, que se encarga de toda la aleatorización de forma automática. Al igual que sucedía con el *bagging*, debemos seleccionar su número de árboles (`n_estimators`) así como la profundidad máxima (`max_depth`), el número de *features* a usar (`max_features`) y el número de muestras (`max_samples`) de cada árbol.\n",
    "\n",
    "Veamos su desempeño sobre diferentes conjuntos de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:55:42.849225Z",
     "start_time": "2021-05-19T13:55:36.200654Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "\n",
    "from make_spirals import make_spirals\n",
    "\n",
    "def plot_clasification(X, y, axs):\n",
    "  min = np.amin(X, axis=0)\n",
    "  max = np.amax(X, axis=0)\n",
    "\n",
    "  diff = max - min\n",
    "\n",
    "  min = min - 0.1 * diff\n",
    "  max = max + 0.1 * diff\n",
    "\n",
    "  axs[0].set_title('Raw data')\n",
    "  axs[0].set_xlabel('X1')\n",
    "  axs[0].set_ylabel('X2')\n",
    "\n",
    "  axs[0].set_xlim(min[0], max[0])\n",
    "  axs[0].set_ylim(min[1], max[1])\n",
    "\n",
    "  axs[0].scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "  for i, max_depth in enumerate([1,2,3]):\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=1)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape) \n",
    "    \n",
    "    axs[i+1].set_title('max_depth=' + str(max_depth))\n",
    "    axs[i+1].set_xlabel('X1')\n",
    "    axs[i+1].set_ylabel('X2')\n",
    "\n",
    "    axs[i+1].set_xlim(min[0], max[0])\n",
    "    axs[i+1].set_ylim(min[1], max[1])\n",
    "\n",
    "    axs[i+1].contourf(xx, yy, Z, alpha=0.5, cmap=plt.cm.bwr)\n",
    "    axs[i+1].scatter(X[:,0], X[:,1], c=y, edgecolor='black', s=20, cmap=plt.cm.bwr )\n",
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20,20))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=2, random_state=42)\n",
    "plot_clasification(X, y, axs[0])\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "plot_clasification(X, y, axs[1])\n",
    "\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "plot_clasification(X, y, axs[2])\n",
    "\n",
    "X, y = make_spirals(n_samples=1000, random_state=42)\n",
    "plot_clasification(X, y, axs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqRy0i3p1Msp"
   },
   "source": [
    "Podemos observar que promediando 100 estimadores aleatorios, los cuales probablemente sufran de **overfitting**, obtenemos un modelo que mitiga el efecto de dicha **sobre-estimación**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0LefBln2Mln"
   },
   "source": [
    "## Probando el rendimiento de Random Forest\n",
    "\n",
    "Para probar el rendimiento de este modelo vamos a utilizar un conjunto de datos sobre imágenes escaneadas de caracteres del alfabeto anglosajón alojado en el [repositorio UCI](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition). \n",
    "\n",
    "Según la descripción del *dataset*:\n",
    "\n",
    ">el objetivo es identificar imágenes rectangulares en blanco y negro como una de las 26 letras mayúsculas del alfabeto inglés. Las imágenes de los caracteres se basaron en 20 fuentes diferentes y cada letra dentro de estas 20 fuentes fue distorsionada aleatoriamente para producir un archivo de 20.000 observaciones únicas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:55:45.463265Z",
     "start_time": "2021-05-19T13:55:42.852425Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KiiluR2f1KWU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features = [\n",
    "            'target', 'x-box', 'y-box', 'width', 'high ', 'onpix', 'x-bar',\n",
    "            'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', \n",
    "            'xegvy', 'y-ege', 'yegvx'\n",
    "]\n",
    "letras = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data', names=features)\n",
    "\n",
    "X = letras.iloc[:,1:]\n",
    "y = letras.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMod5Lfv-71y"
   },
   "source": [
    "Una vez cargados los datos, procedemos a realizar el ajuste y validación del modelo. Para ello vamos a usar validación cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:56:08.915398Z",
     "start_time": "2021-05-19T13:55:45.465042Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "E61PUWmL9O2v"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1337)\n",
    "\n",
    "# Primera aproximación, random forest con parámetros por defecto\n",
    "rf1 = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "scores = cross_validate(rf1, X, y, cv=cv, scoring=('accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:56:08.924136Z",
     "start_time": "2021-05-19T13:56:08.917839Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5IiC-2eO_VpF"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:56:08.944362Z",
     "start_time": "2021-05-19T13:56:08.925646Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BPXZOKzbDOC3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:56:15.520936Z",
     "start_time": "2021-05-19T13:56:08.945675Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 20]\n",
    "plot_confusion_matrix(rf, X_test, y_test, cmap='Blues', normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Creado por **Raúl Lara Cabrera** (raul.lara@upm.es) y **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOMmyH5d7nFD7CXrgjZE4Yp",
   "collapsed_sections": [],
   "name": "RandomForest.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
