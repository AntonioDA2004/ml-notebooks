{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Support Vector Machines.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNKahsCkDDlpqU6j25DY1nN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"z_Qv_xL0MYSF"},"source":["# Support Vector Machines (SVM)\n"," \n","## LinearSVC\n"," \n","Las Máquinas de Vectores Soporte (*Support Vector Machines*) realizan la clasificación encontrando el hiperplano que maximiza el margen entre las clases presentes en un conjunto de datos. Este concepto, que de entrada puede parecer confuso, es en realidad una idea bastante intuitiva y viene descrita por la siguiente figura:\n"," \n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/1280px-SVM_margin.png\" width=500>\n"," \n","En ella podemos observar claramente un conjunto de datos compuesto por 19 muestras (número de puntos) descritas por dos características ($x_1$ y $x_2$) que se clasifican en dos clases (azul y verde). El objetivo de cualquier clasificador es encontrar un hiperplano que separe todo el espacio en 2 de tal modo que las muestras de la clase azul queden a un lado de dicho hiperplano y las muestras de la clase verde queden al lado contrario.\n"," \n","Existen, por tanto, infinitos hiperplanos que son capaces de realizar esta división sin dejar ninguna muestra mal clasificada. Cuando utilizamos *SVM* lo que haremos será fijar este hiperplano como aquel que tenga mayor margen, es decir, aquel que mantenga la mayor distancia con aquellas muestras que está más cerca de dicho hiperplano. A este hiperplano le denominaremos límite de decisión (*decision boundary*).\n"," \n","Esta idea tan básica nos proporciona la definición de **vectores soporte** (*support vectors*) que dan nombre al método. Un vector soporte será aquella muestra que está incorrectamente clasificada o que se encuentra cerca del límite de decisión. En la figura anterior observamos 3 vectores soporte: 2 azules y 1 verde.\n"," \n","Entendida la idea conceptual del clasificador, veamos como podemos formalizarla matemáticamente. No olvidemos que estamos trabajando con un problema de clasificación (por ahora lineal) por lo que la ecuación que define la salida de nuestro clasificador será de la forma:\n"," \n","$$\n","\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i = b + \\sum_{j=1}^{n} w_j \\cdot x_{i,j}\n","$$\n"," \n","Siendo $b$ y $\\vec{w} = (w_1, w_2, \\dots, w_n)$ los parámetros que el modelo debe aprender.\n"," \n","Las Maquinas de Vector Soporte realizan una clasificación binaria atendiendo al siguiente criterio:\n"," \n","- Si $\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i \\geq 1$ la muestra pertenece a la clase 0.\n","- Si $\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i \\leq -1$ la muestra pertenece a la clase 1.\n"," \n","Nótese que los identificadores *clase 0* y *clase 1* son meras etiquetas, nada tienen que ver con los valores límite de $1$ y $-1$ definidos por SVM.\n"," \n","Enlazando esto con la explicación conceptual del método y volviendo a la figura previamente presentada, entendemos que los vectores soporte de la clase 0 son los que verifican que $b + \\vec{w} \\cdot \\vec{x}_i = 1$ mientras que los vectores soporte de la clase 1 son los que verifican que $b + \\vec{w} \\cdot \\vec{x}_i = -1$. Identificaremos por tanto dos nuevos hiperplanos: el **hiperplano positivo** como aquel en el que se sitúan los vectores soporte de la clase 0 y el **hiperplano negativo** como aquel en el que se sitúan los vectores soporte de la clase 0.\n"," \n","Con dichos hiperplanos podemos encontrar el límite de decisión como aquel hiperplano que equidiste del hiperplano positivo y del hiperplano negativo. Para ello nos apoyaremos en el concepto de **margen** (*margin*) que se encarga de medir la distancia entre los hiperplanos positivo y negativo. El valor del margen vendrá definido por la siguiente ecuación:\n"," \n","$$\n","margin = \\frac{2}{||\\vec{w}||}\n","$$\n"," \n","Valor que obtenemos si situamos dos muestras $\\vec{x}_1$ y $\\vec{x}_2$ una en frente de otra sobre los hiperplanos positivo y negativo. Analíticamente:\n"," \n","$$\n","b + \\vec{w} \\cdot \\vec{x}_1 = 1 \n","$$\n"," \n","$$\n","b + \\vec{w} \\cdot \\vec{x}_2 = -1 \n","$$\n"," \n","$$\n","(b + \\vec{w} \\cdot \\vec{x}_1) - (b + \\vec{w} \\cdot \\vec{x}_2) = 1 - (-1)\n","$$\n"," \n","$$\n","\\vec{w} \\cdot (\\vec{x}_1 - \\vec{x}_2) = 2\n","$$\n"," \n","$$\n","margin = \\vec{x}_1 - \\vec{x}_2 = \\frac{2}{||\\vec{w}||}\n","$$\n"," \n","La clave del funcionamiento de las Máquinas de Vector Soporte, y su principal diferencia respecto a otros clasificadores lineales como la Regresión Logística, radica en el hecho de que únicamente los vectores soporte tienen impacto en el entrenamiento del modelo. Todas las muestras del conjunto de datos que no sean vectores soporte no condicionan la definición del límite de decisión.\n"," \n","Para lograr esto las Máquinas de Vector Soporte se construyen apoyándose en la función de pérdida *Hinge Loss* que podemos observar a continuación:"]},{"cell_type":"code","metadata":{"id":"RkwQ4mJTVg3Q"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot([-2, -1, 0, 1, 2, 3], [3, 2, 1, 0, 0, 0])\n","plt.xlabel(\"b+w·x\")\n","plt.ylabel(\"cost\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xPBsAKUW4uL"},"source":["A menudo, a esta función se le conoce como $cost$ y tiene por objetivo sancionar únicamente a las muestras que se encuentren mal clasificadas o muy cercanas al límite de decisión. Retomemos un concepto explicado anteriormente, si se verifica que $b + \\vec{w} \\cdot \\vec{x}_i \\geq 1$ entonces $\\vec{x}_i$ pertenecerá a la clase 0. Asumiendo que la gráfica anterior hace referencia a la clase 0, todas las muestras con $b + \\vec{w} \\cdot \\vec{x}_i \\geq 1$ tendrán $cost = 0$. Dicho de otro modo, todas las muestras bien clasificadas tendrán $cost = 0$. Por el contrario, a medida que nos alejamos del hiperplano positivo ($b + \\vec{w} \\cdot \\vec{x}_i = 1$) y nos acercamos al límite de decisión ($b + \\vec{w} \\cdot \\vec{x}_i = 0$) el valor de $cost$ aumenta.\n"," \n","Entendida esta idea es evidente que necesitaremos contra función $cost$ para la clase 1, quedando definidas por tanto $cost_0$ y $cost_1$ del siguiente modo:"]},{"cell_type":"code","metadata":{"id":"vJuCgqs9Yak1"},"source":["import matplotlib.pyplot as plt\n","\n","fig, axs = plt.subplots(1,2, figsize=(15,4))\n","\n","axs[0].plot([-2, -1, 0, 1, 2, 3], [3, 2, 1, 0, 0, 0])\n","axs[0].set_xlabel(\"b+w·x\")\n","axs[0].set_ylabel(\"cost_0\")\n","\n","axs[1].plot([-3, -2, -1, 0, 1, 2], [0, 0, 0, 1, 2, 3])\n","axs[1].set_xlabel(\"b+w·x\")\n","axs[1].set_ylabel(\"cost_1\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lk4UEFrhZnHr"},"source":["Matemáticamente podemos definir las funciones anteriores con las siguientes expresiones:\n"," \n","$$\n","\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i\n","$$\n"," \n","$$\n","cost_0(\\hat{y}_i) = max(0, 1 - \\hat{y}_i)\n","$$\n"," \n","$$\n","cost_1(\\hat{y}_i) = max(0, 1 + \\hat{y}_i)\n","$$\n"," \n","Y unificarlas del siguiente modo:\n"," \n","$$\n","cost(\\hat{y}_i) = y_i \\cdot cost_1(\\hat{y}_i) + (1 - y_i) \\cdot cost_0(\\hat{y}_i)\n","$$\n"," \n","$$\n","cost(\\hat{y}_i) = y_i \\cdot max(0, 1 + \\hat{y}_i) + (1 - y_i) \\cdot max(0, 1 - \\hat{y}_i)\n","$$\n"," \n","Si tenemos en cuenta los datos de entrada obtenemos:\n"," \n","$$\n","cost(\\vec{x}_i) = y_i \\cdot max(0, 1 + b + \\vec{w} \\cdot \\vec{x}_i) + (1 - y_i) \\cdot max(0, 1 - b - \\vec{w} \\cdot \\vec{x}_i)\n","$$\n"," \n","Y finalmente podemos definir la **función de coste** de las máquinas de vector soporte añadiendo un término de regularización como:\n"," \n","$$\n","loss = C \\left[ \\sum_{i=1}^{m} y_i \\cdot max(0, 1 + b + \\vec{w} \\cdot \\vec{x}_i) + (1 - y_i) \\cdot max(0, 1 - b - \\vec{w} \\cdot \\vec{x}_i) \\right] + \\frac{1}{2} \\sum_{j=1}^{n} w_j^2\n","$$\n"," \n","Donde $m$ representa el número de muestras y $n$ el número de características o *features*. Observamos, además, que se ha añadido un híper-parámetros $C$ al modelo. $C$, al que podemos llamar no-regularización, permite controlar el proceso de aprendizaje: valores grandes de $C$ harán el modelo muy sensible a los *outlayers* mientras que valores bajos de $C$ harán el método muy generalista."]},{"cell_type":"markdown","metadata":{"id":"L-mklWGWOiUk"},"source":["Las Máquinas de Vector Soporte (lineales) se encuentra definidas en `sklearn` en la clase [`sklearn.svm.LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). Como se observa en su documenta, la clase dispone de un parámetro `C` que se corresponde con el híper-parámetro $C$ que controla la no-regularización.\n"," \n","Una vez ajustado el modelo podemos recuperar sus parámetros accediendo a sus atributos `coef_`(que contiene $\\vec{w}$) e `intercep_`(que contiene $b$).\n"," \n","Veamos un ejemplo.\n"," \n","Supongamos el siguiente conjunto de datos:"]},{"cell_type":"code","metadata":{"id":"P4cVq2VgMg-x"},"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","\n","X, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=3)\n","\n","plt.figure()\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6vTGCrX-he--"},"source":["Ajustemos un modelo mediante `LinearSVC`:"]},{"cell_type":"code","metadata":{"id":"Vkjg1a2kMiu2"},"source":["from sklearn.svm import LinearSVC\n","\n","linear_svm = LinearSVC().fit(X, y)\n","print(\"coef_ = \" + str(linear_svm.coef_))\n","print(\"intercept_ = \" + str(linear_svm.intercept_))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mG3hYzOghqcQ"},"source":["Y lo pintamos:"]},{"cell_type":"code","metadata":{"id":"AqYP0P9yMkOI"},"source":["import numpy as np\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr)\n","line = np.linspace(-10, 10)\n","plt.plot(line, -(line * linear_svm.coef_[0][0] + linear_svm.intercept_) / linear_svm.coef_[0][1], c='orange')\n","plt.ylim(-3, 8)\n","plt.xlim(-8, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kdrW__CThZW"},"source":["La clase `LinearSVC` que hemos utilizado es, aproximadamente, equivalente a la clase [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) definiendo el parámetro `kernel='linear'`.Estudiaremos esto más adelante, sin embargo, ahora vamos a hacer uso de la clase `SVC` en lugar de `LinearSVC` para poder representar los vectores soporte.\n","\n","La clase `SVC` tiene un atributo `support_vectors_` que contienen los vectores soporte utilizados para definir el límite de decisión. Veamos dichos puntos con un ejemplo gráfico:"]},{"cell_type":"code","metadata":{"id":"UgGUEEl-Rg9f"},"source":["from sklearn import svm\n","\n","cluster_std = 0.8  #@param {type: \"slider\", min: 0.1, max: 2, step: 0.1}\n","\n","X, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=cluster_std, random_state=3)\n","\n","svc = svm.SVC(kernel='linear').fit(X, y)\n","\n","plt.scatter(svc.support_vectors_[:,0], svc.support_vectors_[:,1], marker='o', color='yellow', s=140)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr)\n","\n","line = np.linspace(-10, 10)\n","plt.plot(line, -(line * svc.coef_[0][0] + svc.intercept_) / svc.coef_[0][1], color='orange')\n","plt.ylim(-3, 8)\n","plt.xlim(-8, 4);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqqy8pTTl7qR"},"source":["La explicación anterior hacer se ha centrado en detallar cómo funcionan las máquinas de vectores soporte para clasificación binaria. No obstante, también se pueden emplear para clasificación multiclase. La única diferencia es que en lugar de realizar una única clasificación binaria haremos tantas clasificaciones binarias como clases haya utilizando la estrategia *one-vs-rest* (también llamada *one-vs-all*). Esta estrategia asume que todas las muestras que no pertenecen a la clase evaluada son de la clase contraria.\n","\n","Ilustremos el resultado con un ejemplo.\n","\n","Generamos un conjunto de datos con tres clases:"]},{"cell_type":"code","metadata":{"id":"FSh-TQ_rnBOV"},"source":["plt.figure()\n","X, y = make_blobs(random_state=42)\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.cool)\n","plt.ylim(-10, 15)\n","plt.xlim(-10, 8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCrKhqASnKTD"},"source":["Ajustamos el modelo:"]},{"cell_type":"code","metadata":{"id":"MD7hkvvrnEVs"},"source":["linear_svm = LinearSVC().fit(X, y)\n","print(\"coef_ = \" + str(linear_svm.coef_))\n","print(\"intercept_ = \" + str(linear_svm.intercept_))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"958n7Lf-nOiY"},"source":["Analizamos el resultado:"]},{"cell_type":"code","metadata":{"id":"bY8FTwUcnQ1v"},"source":["plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.cool)\n","line = np.linspace(-15, 15)\n","\n","plt.plot(line, -(line * linear_svm.coef_[0][0] + linear_svm.intercept_[0]) / linear_svm.coef_[0][1], color=plt.cm.cool(0.0))\n","plt.plot(line, -(line * linear_svm.coef_[1][0] + linear_svm.intercept_[1]) / linear_svm.coef_[1][1], color=plt.cm.cool(0.5))\n","plt.plot(line, -(line * linear_svm.coef_[2][0] + linear_svm.intercept_[2]) / linear_svm.coef_[2][1], color=plt.cm.cool(1.0))\n","\n","plt.ylim(-10, 15)\n","plt.xlim(-10, 8);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oi4uhsnPqeCA"},"source":["## Non-linear SVM\n"," \n","En el apartado anterior hemos visto cómo podemos construir una Máquina de Vectores Soporte que separe clases linealmente. Sin embargo, existen infinidad de problemas en los que la separación lineal no es posible. Las Máquinas de Vectores Soporte pueden funcionar con separaciones no lineales haciendo algunas modificaciones.\n"," \n","Principalmente debemos modificar la función de coste por una nueva:\n"," \n","$$\n","loss = C \\left[ \\sum_{i=1}^{m} y_i \\cdot max(0, 1 + \\theta^T \\cdot f(\\vec{x}_i)) + (1 - y_i) \\cdot max(0, 1 - \\theta^T \\cdot f(\\vec{x}_i)) \\right] + \\frac{1}{2} \\sum_{j=1}^{n} \\theta_j^2\n","$$\n"," \n","Como vemos hemos modificado los vectores $\\vec{w}$ por matrices $\\theta$ y a los valores de los vectores $\\vec{x}_i$ se les ha aplicado una función $f$.\n"," \n","Está función $f$ se denomina **función de Kernel** y permite determinar la similaridad entre una muestra y todas las demás. Si dos muestras son idénticas entonces su similaridad deberá ser 1, mientras que si son opuestas deberá ser 0. Una de las funciones de Kernel más populares es la ***Gaussian Kernel***, que se encuentra implementada en `sklearn` bajo el acrónimo `'rbf'` (*Radial Basis Fuction Kernel*). Esta función determina la distancia euclídea entre dos vectores suavizándose mediante una distribución normal de desviación típica $\\sigma$:\n"," \n","$$\n","similarity(x_1, x_2) = exp\\left( \\frac{||x_1 - x_2||^2}{2\\sigma^2} \\right)\n","$$\n"," \n","Con este ajuste $f(\\vec{x}_i)$ devolverá un vector con la distancia de la muestra $i$ con respecto a todas las demás. Intuitivamente, las muestras cercanas a $i$ tendrán valores próximos a 1 y las muestras lejanas tendrá valores próximos a 0.\n"," \n","Por consiguiente, $\\theta$ pasará a ser una matriz cuadrada de orden $m$ (número de muestras) con los parámetros que el modelo debe aprender.\n"]},{"cell_type":"markdown","metadata":{"id":"Ph0T2hotu8nU"},"source":["Este clasificador se encuentra implementado en la clase [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) de la librería `sklearn`.\n"," \n","Se destacan dos parámetros tres parámetros de su implementación:\n"," \n","- `C` que indica la no-regularización como en `LinearSVC`.\n","- `Kernel` que permite modificar la función de Kernel. Por defecto `'rbf'`.\n","- `gamma` que modifica los parámetros de la función de Kernel. En el caso de `'rbf'`, valores altos de `gamma` indican una baja desviación típica y, por tanto, prestar más atención a las muestras más cercanas, mientras que valores bajos de `gamma` indican lo apuesto.\n"," \n","Analicemos la importancia de estos parámetros con un ejemplo gráfico:\n"," "]},{"cell_type":"code","metadata":{"id":"NyNpgOOywPDV"},"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_moons\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn import svm\n","import numpy as np\n","\n","def draw_decision_function(clf,ax):\n","  xx, yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n","  Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","  Z = Z.reshape(xx.shape) # reshape to 2D\n","  ax.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n","  return\n","\n","X, y = make_moons(n_samples=100, noise=0.08, random_state=10)\n","X = MinMaxScaler().fit_transform(X)\n","\n","fig, axs = plt.subplots(4,4, figsize=(18,12))\n","fig.tight_layout(pad=3.0)\n","\n","for i, C in enumerate([0.005, 0.01, 0.1, 1]):\n","  for j, gamma in enumerate([5,7,10,15]):\n","    \n","    clf = svm.SVC(kernel='rbf', gamma=gamma, C=C)\n","    clf.fit(X, y)\n","\n","    draw_decision_function(clf,axs[i,j])\n","    for target, color, marker in zip(range(2),['w','g'],['o','^']):\n","      axs[i,j].scatter(X[:, 0], X[:, 1], c=plt.cm.binary(y / 2.))\n","      axs[i,j].set_title('SVM, C: ' + format(C) + ', gamma: '+str(gamma))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBr87UkjRkwE"},"source":["---\n","\n","Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]}]}