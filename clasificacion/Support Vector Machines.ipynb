{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:21.986899Z",
     "start_time": "2021-05-17T10:11:17.407809Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install make-spirals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Qv_xL0MYSF"
   },
   "source": [
    "# Support Vector Machines (SVM)\n",
    " \n",
    "## LinearSVC\n",
    " \n",
    "Las Máquinas de Vectores Soporte (*Support Vector Machines*) realizan la clasificación encontrando el hiperplano que maximiza el margen entre las clases presentes en un conjunto de datos. Este concepto, que de entrada puede parecer confuso, es en realidad una idea bastante intuitiva y viene descrita por la siguiente figura:\n",
    " \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/1280px-SVM_margin.png\" width=500>\n",
    " \n",
    "En ella podemos observar claramente un conjunto de datos compuesto por 19 muestras (número de puntos) descritas por dos características ($x_1$ y $x_2$) que se clasifican en dos clases (azul y verde). El objetivo de cualquier clasificador es encontrar un hiperplano que separe todo el espacio en 2 de tal modo que las muestras de la clase azul queden a un lado de dicho hiperplano y las muestras de la clase verde queden al lado contrario.\n",
    " \n",
    "Existen, por tanto, infinitos hiperplanos que son capaces de realizar esta división sin dejar ninguna muestra mal clasificada. Cuando utilizamos *SVM* lo que haremos será fijar este hiperplano como aquel que tenga mayor margen, es decir, aquel que mantenga la mayor distancia con aquellas muestras que está más cerca de dicho hiperplano. A este hiperplano le denominaremos límite de decisión (*decision boundary*).\n",
    " \n",
    "Esta idea tan básica nos proporciona la definición de **vectores soporte** (*support vectors*) que dan nombre al método. Un vector soporte será aquella muestra que está incorrectamente clasificada o que se encuentra cerca del límite de decisión. En la figura anterior observamos 3 vectores soporte: 2 azules y 1 verde.\n",
    " \n",
    "Entendida la idea conceptual del clasificador, veamos como podemos formalizarla matemáticamente. No olvidemos que estamos trabajando con un problema de clasificación (por ahora lineal) por lo que la ecuación que define la salida de nuestro clasificador será de la forma:\n",
    " \n",
    "$$\n",
    "\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i = b + \\sum_{j=1}^{m} w_j \\cdot x_{i,j}\n",
    "$$\n",
    " \n",
    "Siendo $b$ y $\\vec{w} = (w_1, w_2, \\dots, w_m)$ los parámetros que el modelo debe aprender.\n",
    " \n",
    "Las Maquinas de Vector Soporte realizan una clasificación binaria atendiendo al siguiente criterio:\n",
    " \n",
    "- Si $\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i \\geq 1$ la muestra pertenece a la clase 0.\n",
    "- Si $\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i \\leq -1$ la muestra pertenece a la clase 1.\n",
    " \n",
    "Nótese que los identificadores *clase 0* y *clase 1* son meras etiquetas, nada tienen que ver con los valores límite de $1$ y $-1$ definidos por SVM.\n",
    " \n",
    "Enlazando esto con la explicación conceptual del método y volviendo a la figura previamente presentada, entendemos que los vectores soporte de la clase 0 son los que verifican que $b + \\vec{w} \\cdot \\vec{x}_i = 1$ mientras que los vectores soporte de la clase 1 son los que verifican que $b + \\vec{w} \\cdot \\vec{x}_i = -1$. Identificaremos por tanto dos nuevos hiperplanos: el **hiperplano positivo** como aquel en el que se sitúan los vectores soporte de la clase 0 y el **hiperplano negativo** como aquel en el que se sitúan los vectores soporte de la clase 1.\n",
    " \n",
    "Con dichos hiperplanos podemos encontrar el límite de decisión como aquel hiperplano que equidiste del hiperplano positivo y del hiperplano negativo. Para ello nos apoyaremos en el concepto de **margen** (*margin*) que se encarga de medir la distancia entre los hiperplanos positivo y negativo. El valor del margen vendrá definido por la siguiente ecuación:\n",
    " \n",
    "$$\n",
    "margin = \\frac{2}{||\\vec{w}||}\n",
    "$$\n",
    " \n",
    "Valor que obtenemos si situamos dos muestras $\\vec{x}_1$ y $\\vec{x}_2$ una en frente de otra sobre los hiperplanos positivo y negativo. Analíticamente:\n",
    " \n",
    "$$\n",
    "b + \\vec{w} \\cdot \\vec{x}_1 = 1 \n",
    "$$\n",
    " \n",
    "$$\n",
    "b + \\vec{w} \\cdot \\vec{x}_2 = -1 \n",
    "$$\n",
    " \n",
    "$$\n",
    "(b + \\vec{w} \\cdot \\vec{x}_1) - (b + \\vec{w} \\cdot \\vec{x}_2) = 1 - (-1)\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\vec{w} \\cdot (\\vec{x}_1 - \\vec{x}_2) = 2\n",
    "$$\n",
    " \n",
    "$$\n",
    "margin = \\vec{x}_1 - \\vec{x}_2 = \\frac{2}{||\\vec{w}||}\n",
    "$$\n",
    " \n",
    "La clave del funcionamiento de las Máquinas de Vector Soporte, y su principal diferencia respecto a otros clasificadores lineales como la Regresión Logística, radica en el hecho de que únicamente los vectores soporte tienen impacto en el entrenamiento del modelo. Todas las muestras del conjunto de datos que no sean vectores soporte no condicionan la definición del límite de decisión.\n",
    " \n",
    "Para lograr esto las Máquinas de Vector Soporte se construyen apoyándose en la función de pérdida *Hinge Loss* que podemos observar a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:22.176950Z",
     "start_time": "2021-05-17T10:11:21.992285Z"
    },
    "id": "RkwQ4mJTVg3Q"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "hinge = np.vectorize(lambda v: abs(min(v-1,0.0)))\n",
    "\n",
    "x = np.linspace(-4, 4)\n",
    "y = hinge(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"b+w·x\")\n",
    "plt.ylabel(\"hinge(b+w·x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xPBsAKUW4uL"
   },
   "source": [
    "A menudo, a esta función se le conoce como $cost$ y tiene por objetivo sancionar únicamente a las muestras que se encuentren mal clasificadas o muy cercanas al límite de decisión. Retomemos un concepto explicado anteriormente, si se verifica que $b + \\vec{w} \\cdot \\vec{x}_i \\geq 1$ entonces $\\vec{x}_i$ pertenecerá a la clase 0. Asumiendo que la gráfica anterior hace referencia a la clase 0, todas las muestras con $b + \\vec{w} \\cdot \\vec{x}_i \\geq 1$ tendrán $cost = 0$. Dicho de otro modo, todas las muestras bien clasificadas tendrán $cost = 0$. Por el contrario, a medida que nos alejamos del hiperplano positivo ($b + \\vec{w} \\cdot \\vec{x}_i = 1$) y nos acercamos al límite de decisión ($b + \\vec{w} \\cdot \\vec{x}_i = 0$) el valor de $cost$ aumenta.\n",
    " \n",
    "Entendida esta idea es evidente que necesitaremos contra función $cost$ para la clase 1, quedando definidas por tanto $cost_0$ y $cost_1$ del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:22.369028Z",
     "start_time": "2021-05-17T10:11:22.178752Z"
    },
    "id": "vJuCgqs9Yak1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12, 5))\n",
    "\n",
    "cost_0 = np.vectorize(lambda v: max(1-v,0.0))\n",
    "cost_1 = np.vectorize(lambda v: max(1+v,0.0))\n",
    "\n",
    "x = np.linspace(-4, 4)\n",
    "\n",
    "axs[0].plot(x, cost_0(x))\n",
    "axs[0].set_xlabel(\"b+w·x\")\n",
    "axs[0].set_ylabel(\"cost_0\")\n",
    "\n",
    "axs[1].plot(x, cost_1(x))\n",
    "axs[1].set_xlabel(\"b+w·x\")\n",
    "axs[1].set_ylabel(\"cost_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4UEFrhZnHr"
   },
   "source": [
    "Matemáticamente podemos definir las funciones anteriores con las siguientes expresiones:\n",
    " \n",
    "$$\n",
    "\\hat{y}_i = b + \\vec{w} \\cdot \\vec{x}_i\n",
    "$$\n",
    " \n",
    "$$\n",
    "cost_0(\\hat{y}_i) = max(0, 1 - \\hat{y}_i)\n",
    "$$\n",
    " \n",
    "$$\n",
    "cost_1(\\hat{y}_i) = max(0, 1 + \\hat{y}_i)\n",
    "$$\n",
    " \n",
    "Y unificarlas del siguiente modo:\n",
    " \n",
    "$$\n",
    "cost(\\hat{y}_i) = y_i \\cdot cost_1(\\hat{y}_i) + (1 - y_i) \\cdot cost_0(\\hat{y}_i)\n",
    "$$\n",
    " \n",
    "$$\n",
    "cost(\\hat{y}_i) = y_i \\cdot max(0, 1 + \\hat{y}_i) + (1 - y_i) \\cdot max(0, 1 - \\hat{y}_i)\n",
    "$$\n",
    " \n",
    "Si tenemos en cuenta los datos de entrada obtenemos:\n",
    " \n",
    "$$\n",
    "cost(\\vec{x}_i) = y_i \\cdot max(0, 1 + b + \\vec{w} \\cdot \\vec{x}_i) + (1 - y_i) \\cdot max(0, 1 - b - \\vec{w} \\cdot \\vec{x}_i)\n",
    "$$\n",
    " \n",
    "Y finalmente podemos definir la **función de coste** de las máquinas de vector soporte añadiendo un término de regularización como:\n",
    " \n",
    "$$\n",
    "loss = C \\left[ \\sum_{i=1}^{n} y_i \\cdot max(0, 1 + b + \\vec{w} \\cdot \\vec{x}_i) + (1 - y_i) \\cdot max(0, 1 - b - \\vec{w} \\cdot \\vec{x}_i) \\right] + \\frac{1}{2} \\sum_{j=1}^{m} w_j^2\n",
    "$$\n",
    " \n",
    "Donde $n$ representa el número de muestras y $m$ el número de características o *features*. Observamos, además, que se ha añadido un híper-parámetros $C$ al modelo. $C$, al que podemos llamar no-regularización, permite controlar el proceso de aprendizaje: valores grandes de $C$ harán el modelo muy sensible a los *outlayers* mientras que valores bajos de $C$ harán el método muy generalista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-mklWGWOiUk"
   },
   "source": [
    "Las Máquinas de Vector Soporte (lineales) se encuentra definidas en `sklearn` en la clase [`sklearn.svm.LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). Como se observa en su documenta, la clase dispone de un parámetro `C` que se corresponde con el híper-parámetro $C$ que controla la no-regularización.\n",
    " \n",
    "Una vez ajustado el modelo podemos recuperar sus parámetros accediendo a sus atributos `coef_`(que contiene $\\vec{w}$) e `intercep_`(que contiene $b$).\n",
    " \n",
    "Veamos un ejemplo.\n",
    " \n",
    "Supongamos el siguiente conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:24.589171Z",
     "start_time": "2021-05-17T10:11:23.940836Z"
    },
    "id": "P4cVq2VgMg-x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=3)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vTGCrX-he--"
   },
   "source": [
    "Ajustemos un modelo mediante `LinearSVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:26.281178Z",
     "start_time": "2021-05-17T10:11:26.240518Z"
    },
    "id": "Vkjg1a2kMiu2"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(\"coef_ = \" + str(linear_svm.coef_))\n",
    "print(\"intercept_ = \" + str(linear_svm.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG3hYzOghqcQ"
   },
   "source": [
    "Y lo pintamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:27.893543Z",
     "start_time": "2021-05-17T10:11:27.747049Z"
    },
    "id": "AqYP0P9yMkOI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "line = np.linspace(min[0], max[0])\n",
    "plt.plot(line, -(line * linear_svm.coef_[0][0] + linear_svm.intercept_) / linear_svm.coef_[0][1], color='green', label='Límite de decisión')\n",
    "plt.plot(line, (1-(line * linear_svm.coef_[0][0] + linear_svm.intercept_)) / linear_svm.coef_[0][1], color='red', linestyle='--', label='Hiperplano positivo')\n",
    "plt.plot(line, (-1-(line * linear_svm.coef_[0][0] + linear_svm.intercept_)) / linear_svm.coef_[0][1], color='blue', linestyle='--', label='Hiperplano negativo')\n",
    "\n",
    "plt.legend(framealpha=0.9).set_zorder(20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdrW__CThZW"
   },
   "source": [
    "La clase `LinearSVC` que hemos utilizado es, aproximadamente, equivalente a la clase [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) definiendo el parámetro `kernel='linear'`.Estudiaremos esto más adelante, sin embargo, ahora vamos a hacer uso de la clase `SVC` en lugar de `LinearSVC` para poder representar los vectores soporte.\n",
    "\n",
    "La clase `SVC` tiene un atributo `support_vectors_` que contienen los vectores soporte utilizados para definir el límite de decisión. Veamos dichos puntos con un ejemplo gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:30.164921Z",
     "start_time": "2021-05-17T10:11:30.001885Z"
    },
    "id": "UgGUEEl-Rg9f"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "cluster_std = 1.7  #@param {type: \"slider\", min: 0.1, max: 2, step: 0.1}\n",
    "\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=cluster_std, random_state=3)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr, zorder=15)\n",
    "\n",
    "svc = svm.SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "line = np.linspace(min[0], max[0])\n",
    "plt.plot(line, -(line * svc.coef_[0][0] + svc.intercept_) / svc.coef_[0][1], color='green', label='Límite de decisión')\n",
    "plt.plot(line, (1-(line * svc.coef_[0][0] + svc.intercept_)) / svc.coef_[0][1], color='red', linestyle='--', label='Hiperplano positivo')\n",
    "plt.plot(line, (-1-(line * svc.coef_[0][0] + svc.intercept_)) / svc.coef_[0][1], color='blue', linestyle='--', label='Hiperplano negativo')\n",
    "\n",
    "plt.scatter(svc.support_vectors_[:,0], svc.support_vectors_[:,1], marker='o', color='yellow', s=140)\n",
    "\n",
    "plt.legend(framealpha=0.9).set_zorder(20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible obtener una salida probabilística del modelo `SVC` estableciendo el párametro `probability=True` en su constructor. Esta variación, en esencia, mide la distancia de una muestra al límite de decisión y le asocia una probabilidad: las muestras alejadas tendrán una probabilidad cercana a 1 de pertenecer a su clase, mientras que las más cercanas tendrán una probabilidad en torno a 0.5. Mediante el método `predict_proba()` podemos recuperar y representar dichas probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:11:44.203035Z",
     "start_time": "2021-05-17T10:11:44.023689Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_std = 1.7  #@param {type: \"slider\", min: 0.1, max: 2, step: 0.1}\n",
    "\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=cluster_std, random_state=3)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "svc = svm.SVC(kernel='linear', probability=True).fit(X, y)\n",
    "\n",
    "line = np.linspace(min[0], max[0])\n",
    "plt.plot(line, -(line * svc.coef_[0][0] + svc.intercept_) / svc.coef_[0][1], color='black', label='Límite de decisión')\n",
    "plt.plot(line, (1-(line * svc.coef_[0][0] + svc.intercept_)) / svc.coef_[0][1], color='pink', linestyle='--', label='Hiperplano positivo')\n",
    "plt.plot(line, (-1-(line * svc.coef_[0][0] + svc.intercept_)) / svc.coef_[0][1], color='cyan', linestyle='--', label='Hiperplano negativo')\n",
    "\n",
    "x1, x2 = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "proba = svc.predict_proba(np.c_[x1.ravel(), x2.ravel()])[:,0]\n",
    "proba = proba.reshape(x1.shape)\n",
    "plt.pcolormesh(x1, x2, proba, cmap=plt.cm.bwr_r, vmin=0, vmax=1)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap='bwr')\n",
    "\n",
    "plt.legend(framealpha=0.9).set_zorder(20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqqy8pTTl7qR"
   },
   "source": [
    "La explicación anterior hacer se ha centrado en detallar cómo funcionan las máquinas de vectores soporte para clasificación binaria. No obstante, también se pueden emplear para clasificación multiclase. La única diferencia es que en lugar de realizar una única clasificación binaria haremos tantas clasificaciones binarias como clases haya utilizando la estrategia *one-vs-rest* (también llamada *one-vs-all*). Esta estrategia asume que todas las muestras que no pertenecen a la clase evaluada son de la clase contraria.\n",
    "\n",
    "Ilustremos el resultado con un ejemplo.\n",
    "\n",
    "Generamos un conjunto de datos con tres clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:12:05.768630Z",
     "start_time": "2021-05-17T10:12:05.592732Z"
    },
    "id": "FSh-TQ_rnBOV"
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=3, random_state=42)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCrKhqASnKTD"
   },
   "source": [
    "Ajustamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:12:08.676324Z",
     "start_time": "2021-05-17T10:12:08.664685Z"
    },
    "id": "MD7hkvvrnEVs"
   },
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(\"coef_ = \" + str(linear_svm.coef_))\n",
    "print(\"intercept_ = \" + str(linear_svm.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "958n7Lf-nOiY"
   },
   "source": [
    "Analizamos el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:12:35.827041Z",
     "start_time": "2021-05-17T10:12:35.710477Z"
    },
    "id": "bY8FTwUcnQ1v"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='rainbow')\n",
    "\n",
    "line = np.linspace(min[0], max[0])\n",
    "\n",
    "plt.plot(line, -(line * linear_svm.coef_[0][0] + linear_svm.intercept_[0]) / linear_svm.coef_[0][1], color=plt.cm.rainbow(0.0))\n",
    "plt.plot(line, -(line * linear_svm.coef_[1][0] + linear_svm.intercept_[1]) / linear_svm.coef_[1][1], color=plt.cm.rainbow(0.5))\n",
    "plt.plot(line, -(line * linear_svm.coef_[2][0] + linear_svm.intercept_[2]) / linear_svm.coef_[2][1], color=plt.cm.rainbow(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi4uhsnPqeCA"
   },
   "source": [
    "## Non-linear SVM\n",
    " \n",
    "En el apartado anterior hemos visto cómo podemos construir una Máquina de Vectores Soporte que separe clases linealmente. Sin embargo, existen infinidad de problemas en los que la separación lineal no es posible. Las Máquinas de Vectores Soporte pueden funcionar con separaciones no lineales haciendo algunas modificaciones.\n",
    " \n",
    "Principalmente debemos modificar la función de coste por una nueva:\n",
    " \n",
    "$$\n",
    "loss = C \\left[ \\sum_{i=1}^{n} y_i \\cdot max(0, 1 + \\theta^T \\cdot f(\\vec{x}_i)) + (1 - y_i) \\cdot max(0, 1 - \\theta^T \\cdot f(\\vec{x}_i)) \\right] + \\frac{1}{2} \\sum_{j=1}^{m} \\theta_j^2\n",
    "$$\n",
    " \n",
    "Como vemos hemos modificado los vectores $\\vec{w}$ por vectores $\\theta$ y a los valores de los vectores $\\vec{x}_i$ se les ha aplicado una función $f$.\n",
    " \n",
    "Está función $f$ hace uso de una **función de Kernel** y permite determinar la similaridad entre una muestra y todas las demás. Si dos muestras son idénticas entonces su similaridad deberá ser 1, mientras que si son opuestas deberá ser 0.\n",
    "\n",
    "La idea subyacente de las funciones de Kernel consiste en aumentar la dimensión del conjunto de datos para que, al realizar una separación lineal en un espacio de dimensión superior, se muestre una separación no lineal en el espacio original. Ilustremos esto con un sencillo ejemplo. \n",
    "\n",
    "Asumamos la siguiente función de Kernel para comparar dos muestras $x_i$ y $x_j$ cuando trabajamos en $\\mathbb{R}^2$:\n",
    "\n",
    "$$\n",
    "K(x_i,x_j) = (x_i^t \\cdot x_j)^2 = (x_{i,1} \\cdot x_{j,1} + x_{i,2} \\cdot x_{j,2})^2 = x_{i,1}^2 \\cdot x_{j,1}^2 + x_{i,2}^2 \\cdot x_{j,2}^2 + 2 \\cdot x_{i,1} \\cdot x_{j,1} \\cdot x_{i,2} \\cdot x_{j,2}\n",
    "$$\n",
    "\n",
    "Esta función de Kernel es equivalente a:\n",
    "\n",
    "$$\n",
    "K(x_i,x_j) = (x_i^t \\cdot x_j)^2 = (x_{i,1}^2, x_{i,2}^2, \\sqrt{2} \\cdot x_{i,1} \\cdot x_{i,2}) \\cdot (x_{j,1}^2, x_{j,2}^2, \\sqrt{2} \\cdot x_{j,1} \\cdot x_{j,2})\n",
    "$$\n",
    "\n",
    "O dicho de otro modo, usar la función de Kernel $K(x_i,x_j) = (x_i^t \\cdot x_j)^2$ equivale a transformar nuestro espacio en $\\mathbb{R}^2$ a un espacio en $\\mathbb{R}^3$ en el que trabajar mediante la siguiente transformación:\n",
    "\n",
    "$$\n",
    "(x_{i,1}, x_{i,2}) \\rightarrow (x_{i,1}^2, x_{i,2}^2, \\sqrt{2} \\cdot x_{i,1} \\cdot x_{i,2})\n",
    "$$\n",
    "\n",
    "¿Y por qué es mejor trabajar en $\\mathbb{R}^3$ que en $\\mathbb{R}^2$? Veámoslo gráficamente.\n",
    "\n",
    "Supongamos que tenemos el siguiente conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:12:42.961696Z",
     "start_time": "2021-05-17T10:12:42.851580Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=50, noise=0.1, factor=0.4, random_state=42)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su transformación a $\\mathbb{R}^3$ generaría el siguiente conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:12:45.063507Z",
     "start_time": "2021-05-17T10:12:44.907276Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x = X[:,0]**2,\n",
    "    y = X[:,1]**2,\n",
    "    z = sqrt(2)*X[:,0]*X[:,1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 6,\n",
    "        color = -y,\n",
    "        colorscale = 'RdBu',\n",
    "        opacity = 0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                  scene = dict(\n",
    "                      xaxis = dict(title='x1^2'),\n",
    "                      yaxis = dict(title='x2^2'),\n",
    "                      zaxis = dict(title='sqrt(2)*x1*x2')\n",
    "                  ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cual es separable linealmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph0T2hotu8nU"
   },
   "source": [
    "Existen infinidad de funciones de Kernel, denotadas por $K(x_i,x_j)$:\n",
    "\n",
    "- Lineal: $K(x_i,x_j) = x_i^t \\cdot x_j$\n",
    "- Polinomial: $K(x_i,x_j) = (x_i^t \\cdot x_j+ c)^d$ donde $c>0$\n",
    "- Sigmoidal: $K(x_i,x_j) = tanh(a \\cdot (x_i^t \\cdot x_j) + b)$ donde $a,b>0$\n",
    "\n",
    "Sin embargo, la función de Kernel más popular es la ***Gaussian Kernel***, que se encuentra implementada en `sklearn` bajo el acrónimo `'rbf'` (*Radial Basis Fuction Kernel*). Esta función determina la distancia euclídea entre dos vectores suavizándose mediante una distribución normal de desviación típica $\\sigma$:\n",
    " \n",
    "$$\n",
    "K(x_i,x_j) = exp\\left(- \\frac{||x_i - x_j||^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    " \n",
    "Independientemente de la función de Kernel a utilizar, $f(\\vec{x}_i)$ devolverá un vector con el resultado de la función de Kernel (i.e. la distancia) de la muestra $i$ con respecto a todas las demás.\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\vec{x}_i) = \\begin{pmatrix} K(x_i,x_1) \\\\ K(x_i,x_2) \\\\ \\vdots \\\\ K(x_i,x_i) \\\\ \\vdots \\\\ K(x_i,x_n) \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Intuitivamente, las muestras cercanas a $i$ tendrán $K(x_i,x_j) \\approx 1$ y las muestras lejanas tendrá $K(x_i,x_j) \\approx 0$.\n",
    " \n",
    "Por consiguiente, $\\theta$ pasará a ser un vector de tamaño $n$ (número de muestras) con los parámetros que el modelo debe aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este clasificador se encuentra implementado en la clase [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) de la librería `sklearn`.\n",
    " \n",
    "Se destacan dos parámetros tres parámetros de su implementación:\n",
    " \n",
    "- `C` que indica la no-regularización como en `LinearSVC`.\n",
    "- `Kernel` que permite modificar la función de Kernel. Por defecto `'rbf'`.\n",
    "- `gamma` que modifica los parámetros de la función de Kernel. En el caso de `'rbf'`, valores altos de `gamma` indican una baja desviación típica y, por tanto, prestar más atención a las muestras más cercanas, mientras que valores bajos de `gamma` indican lo apuesto.\n",
    " \n",
    "Analicemos la importancia de estos parámetros con un ejemplo gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:13:26.921535Z",
     "start_time": "2021-05-17T10:13:25.059752Z"
    },
    "id": "Hf4aJZxD3E6t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "import numpy as np  \n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.08, random_state=10)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "fig, axs = plt.subplots(4,4, figsize=(20,20))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for i, C in enumerate([0.01, 0.04, 0.08, 0.12]):\n",
    "  for j, gamma in enumerate([0.1,2,3,10]):\n",
    "    \n",
    "    clf = svm.SVC(kernel='rbf', gamma=gamma, C=C, probability=True)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    axs[i,j].set_title('SVM, C: ' + format(C) + ', gamma: '+ str(gamma))\n",
    "\n",
    "    axs[i,j].set_xlabel('X1')\n",
    "    axs[i,j].set_ylabel('X2')\n",
    "\n",
    "    axs[i,j].set_xlim(min[0], max[0])\n",
    "    axs[i,j].set_ylim(min[1], max[1])\n",
    "\n",
    "    x1, x2 = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "    proba = clf.predict_proba(np.c_[x1.ravel(), x2.ravel()])[:,0]\n",
    "    proba = proba.reshape(x1.shape)\n",
    "    axs[i,j].pcolormesh(x1, x2, proba, cmap='bwr_r', vmin=0, vmax=1)\n",
    "\n",
    "    axs[i,j].scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:13:52.643596Z",
     "start_time": "2021-05-17T10:13:48.241987Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from make_spirals import make_spirals\n",
    "import numpy as np  \n",
    "\n",
    "X, y = make_spirals(n_samples=1000, random_state=10)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "fig, axs = plt.subplots(4,4, figsize=(20,20))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for i, C in enumerate([0.01, 0.04, 0.08, 0.12]):\n",
    "  for j, gamma in enumerate([0.01,0.05,0.1,5]):\n",
    "    \n",
    "    clf = svm.SVC(kernel='rbf', gamma=gamma, C=C, probability=True)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    axs[i,j].set_title('SVM, C: ' + format(C) + ', gamma: '+ str(gamma))\n",
    "\n",
    "    axs[i,j].set_xlabel('X1')\n",
    "    axs[i,j].set_ylabel('X2')\n",
    "\n",
    "    axs[i,j].set_xlim(min[0], max[0])\n",
    "    axs[i,j].set_ylim(min[1], max[1])\n",
    "\n",
    "    x1, x2 = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "    proba = clf.predict_proba(np.c_[x1.ravel(), x2.ravel()])[:,0]\n",
    "    proba = proba.reshape(x1.shape)\n",
    "    axs[i,j].pcolormesh(x1, x2, proba, cmap='bwr_r', vmin=0, vmax=1)\n",
    "\n",
    "    axs[i,j].scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:14:29.223540Z",
     "start_time": "2021-05-17T10:14:27.351045Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import svm\n",
    "import numpy as np  \n",
    "\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "fig, axs = plt.subplots(4,4, figsize=(20,20))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for i, C in enumerate([0.001, 0.02, 0.04, 0.10]):\n",
    "  for j, gamma in enumerate([0.25,0.50,0.75,1.00]):\n",
    "    \n",
    "    clf = svm.SVC(kernel='rbf', gamma=gamma, C=C, probability=True)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    axs[i,j].set_title('SVM, C: ' + format(C) + ', gamma: '+ str(gamma))\n",
    "\n",
    "    axs[i,j].set_xlabel('X1')\n",
    "    axs[i,j].set_ylabel('X2')\n",
    "\n",
    "    axs[i,j].set_xlim(min[0], max[0])\n",
    "    axs[i,j].set_ylim(min[1], max[1])\n",
    "\n",
    "    x1, x2 = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "    proba = clf.predict_proba(np.c_[x1.ravel(), x2.ravel()])[:,0]\n",
    "    proba = proba.reshape(x1.shape)\n",
    "    axs[i,j].pcolormesh(x1, x2, proba, cmap='bwr_r', vmin=0, vmax=1)\n",
    "\n",
    "    axs[i,j].scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBr87UkjRkwE"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Support Vector Machines.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
