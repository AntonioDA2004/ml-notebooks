{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:42:59.788021Z",
     "start_time": "2021-05-17T10:42:50.635830Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pydotplus\n",
    "!pip install make-spirals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQp26VacycHc"
   },
   "source": [
    "# Árboles de decisión\n",
    "\n",
    "Un **árbol de decisión** es una estructura de árbol similar a un diagrama de flujo donde un nodo interno representa una característica (o atributo), la rama representa una regla de decisión y cada nodo de hoja representa el resultado. El nodo superior de un **árbol de decisión** se conoce como nodo raíz. Aprende a particionar en base al valor del atributo. Particiona el árbol de manera recursiva llamada partición recursiva.\n",
    "\n",
    "El **árbol de decisión** es un algoritmo de ML de tipo caja blanca. Comparte la lógica de toma de decisiones interna, a diferencia de los algoritmos de caja negra como las redes neuronales. Esto quiere decir que los árboles de decisión son **modelos explicativos**.\n",
    "\n",
    "Su complejidad temporal de entrenamiento es menor en comparación con otros algoritmos clásicos de clasificación. La complejidad temporal de los árboles de decisión depende del número de registros y atributos de los datos dados. El árbol de decisión es un método no paramétrico (también conocido como libre de distribución), por lo que no presuponen una determinada distribución en los datos de entrada para poder funcionar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4yiDk6oz81B"
   },
   "source": [
    "## Pseudo-código del algoritmo de árbol de decisión\n",
    "La idea básica detrás de cualquier algoritmo de árbol de decisión es la siguiente:\n",
    "\n",
    "1. Seleccionar el mejor atributo utilizando Medidas de selección de atributos (ASM, Attribute Selection Metric en inglés) para dividir los registros.\n",
    "2. Hacer de ese atributo un nodo de decisión y dividir el conjunto de datos en subconjuntos más pequeños.\n",
    "3. Comenzar a construir árboles repitiendo este proceso recursivamente para cada nodo hijo hasta que una de las siguientes condiciones se cumpla:\n",
    "\n",
    "    * Todas las observaciones (filas) se corresponden con el mismo valor de atributo.\n",
    "    * No quedan más atributos\n",
    "    * No quedan más filas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UypjTVQ41PSZ"
   },
   "source": [
    "## Medidas de selección de atributos\n",
    "Una medida de selección de atributos es una heurística para seleccionar el criterio de división que divide los datos de la mejor manera posible. También se conoce como reglas de **partición** porque nos ayuda a determinar puntos de ruptura en un nodo dado. Estas medidas proporcionan una puntuación a cada característica (o atributo). El atributo de mejor puntuación se seleccionará como atributo de división.\n",
    "\n",
    "Las medidas de selección más populares son la ganancia de información (_Information Gain_), y el coeficiente de Gini (_Gini Impurity_).\n",
    "\n",
    "\n",
    "### Information Gain\n",
    "[Shannon](https://es.wikipedia.org/wiki/Claude_Elwood_Shannon) inventó el concepto de [entropía](https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n)), que mide la impureza del conjunto de entradas. En física y matemáticas, la entropía se refiere a la aleatoriedad o la impureza en el sistema. En la teoría de la información, se refiere a la impureza en un grupo de muestras. \n",
    "\n",
    "La **ganancia de información** es la disminución de la entropía. La ganancia de información calcula la diferencia entre la entropía *antes de la división* y la entropía media *después de la división* del conjunto de datos basada en los valores de atributo dados.\n",
    "\n",
    "La formulación matemática de esta métrica es la siguiente:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "IG(T,a) &=& H(T) - H(T|a) \\\\\n",
    "H(T) &=& -\\sum_{i\\in C}^{}p_i \\log_2(p_i) \\\\\n",
    "H(T|a) &=& \\sum_{j\\in vals(a)}^{}\\frac{\\left | T_j \\right |}{\\left | T \\right |} H(T_j)\n",
    "\\end{eqnarray}$\n",
    "\n",
    "donde $C$ es el conjunto de clases o etiquetas de nuestro conjunto de datos $T$, $p_i$ es la probabilidad de que una observación (fila) del conjunto de datos pertenezca a la clase $i$, $a$ es una característica (columna) del conjunto de datos y $vals(a)$ son todos los valores que toma la característica $a$.\n",
    "\n",
    "### Coeficiente de Gini\n",
    "La impureza de Gini se mide como:\n",
    "\n",
    "$\\begin{eqnarray}Gini(T) = 1 - \\sum_{i\\in C}^{}p_i^2\\end{eqnarray}$\n",
    "\n",
    "El **coeficiente de Gini** considera una división binaria para cada atributo. Se puede calcular una suma ponderada de la impureza de cada partición. Si una división binaria en las particiones del atributo $a$ divide los datos $T$ en $T_1$ y $T_2$, el coeficiente de Gini de $T$ es:\n",
    "\n",
    "$\\begin{eqnarray}Gini(T|a) = \\frac{\\left | T_1 \\right |}{\\left | T \\right |}Gini(T_1)+\\frac{\\left | T_2 \\right |}{\\left | T \\right |}Gini(T_2)\\end{eqnarray}$\n",
    "\n",
    "En el caso de un atributo discreto, el subconjunto que da el coeficiente de Gini más pequeño para el atributo elegido se selecciona como atributo de división. En el caso de los atributos continuos, la estrategia es seleccionar cada par de valores adyacentes como un posible punto de división y el punto con el coeficiente de Gini más pequeño es elegido como punto de división.\n",
    "\n",
    "El atributo con el coeficiente mínimo de Gini se elige como atributo de división.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUfM6aG-BlAl"
   },
   "source": [
    "## Caso de estudio: clasificación de vinos con árboles de decisión\n",
    "Vamos a usar el conjunto de datos incluido en `sklearn` sobre [clasificación de vinos](https://scikit-learn.org/stable/datasets/index.html#wine-dataset) para probar los árboles de decisión.\n",
    "\n",
    "En primer lugar se importan las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:00.514992Z",
     "start_time": "2021-05-17T10:42:59.793610Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wRXVRi2gyYFY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "\n",
    "from make_spirals import make_spirals\n",
    "\n",
    "from six import StringIO  \n",
    "\n",
    "from IPython.display import Image  \n",
    "\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5b-XHG5B9CW"
   },
   "source": [
    "Cargamos el _dataset_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:00.538217Z",
     "start_time": "2021-05-17T10:43:00.517142Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bxr7vZvvCnL7"
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.DataFrame(wine.target, columns=['Class'])\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:00.544297Z",
     "start_time": "2021-05-17T10:43:00.540062Z"
    }
   },
   "outputs": [],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxdO-uGvD-VV"
   },
   "source": [
    "Hagamos una división rápida del conjunto de datos en entrenamiento y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:00.549741Z",
     "start_time": "2021-05-17T10:43:00.545870Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EQ6neP8fD8uX"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_P62wiJESji"
   },
   "source": [
    "Entrenamos el modelo usando [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) con el conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:23.972333Z",
     "start_time": "2021-05-17T10:43:23.957220Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0agVjn7pENDP"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=3, random_state=43)\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjsQWAzJEfEK"
   },
   "source": [
    "Y evaluamos la calidad de las predicciones del árbol de decisión con el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:25.165792Z",
     "start_time": "2021-05-17T10:43:25.147596Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zuE0Qlf3EOdd"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, clf.predict(X_test)))\n",
    "print(\"F1:\", metrics.f1_score(y_test, clf.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:26.135547Z",
     "start_time": "2021-05-17T10:43:25.910315Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, X_test, y_test, display_labels=wine.target_names, cmap=plt.cm.Blues)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4dde0qDEtYE"
   },
   "source": [
    "Gracias a que los árboles de decisión son un modelo de caja blanca, podemos obtener información muy valiosa sobre cómo realiza las particiones y lo que ello implica.\n",
    "\n",
    "Vamos a representar gráficamente el árbol de decisión. Para ello convertimos el árbol de decisión en un grafo y lo mostramos por pantalla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:43:48.946047Z",
     "start_time": "2021-05-17T10:43:48.746078Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "r2lJfxGWEoGW"
   },
   "outputs": [],
   "source": [
    "#plt.rcParams[\"figure.figsize\"] = [20, 20]\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, \n",
    "                feature_names=wine.feature_names, class_names=wine.target_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de híper-parámetros\n",
    "\n",
    "Los árboles de decisión tienen múltiples híper-parámetros que podemos (y debemos) ajustar para que ofrezcan un rendimiento óptimo. El más relevante y que va a condicionar en gran medida el desempeño de nuestro clasificador es la **profundidad del árbol** (`max_depth`). Este híper-parámetro define el número de nodos que debe tener el árbol para determinar la clasificación final de la muestra. Árboles muy profundos reportarán mucho *overfitting*. Árboles poco profundos reportarán *underfitting*.\n",
    "\n",
    "Ilustremos la importancia del `max_depth` con un ejemplo gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:44:40.397933Z",
     "start_time": "2021-05-17T10:44:38.753741Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_clasification(X, y, axs):\n",
    "  min = np.amin(X, axis=0)\n",
    "  max = np.amax(X, axis=0)\n",
    "\n",
    "  diff = max - min\n",
    "\n",
    "  min = min - 0.1 * diff\n",
    "  max = max + 0.1 * diff\n",
    "\n",
    "  axs[0].set_title('Raw data')\n",
    "  axs[0].set_xlabel('X1')\n",
    "  axs[0].set_ylabel('X2')\n",
    "\n",
    "  axs[0].set_xlim(min[0], max[0])\n",
    "  axs[0].set_ylim(min[1], max[1])\n",
    "\n",
    "  axs[0].scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "  for i, max_depth in enumerate([1,2,5]):    \n",
    "    axs[i+1].set_title('DecisionTreeClassifier (max_depth=' + str(max_depth) + ')')\n",
    "    axs[i+1].set_xlabel('X1')\n",
    "    axs[i+1].set_ylabel('X2')\n",
    "\n",
    "    axs[i+1].set_xlim(min[0], max[0])\n",
    "    axs[i+1].set_ylim(min[1], max[1])\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=43).fit(X, y)\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axs[i+1].contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.7, antialiased=True)\n",
    "    axs[i+1].scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap=plt.cm.bwr)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20,20))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=2, random_state=42)\n",
    "plot_clasification(X, y, axs[0])\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "plot_clasification(X, y, axs[1])\n",
    "\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "plot_clasification(X, y, axs[2])\n",
    "\n",
    "X, y = make_spirals(n_samples=1000, random_state=10)\n",
    "plot_clasification(X, y, axs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0RgEOsuBCdw"
   },
   "source": [
    "## Árboles de decisión como clasificadores multiclase\n",
    "\n",
    "Los árboles de decisión pueden trabajar con más de dos clases:\n",
    "\n",
    "Para ayudar a entender cómo funciona un **árbol de decisión** vamos a generar un conjunto de datos sintético con `make_blobs`. Generamos un *dataset* sintético con 3 clases y 1000 observaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:45:45.551261Z",
     "start_time": "2021-05-17T10:45:44.949136Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_clasification(X, y, axs):\n",
    "  min = np.amin(X, axis=0)\n",
    "  max = np.amax(X, axis=0)\n",
    "\n",
    "  diff = max - min\n",
    "\n",
    "  min = min - 0.1 * diff\n",
    "  max = max + 0.1 * diff\n",
    "\n",
    "  axs[0].set_title('Raw data')\n",
    "  axs[0].set_xlabel('X1')\n",
    "  axs[0].set_ylabel('X2')\n",
    "\n",
    "  axs[0].set_xlim(min[0], max[0])\n",
    "  axs[0].set_ylim(min[1], max[1])\n",
    "\n",
    "  axs[0].scatter(X[:,0], X[:,1], c=y, cmap='rainbow')\n",
    "  \n",
    "  axs[1].set_title('DecisionTreeClassifier')\n",
    "  axs[1].set_xlabel('X1')\n",
    "  axs[1].set_ylabel('X2')\n",
    "\n",
    "  axs[1].set_xlim(min[0], max[0])\n",
    "  axs[1].set_ylim(min[1], max[1])\n",
    "\n",
    "  clf = DecisionTreeClassifier(random_state=43).fit(X, y)\n",
    "\n",
    "  xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z = Z.reshape(xx.shape)\n",
    "\n",
    "  axs[1].contourf(xx, yy, Z, cmap='rainbow', alpha=0.7, antialiased=True)\n",
    "  axs[1].scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap='rainbow')\n",
    "\n",
    "n_centers = [3, 4, 5]    \n",
    "    \n",
    "fig, axs = plt.subplots(nrows=len(n_centers), ncols=2, figsize=(10, 5*len(n_centers)))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for i, centers in enumerate(n_centers):\n",
    "    X, y = make_blobs(n_samples=300, n_features=2, centers=centers, cluster_std=2, random_state=42)\n",
    "    plot_clasification(X, y, axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Creado por **Raúl Lara Cabrera** (raul.lara@upm.es) y **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Árboles de decisión.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
