{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQp26VacycHc"
   },
   "source": [
    "# Árboles de decisión\n",
    "\n",
    "Un **árbol de decisión** es una estructura de árbol similar a un diagrama de flujo donde un nodo interno representa una característica (o atributo), la rama representa una regla de decisión y cada nodo de hoja representa el resultado. El nodo superior de un **árbol de decisión** se conoce como nodo raíz. Aprende a particionar en base al valor del atributo. Particiona el árbol de manera recursiva llamada partición recursiva.\n",
    "\n",
    "El **árbol de decisión** es un algoritmo de ML de tipo caja blanca. Comparte la lógica de toma de decisiones interna, a diferencia de los algoritmos de caja negra como las redes neuronales. Esto quiere decir que los árboles de decisión son **modelos explicativos**.\n",
    "\n",
    "Su complejidad temporal de entrenamiento es menor en comparación con otros algoritmos clásicos de clasificación. La complejidad temporal de los árboles de decisión depende del número de registros y atributos de los datos dados. El árbol de decisión es un método no paramétrico (también conocido como libre de distribución), por lo que no presuponen una determinada distribución en los datos de entrada para poder funcionar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4yiDk6oz81B"
   },
   "source": [
    "## Pseudo-código del algoritmo de árbol de decisión\n",
    "La idea básica detrás de cualquier algoritmo de árbol de decisión es la siguiente:\n",
    "\n",
    "1. Seleccionar el mejor atributo utilizando Medidas de selección de atributos (ASM, Attribute Selection Metric en inglés) para dividir los registros.\n",
    "2. Hacer de ese atributo un nodo de decisión y dividir el conjunto de datos en subconjuntos más pequeños.\n",
    "3. Comenzar a construir árboles repitiendo este proceso recursivamente para cada nodo hijo hasta que una de las siguientes condiciones se cumpla:\n",
    "\n",
    "    * Todas las observaciones (filas) se corresponden con el mismo valor de atributo.\n",
    "    * No quedan más atributos\n",
    "    * No quedan más filas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UypjTVQ41PSZ"
   },
   "source": [
    "## Medidas de selección de atributos\n",
    "Una medida de selección de atributos es una heurística para seleccionar el criterio de división que divide los datos de la mejor manera posible. También se conoce como reglas de **partición** porque nos ayuda a determinar puntos de ruptura en un nodo dado. Estas medidas proporcionan una puntuación a cada característica (o atributo). El atributo de mejor puntuación se seleccionará como atributo de división.\n",
    "\n",
    "Las medidas de selección más populares son la ganancia de información (_Information Gain_), y el coeficiente de Gini (_Gini Impurity_).\n",
    "\n",
    "\n",
    "### Information Gain\n",
    "[Shannon](https://es.wikipedia.org/wiki/Claude_Elwood_Shannon) inventó el concepto de [entropía](https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n)), que mide la impureza del conjunto de entradas. En física y matemáticas, la entropía se refiere a la aleatoriedad o la impureza en el sistema. En la teoría de la información, se refiere a la impureza en un grupo de muestras. \n",
    "\n",
    "La **ganancia de información** es la disminución de la entropía. La ganancia de información calcula la diferencia entre la entropía *antes de la división* y la entropía media *después de la división* del conjunto de datos basada en los valores de atributo dados.\n",
    "\n",
    "La formulación matemática de esta métrica es la siguiente:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "IG(T,a) &=& H(T) - H(T|a) \\\\\n",
    "H(T) &=& -\\sum_{i\\in C}^{}p_i \\log_2(p_i) \\\\\n",
    "H(T|a) &=& \\sum_{j\\in vals(a)}^{}\\frac{\\left | T_j \\right |}{\\left | T \\right |} H(T_j)\n",
    "\\end{eqnarray}$\n",
    "\n",
    "donde $C$ es el conjunto de clases o etiquetas de nuestro conjunto de datos $T$, $p_i$ es la probabilidad de que una observación (fila) del conjunto de datos pertenezca a la clase $i$, $a$ es una característica (columna) del conjunto de datos y $vals(a)$ son todos los valores que toma la característica $a$.\n",
    "\n",
    "### Coeficiente de Gini\n",
    "La impureza de Gini se mide como:\n",
    "\n",
    "$\\begin{eqnarray}Gini(T) = 1 - \\sum_{i\\in C}^{}p_i^2\\end{eqnarray}$\n",
    "\n",
    "El **coeficiente de Gini** considera una división binaria para cada atributo. Se puede calcular una suma ponderada de la impureza de cada partición. Si una división binaria en las particiones del atributo $a$ divide los datos $T$ en $T_1$ y $T_2$, el coeficiente de Gini de $T$ es:\n",
    "\n",
    "$\\begin{eqnarray}Gini(T|a) = \\frac{\\left | T_1 \\right |}{\\left | T \\right |}Gini(T_1)+\\frac{\\left | T_2 \\right |}{\\left | T \\right |}Gini(T_2)\\end{eqnarray}$\n",
    "\n",
    "En el caso de un atributo discreto, el subconjunto que da el coeficiente de Gini más pequeño para el atributo elegido se selecciona como atributo de división. En el caso de los atributos continuos, la estrategia es seleccionar cada par de valores adyacentes como un posible punto de división y el punto con el coeficiente de Gini más pequeño es elegido como punto de división.\n",
    "\n",
    "El atributo con el coeficiente mínimo de Gini se elige como atributo de división.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUfM6aG-BlAl"
   },
   "source": [
    "## Caso de estudio: clasificación de vinos con árboles de decisión\n",
    "Vamos a usar el conjunto de datos incluido en `sklearn` sobre [clasificación de vinos](https://scikit-learn.org/stable/datasets/index.html#wine-dataset) para probar los árboles de decisión.\n",
    "\n",
    "En primer lugar se importan las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRXVRi2gyYFY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5b-XHG5B9CW"
   },
   "source": [
    "Cargamos el _dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxr7vZvvCnL7"
   },
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.DataFrame(data.target, columns=['Class'])\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxdO-uGvD-VV"
   },
   "source": [
    "Hagamos una división rápida del conjunto de datos en entrenamiento y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQ6neP8fD8uX"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_P62wiJESji"
   },
   "source": [
    "Entrenamos el modelo con el conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0agVjn7pENDP"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=3)\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjsQWAzJEfEK"
   },
   "source": [
    "Y evaluamos el _accuracy_ del árbol de decisión con el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuE0Qlf3EOdd"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, clf.predict(X_test)))\n",
    "print(\"F1:\",metrics.f1_score(y_test, clf.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4dde0qDEtYE"
   },
   "source": [
    "Gracias a que los árboles de decisión son un modelo de caja blanca, podemos obtener información muy valiosa sobre cómo realiza las particiones y lo que ello implica.\n",
    "\n",
    "Vamos a representar gráficamente el árbol de decisión. Para ello convertimos el árbol de decisión en un grafo y lo mostramos por pantalla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2lJfxGWEoGW"
   },
   "outputs": [],
   "source": [
    "#plt.rcParams[\"figure.figsize\"] = [20, 20]\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = data.feature_names,class_names=data.target_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0RgEOsuBCdw"
   },
   "source": [
    "## Caso de estudio: dibujando las fronteras de decisión de un árbol\n",
    "\n",
    "Para ayudar a entender cómo funciona un **árbol de decisión** vamos a generar un conjunto de datos sintético con `make_blobs`. Generamos un *dataset* sintético con 3 clases y 1000 observaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNbr0Rx6FCEm"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X,y = make_blobs(1000, 2, 3, cluster_std=3, random_state=17)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "            c='red', s=40, label='0')\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "            c='yellow', s=40, label='1')\n",
    "plt.scatter(X[y == 2, 0], X[y == 2, 1], \n",
    "            c='blue', s=40, label='2')\n",
    "\n",
    "plt.xlabel(\"primera característica\")\n",
    "plt.ylabel(\"segunda característica\")\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvFV4OAVIvFf"
   },
   "source": [
    "Entrenamos el modelo y pintamos sus fronteras de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vp6y_3vHv17"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "clf = DecisionTreeClassifier(max_features='auto').fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                        np.arange(y_min, y_max, 0.02))\n",
    "#plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "for i, color in zip(range(3), ('red', 'yellow', 'blue')):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=range(3),\n",
    "                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMnkswoujEFw"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DO-ZXyDNKope"
   },
   "source": [
    "Ahora lo único que nos queda por hacer es estudiar el efecto que tienen los hiper-parámetros del árbol de decisión."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Árboles de decisión.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
