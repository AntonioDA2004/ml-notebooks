{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regresión logística.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.7-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"0.26.0"}},"cells":[{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"W5kKZ0FHG0sX"},"source":["# Aprendizaje Supervisado: Clasificación"]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"JexeUSmrG0sY"},"source":["A diferencia de la regresión, en el cual predecíamos un valor continuo, ahora intentaremos predecir la etiqueta, grupo o categoría de distintas observaciones. Por ejemplo, la regresión lineal de un conjunto de datos sociales y económicos podría utilizarse para predecir los ingresos de una persona, pero la regresión logística podría utilizarse para predecir si esa persona estaba casada, tenía hijos o había sido detenida alguna vez.\n","\n","Es decir, para resolver un **Problema de clasificación** necesitamos encontrar una función\n","\n","$$\n","M: \\mathbb{V}^n \\rightarrow \\left \\{ c_1, c_2, \\ldots, c_k \\right \\}\n","$$\n","\n","que, a partir de una entrada $n$-dimensional nos devuelva a cuál de las $k$ categorías o clases pertenece dicha entrada. Hay que tener en cuenta que cada variable $v_i\\in \\mathbb{V}, i\\in\\left \\{1, 2, \\ldots, n\\right \\}$ puede ser categórica, ordinal, discreta o continua.\n","\n","Otra característica importante de los **problemas de clasificación** es que se encuadran dentro del **aprendizaje supervisado**, puesto que es necesario ajustar (entrenar) los modelos de clasificación a partir de un conjunto de datos de entrada etiquetados.\n","\n","Un algoritmo que implementa la clasificación, especialmente en una implementación concreta, se conoce como **clasificador**. El término **clasificador** a veces también se refiere a la función matemática, implementada por un algoritmo de clasificación, que asigna los datos de entrada a una categoría."]},{"cell_type":"markdown","metadata":{"id":"qsQ25mAARw00"},"source":["# Regresión logística\n","\n","La regresión logística, a pesar de su nombre, es una algoritmo de **clasificación** binario que basa su funcionamiento en los principios de la regresión lineal. Aunque en esencia este modelo solo puede aplicarse a **problemas de clasificación binaria**, es decir, donde solo hay dos categorías posibles, es fácilmente [extensible](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) para que pueda dar respuesta a problemas con un número de clases $k>2$.\n","\n","Para poder entender su funcionamiento vamos a suponer que disponemos del siguiente conjunto de datos:\n","\n","| x1 | x2 | y |\n","| -- | -- | - |\n","| 0  | 0  | 0 |\n","| 0  | 1  | 0 |\n","| 1  | 0  | 0 |\n","| 1  | 1  | 1 |\n","\n","Como observamos disponemos de dos características `x1` y `x2` que condicionan la clase `0` o `1` a la que pertenece una determinada muestra. A priori, no es posible tratar este problema como uno de regresión ya que la salida `y` es discreta y no continua. Para poder resolver una clasificación usando regresión lineal, la regresión logística transforma esta salida en una salida continua empleando la función sigmoide o `logit`.\n","\n","La función sigmoide es una función monótona creciente que aproxima asintóticamente a 1 para valores positivos y asintóticamente a 0 para valores negativos. Podemos verla en la siguiente imagen:"]},{"cell_type":"code","metadata":{"id":"djuzq6KjTzK4","execution":{"iopub.status.busy":"2020-10-27T20:25:59.612Z","iopub.execute_input":"2020-10-27T20:25:59.628Z","iopub.status.idle":"2020-10-27T20:26:00.945Z","shell.execute_reply":"2020-10-27T20:26:00.954Z"}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","sigmoid = lambda x: 1 / (1 + np.exp(-x))\n","x = np.linspace(-10,10,100)\n","y = sigmoid(x)\n","\n","plt.plot(x,y)\n","plt.xlabel('x')\n","plt.ylabel('logit(x)')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1QUWb0GkUfQ-"},"source":["Matemáticamente la función sigmoide se define como:\n","\n","$$\n","logit(x) = \\frac{1}{1+ e^{-x}}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"ITmi7mCdU3H4"},"source":["Por tanto, la regresión logística transforma un problema de clasificación en uno de regresión (de ahí su nombre) donde la variable objetivo $y$ es en realidad una función sigmoide. Dicho de otro modo, usando las características $x = x_1, \\dots, x_m$ se calculará un valor de regresión que será pasado como parámetro de una función sigmoide. Posteriormente se establecerá un umbral para dicha salida, habitualmente $0.5$, de tal modo que todas las \"regresiones\" inferiores a dicho valor se clasificarán en la `clase 0` mientras que el resto se clasificarán en la `clase 1`:\n","\n","$$\n","\\hat{y} =\n","\\begin{cases}\n","0 & \\text{si } logit(w x + b) < 0.5 \\\\\n","1 & \\text{si } logit(w x + b) \\geq 0.5 \\\\\n","\\end{cases}\n","$$\n","\n","Al estar la función sigmoide acotada al intervalo $(0,1)$ podemos expresar la salida de la regresión logística en términos probabísticos. Se define, por tanto, la probabilidad de pertenecer a cada una de las clases como:\n","\n","$$\n","p_{w,b}(x,y) =\n","\\begin{cases}\n","logit(w x+b)     & \\text{si } y = 1 \\\\\n","1 - logit(w x+b) & \\text{si } y = 0 \\\\\n","\\end{cases}\n","$$\n","\n","Evaluando todo el conjunto de datos se obtiene la siguiente función de verosimilitud:\n","\n","$$\n","L(w,b)= \\left( \\prod_{i=1}^{n} p_{w,b}(x_i,y_i) \\right)^\\frac{1}{n} = \\left( \\prod_{y_i=1} logit(wx_i+b) \\prod_{y_i=0} 1-logit(wx_i+b) \\right)^\\frac{1}{n}\n","$$\n","\n","Tomando la *log*-verosimilitud tenemos:\n","\n","$$\n","l(w,b) = \\log L(w,b) = \\\\ = \\frac{1}{n} \\left( \\sum_{y_i=1} \\log logit(wx_i+b) + \\sum_{y_i=0} \\log(1- logit(wx_i+b)) \\right) = \\\\ = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log logit(wx_i+b) + (1-y_i) log(1- logit(wx_i+b))\n","$$\n","\n","Con la que finalmente definimos la función de coste $J(w,b)$ de la regresión logística para transformarla en un problema de optimización:\n","\n","$$\n","\\underset{w,b}{\\mathrm{argmin}} - \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log logit(wx_i+b) + (1-y_i) log(1- logit(wx_i+b))\n","$$\n","\n","Al que podemos añadirle regularización ($\\ell1$, $\\ell2$ o *elasticNet*) y un hiperparámetro $C$ para controlar su importancia:\n","\n","$$\n","\\underset{w,b}{\\mathrm{argmin}}  \\frac{1}{2} (b^2 + w^2) - C \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log logit(wx_i+b) + (1-y_i) log(1- logit(wx_i+b))\n","$$\n","\n","Problema que podemos resolver mediante el descenso de gradiente sabiendo que $logit'(x) = logit(x) \\cdot (1-logit(x))$. Las derivadas parciales respecto de $w_j$ y $b$ quedan definidas como:\n","\n","$$\n","\\frac{\\partial J(w,b)}{\\partial w_j} = w_j + C \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j} ( logit(wx_i+b)-y_i)\n","$$\n","\n","$$\n","\\frac{\\partial J(w,b)}{\\partial b} = b + C \\frac{1}{n} \\sum_{i=1}^{n} logit(wx_i+b)-y_i\n","$$\n","\n","Y, en consecuencia, las ecuaciones de actualización añadiendo un híper-parámetro de regularización pasan a ser:\n","\n","$$\n","w_j \\leftarrow w_j - \\eta \\left( w_j + C \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j} ( logit(wx_i+b)-y_i) \\right)\n","$$\n","\n","$$\n","b \\leftarrow b - \\eta \\left( b + C \\frac{1}{n} \\sum_{i=1}^{n} logit(wx_i+b)-y_i \\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"rxATamrWidQk"},"source":["Gráficamente la regresión logística puede interpretarse como el problema de encontrar una división lineal para un conjunto de muestras etiquetadas.\n","\n","Por ejemplo, si disponemos del siguiente conjunto de datos:"]},{"cell_type":"code","metadata":{"id":"vb6-18kSEqS6","execution":{"iopub.status.busy":"2020-10-27T20:39:21.306Z","iopub.execute_input":"2020-10-27T20:39:21.322Z","iopub.status.idle":"2020-10-27T20:39:21.354Z","shell.execute_reply":"2020-10-27T20:39:21.379Z"}},"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","\n","X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=2, random_state=42)\n","\n","plt.figure()\n","\n","plt.ylim(-3, 18)\n","plt.xlim(-10, 11)\n","\n","plt.xlabel('X1')\n","plt.ylabel('X2')\n","\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZxj33AVitzA"},"source":["Aplicar la regresión logística dará como resultado una línea recta (estamos trabajando con 2 dimensiones) que separa ambos conjuntos de puntos:"]},{"cell_type":"code","metadata":{"id":"2KHdk_8OIG1R","execution":{"iopub.status.busy":"2020-10-27T20:39:25.966Z","iopub.execute_input":"2020-10-27T20:39:25.977Z","iopub.status.idle":"2020-10-27T20:39:26.360Z","shell.execute_reply":"2020-10-27T20:39:26.377Z"}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(random_state=42).fit(X, y)\n","\n","plt.figure()\n","\n","plt.ylim(-3, 18)\n","plt.xlim(-10, 11)\n","\n","plt.xlabel('X1')\n","plt.ylabel('X2')\n","\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n","\n","line = np.linspace(-10, 12)\n","plt.plot(line, -(line * clf.coef_[0][0] + clf.intercept_) / clf.coef_[0][1], c='grey')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ku_NOv8djK0X"},"source":["Es decir, el espacio en el que viven las muestras de nuestro conjunto de datos quedará separado linealmente en dos regiones:"]},{"cell_type":"code","metadata":{"id":"I9ItP6cpjZRR","execution":{"iopub.status.busy":"2020-10-27T20:39:31.953Z","iopub.execute_input":"2020-10-27T20:39:31.963Z","iopub.status.idle":"2020-10-27T20:39:32.061Z","shell.execute_reply":"2020-10-27T20:39:32.077Z"}},"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","\n","plt.figure()\n","\n","plt.ylim(-3, 18)\n","plt.xlim(-10, 11)\n","\n","plt.xlabel('X1')\n","plt.ylabel('X2')\n","\n","xx, yy = np.meshgrid(np.linspace(-10, 11, 100), np.linspace(-3, 18, 100))\n","Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n","Z = Z.reshape(xx.shape) \n","\n","plt.pcolormesh(xx, yy, Z, cmap=plt.cm.bwr_r)\n","plt.scatter(X[:,0], X[:,1], c='k', marker='x')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_ks4i4ckG94"},"source":["En las que el límite de decisión generará incertidumbre sobre cómo clasificar una muestra y genera como salida probabilidades inferiores intermedias que dificultarán la clasificación de las mismas."]},{"cell_type":"markdown","metadata":{"id":"VXsvR1k5a94W"},"source":["Si extendemos el ejemplo anterior para trabajar con 3 características ($x_1, x_2, x_3$) podemos observar que en lugar de dividir el espacio mediante una linea, la regresión logística encuentra un plano de separación entre ambos conjuntos de puntos:"]},{"cell_type":"code","metadata":{"id":"kQgExRYARuMM","execution":{"iopub.status.busy":"2020-10-27T20:39:50.759Z","iopub.execute_input":"2020-10-27T20:39:50.771Z","iopub.status.idle":"2020-10-27T20:39:50.919Z","shell.execute_reply":"2020-10-27T20:39:50.930Z"}},"source":["import numpy as np\n","\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import make_blobs\n","\n","# 3d figure\n","fig = plt.figure(figsize=(10,10))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.view_init(12,15)\n","\n","ax.set_xlabel('X1')\n","ax.set_ylabel('X2')\n","ax.set_zlabel('X3')\n","\n","# draw scatter points\n","X, y = make_blobs(n_samples=300, n_features=3, centers=2, cluster_std=2, random_state=42)\n","\n","min = np.amin(X, axis=0) * 1.2\n","max = np.amax(X, axis=0) * 1.2\n","\n","ax.set_xlim(min[0], max[0])\n","ax.set_ylim(min[1], max[1])\n","ax.set_zlim(min[2], max[2])\n","\n","ax.scatter(xs=X[:,0], ys=X[:,1], zs=X[:,2], c=y, cmap=plt.cm.bwr)\n","\n","# draw decision plane\n","clf = LogisticRegression(random_state=42).fit(X, y)\n","\n","xx = np.arange(min[0], max[0], 0.25)\n","yy = np.arange(min[1], max[1], 0.25)\n","xx, yy = np.meshgrid(xx, yy)\n","zz = -(clf.intercept_ + xx * clf.coef_[0][0] + yy * clf.coef_[0][1]) / clf.coef_[0][2]\n","\n","ax.plot_surface(xx, yy, zz, alpha=0.3, color='grey')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9iwsoe7kdEz"},"source":["Aunque la regresión logística funciona en términos generales de forma aceptable, se enfrenta al problema de que las muestras a clasificar deben ser separables linealmente. De lo contrario, la regresión logística no consigue aproximar bien el modelo y tiene *underfitting*."]},{"cell_type":"markdown","metadata":{"id":"INX91q7RcHNC"},"source":["Está limitación puede ser subsanada si combinamos el concepto de regresión logística con la regresión polinómica. De este modo, en lugar de encontrar una recta que separe linealmente las clases, encontrado una curva, que dependerá del grado del polinomio a utilizar, que logrará dicha separación.\n","\n","La siguiente imagen muestra el resultado de aplicar regresión logística polinómica con polinomios de grado 1 (equivalente a la regresión logística lineal), 2 y 3 sobre los conjuntos de datos *blobs*, *moons* y *circles*."]},{"cell_type":"code","metadata":{"id":"EXdb56BHmCFb","execution":{"iopub.status.busy":"2020-10-27T20:40:14.063Z","iopub.execute_input":"2020-10-27T20:40:14.083Z","iopub.status.idle":"2020-10-27T20:40:15.160Z","shell.execute_reply":"2020-10-27T20:40:15.179Z"}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import make_blobs, make_moons, make_circles\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","def plot_clasification(X, y, axs):\n","  min = np.amin(X, axis=0) * 1.2\n","  max = np.amax(X, axis=0) * 1.2\n","\n","  axs[0].set_title('Raw data')\n","  axs[0].set_xlabel('X1')\n","  axs[0].set_ylabel('X2')\n","\n","  axs[0].set_xlim(min[0], max[0])\n","  axs[0].set_ylim(min[1], max[1])\n","\n","  axs[0].scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n","\n","  for degree in range(1,4,1):\n","    poly = PolynomialFeatures(degree)\n","    poly.fit(X)\n","\n","    poly_X = poly.transform(X)\n","\n","    clf = LogisticRegression(random_state=42).fit(poly_X, y)\n","\n","    xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n","    Z = clf.predict_proba(poly.transform(np.c_[xx.ravel(), yy.ravel()]))[:,0]\n","    Z = Z.reshape(xx.shape) \n","\n","    if degree == 1:\n","      axs[degree].set_title('Lineal logistic regression')\n","    else:\n","      axs[degree].set_title('Polynomial logistic regresion (degree=' + str(degree) + ')')\n","    axs[degree].set_xlabel('X1')\n","    axs[degree].set_ylabel('X2')\n","\n","    axs[degree].set_xlim(min[0], max[0])\n","    axs[degree].set_ylim(min[1], max[1])\n","\n","    axs[degree].pcolormesh(xx, yy, Z, cmap=plt.cm.bwr_r)\n","    axs[degree].scatter(X[:,0], X[:,1], c='k', marker='x', alpha=0.75)\n","\n","fig, axs = plt.subplots(3, 4, figsize=(20,15))\n","\n","X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=2, random_state=42)\n","plot_clasification(X, y, axs[0])\n","\n","X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n","plot_clasification(X, y, axs[1])\n","\n","X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n","plot_clasification(X, y, axs[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"MGv9BwGbG0s0"},"source":["## Datos sintéticos\n","\n","Antes de empezar a trabajar con nuestro primer modelo de clasificación vamos a estudiar cómo podemos generar conjuntos de **datos sintéticos**. Como su propio nombre indica, los **conjuntos de datos sintéticos** son un conjunto de observaciones en el espacio $n$-dimensional de entrada de un determinado problema a las cuales se le asigna a su vez una etiqueta. Por tanto, son conjuntos de **datos etiquetados**. A estas observaciones se les asignan valores y etiquetas en base a alguna función matemática a la cual además se le añade ruido aleatorio.\n","\n","La principal utilidad de estas funciones generadoras es que podemos obtener conjuntos de datos etiquetados **tan grandes como queramos**. Obviamente, estos conjuntos de datos sintéticos dificilmente se podrán equiparar a un conjunto de datos tomados de algún proceso real, aunque como veremos a continuación, son muy interesantes para evaluar modelos de clasificación.\n","\n","Primero, vamos a inspeccionar un problema de clasificación binaria con dos dimensiones. Utilizaremos los datos sintéticos que nos proporciona la función [`make_blobs`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) de _scikit-learn_. Podemos observar en la documentación que esta función recibe algunos parámetros interesantes que definen las características del conjunto de datos sintético:\n","\n","* `n_samples` es el número de muestras que queremos generar. Básicamente, el tamaño en filas del _dataset_ sintético.\n","* `n_features` es el número de variables o _features_ $n$.\n","* `centers` es el número de centroides que tendrá el *dataset*, por lo que en nuestro caso se corresponde con el número de clases, categorías o etiquetas $k$."]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"execution":{"iopub.status.busy":"2020-10-27T20:46:47.992Z","iopub.execute_input":"2020-10-27T20:46:48.008Z","iopub.status.idle":"2020-10-27T20:46:48.035Z","shell.execute_reply":"2020-10-27T20:46:48.058Z"},"id":"m-EooVjHG0s0"},"source":["from sklearn.datasets import make_blobs\n","\n","X, y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=1337, cluster_std=3.0)\n","\n","print('X ~ n_samples x n_features:', X.shape)\n","print('y ~ n_samples:', y.shape)\n","\n","print('\\n5 primeros ejemplos:\\n', X[:5, :])\n","print('\\n5 primeras etiquetas:', y[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"kGwKT2CjG0s3"},"source":["Como los datos son bidimensionales, podemos representar cada punto en un sistema de coordenadas (ejes x e y)."]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"execution":{"iopub.status.busy":"2020-10-27T20:46:51.722Z","iopub.execute_input":"2020-10-27T20:46:51.736Z","iopub.status.idle":"2020-10-27T20:46:51.890Z","shell.execute_reply":"2020-10-27T20:46:51.878Z"},"id":"Lv21P3-fG0s3"},"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 5))\n","plt.scatter(X[y == 0, 0], X[y == 0, 1], \n","            c='blue', s=40, label='0')\n","plt.scatter(X[y == 1, 0], X[y == 1, 1], \n","            c='red', s=40, label='1', marker='s')\n","\n","plt.xlabel(\"primera característica\")\n","plt.ylabel(\"segunda característica\")\n","plt.legend(loc='upper right');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"BYOWNH6jG0s6"},"source":["### Un primer problema de clasificación\n","\n","Una vez hemos visto la formulación matemática que hay detrás del modelo, pasamo a utilizarlo para resolver problemas de clasificación. Vamos a utilizar el conjunto de datos sintéticos que hemos generado anteriormente.\n","\n","Como primera aproximación, vamos a dividir los datos en conjuntos de entrenamiento y test usando `train_test_split`:"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"execution":{"iopub.status.busy":"2020-10-27T20:41:40.361Z","iopub.execute_input":"2020-10-27T20:41:40.375Z","iopub.status.idle":"2020-10-27T20:41:40.391Z","shell.execute_reply":"2020-10-27T20:41:40.408Z"},"id":"4tnG7e16G0s6"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size=0.25,\n","                                                    random_state=1337,\n","                                                    stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"oYpd8KLvG0s8"},"source":["Como era de esperar, _scikit-learn_ tiene implementado el modelo de **regresión logística**. Se encuentra en el módulo de modelos lineales `linear_model`. Vamos a importarlo, crear un modelo con los [parámetros por defecto](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) y ajustarlo al conjunto de entrenamiento obtenido a partir del _dataset_ sintético:"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"execution":{"iopub.status.busy":"2020-10-27T20:43:01.430Z","iopub.execute_input":"2020-10-27T20:43:01.442Z","iopub.status.idle":"2020-10-27T20:43:01.473Z","shell.execute_reply":"2020-10-27T20:43:01.493Z"},"id":"C6alZAwoG0s9"},"source":["from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(solver='liblinear')\n","classifier.fit(X_train, y_train)\n","\n","classifier.get_params()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"aIMS2zS-G0s_"},"source":["Y ahora comprobamos la puntuación del clasificador utilizando el conjunto de test:"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"execution":{"iopub.status.busy":"2020-10-27T20:43:25.713Z","iopub.execute_input":"2020-10-27T20:43:25.727Z","iopub.status.idle":"2020-10-27T20:43:25.748Z","shell.execute_reply":"2020-10-27T20:43:25.765Z"},"id":"j17oGNjnG0tA"},"source":["classifier.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"TQ-5lgV3G0tC"},"source":["A continuación, se representa gráficamente la frontera de decisión que ha encontrado para las observaciones del conjunto de entrenamiento. Podemos consultar los coeficientes de la recta que ha calculado el modelo accediendo a las propiedades del mismo:"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"execution":{"iopub.status.busy":"2020-10-27T20:44:43.187Z","iopub.execute_input":"2020-10-27T20:44:43.202Z","iopub.status.idle":"2020-10-27T20:44:43.230Z","shell.execute_reply":"2020-10-27T20:44:43.254Z"},"id":"7XjRAi2mG0tC"},"source":["print(classifier.coef_)\n","print(classifier.intercept_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"execution":{"iopub.status.busy":"2020-10-27T20:45:14.197Z","iopub.execute_input":"2020-10-27T20:45:14.211Z","iopub.status.idle":"2020-10-27T20:45:14.356Z","shell.execute_reply":"2020-10-27T20:45:14.368Z"},"id":"3xq7XVZcG0tF"},"source":["import numpy as np\n","plt.figure(figsize=(10, 5))\n","plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n","            c='blue', s=40, label='0')\n","plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n","            c='red', s=40, label='1', marker='s')\n","\n","x_values = [np.min(X_train[y_train == 0, 0] - 1), np.max(X_train[y_train == 0, 1] + 3)]\n","y_values = - (classifier.intercept_ + np.dot(classifier.coef_[0][0], x_values)) / classifier.coef_[0][1]\n","\n","plt.xlabel(\"primera característica\")\n","plt.ylabel(\"segunda característica\")\n","plt.plot(x_values, y_values, label='Frontera')\n","plt.legend(loc='upper right');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FEVeKp2rk4Pb"},"source":["---\n","\n","Creado por **Fernando Ortega** (fernando.ortega@upm.es), **Ángel González Prieto** (angel.gonzalez.prieto@upm.es) y **Raúl Lara Cabrera** (raul.lara@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"EZmAC501G0tH"},"source":[""]}]}