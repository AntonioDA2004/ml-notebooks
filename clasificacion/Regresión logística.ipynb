{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5kKZ0FHG0sX",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Aprendizaje Supervisado: Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JexeUSmrG0sY",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "A diferencia de la regresión, en el cual predecíamos un valor continuo, ahora intentaremos predecir la etiqueta, grupo o categoría de distintas observaciones. Por ejemplo, la regresión lineal de un conjunto de datos sociales y económicos podría utilizarse para predecir los ingresos de una persona, pero la regresión logística podría utilizarse para predecir si esa persona estaba casada, tenía hijos o había sido detenida alguna vez.\n",
    "\n",
    "Es decir, para resolver un **Problema de clasificación** necesitamos encontrar una función\n",
    "\n",
    "$$\n",
    "M: \\mathbb{V}^n \\rightarrow \\left \\{ c_1, c_2, \\ldots, c_k \\right \\}\n",
    "$$\n",
    "\n",
    "que, a partir de una entrada $n$-dimensional nos devuelva a cuál de las $k$ categorías o clases pertenece dicha entrada. Hay que tener en cuenta que cada variable $v_i\\in \\mathbb{V}, i\\in\\left \\{1, 2, \\ldots, n\\right \\}$ puede ser categórica, ordinal, discreta o continua.\n",
    "\n",
    "Otra característica importante de los **problemas de clasificación** es que se encuadran dentro del **aprendizaje supervisado**, puesto que es necesario ajustar (entrenar) los modelos de clasificación a partir de un conjunto de datos de entrada etiquetados.\n",
    "\n",
    "Un algoritmo que implementa la clasificación, especialmente en una implementación concreta, se conoce como **clasificador**. El término **clasificador** a veces también se refiere a la función matemática, implementada por un algoritmo de clasificación, que asigna los datos de entrada a una categoría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsQ25mAARw00"
   },
   "source": [
    "# Regresión logística\n",
    "\n",
    "La regresión logística, a pesar de su nombre, es una algoritmo de **clasificación** binario que basa su funcionamiento en los principios de la regresión lineal. Aunque en esencia este modelo solo puede aplicarse a **problemas de clasificación binaria**, es decir, donde solo hay dos categorías posibles, es fácilmente [extensible](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) para que pueda dar respuesta a problemas con un número de clases $k>2$.\n",
    "\n",
    "Para poder entender su funcionamiento vamos a suponer que disponemos del siguiente conjunto de datos:\n",
    "\n",
    "| x1 | x2 | y |\n",
    "| -- | -- | - |\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 0 |\n",
    "| 1  | 0  | 0 |\n",
    "| 1  | 1  | 1 |\n",
    "\n",
    "Como observamos disponemos de dos características `x1` y `x2` que condicionan la clase `0` o `1` a la que pertenece una determinada muestra. A priori, no es posible tratar este problema como uno de regresión ya que la salida `y` es discreta y no continua. Para poder resolver una clasificación usando regresión lineal, la regresión logística transforma esta salida en una salida continua empleando la función sigmoide o `logit`.\n",
    "\n",
    "La función sigmoide es una función monótona creciente que aproxima asintóticamente a 1 para valores positivos y asintóticamente a 0 para valores negativos. Podemos verla en la siguiente imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:08:46.012818Z",
     "start_time": "2021-05-17T10:08:45.843229Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-27T20:25:59.628Z",
     "iopub.status.busy": "2020-10-27T20:25:59.612Z",
     "iopub.status.idle": "2020-10-27T20:26:00.945Z",
     "shell.execute_reply": "2020-10-27T20:26:00.954Z"
    },
    "id": "djuzq6KjTzK4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "x = np.linspace(-10,10,100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('logit(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QUWb0GkUfQ-"
   },
   "source": [
    "Matemáticamente la función sigmoide se define como:\n",
    "\n",
    "$$\n",
    "logit(x) = \\frac{1}{1+ e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITmi7mCdU3H4"
   },
   "source": [
    "Por tanto, la regresión logística transforma un problema de clasificación en uno de regresión (de ahí su nombre) donde la variable objetivo $y$ es en realidad una función sigmoide. Dicho de otro modo, usando las características $x = x_1, \\dots, x_m$ se calculará un valor de regresión que será pasado como parámetro de una función sigmoide. Posteriormente se establecerá un umbral para dicha salida, habitualmente $0.5$, de tal modo que todas las \"regresiones\" inferiores a dicho valor se clasificarán en la `clase 0` mientras que el resto se clasificarán en la `clase 1`:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "0 & \\text{si } logit(w x + b) < 0.5 \\\\\n",
    "1 & \\text{si } logit(w x + b) \\geq 0.5 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Al estar la función sigmoide acotada al intervalo $(0,1)$ podemos expresar la salida de la regresión logística en términos probabísticos. Se define, por tanto, la probabilidad de pertenecer a cada una de las clases como:\n",
    "\n",
    "$$\n",
    "p_{w,b}(x,y) =\n",
    "\\begin{cases}\n",
    "logit(w x+b)     & \\text{si } y = 1 \\\\\n",
    "1 - logit(w x+b) & \\text{si } y = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Evaluando todo el conjunto de datos se obtiene la siguiente función de verosimilitud:\n",
    "\n",
    "$$\n",
    "L(w,b)= \\left( \\prod_{i=1}^{n} p_{w,b}(x_i,y_i) \\right)^\\frac{1}{n} = \\left( \\prod_{y_i=1} logit(wx_i+b) \\prod_{y_i=0} 1-logit(wx_i+b) \\right)^\\frac{1}{n}\n",
    "$$\n",
    "\n",
    "Tomando la *log*-verosimilitud tenemos:\n",
    "\n",
    "$$\n",
    "l(w,b) = \\log L(w,b) = \\\\ = \\frac{1}{n} \\left( \\sum_{y_i=1} \\log (logit(wx_i+b)) + \\sum_{y_i=0} \\log(1- logit(wx_i+b)) \\right) = \\\\ = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log (logit(wx_i+b)) + (1-y_i) log(1- logit(wx_i+b))\n",
    "$$\n",
    "\n",
    "Con la que finalmente definimos la función de coste $J(w,b)$ de la regresión logística para transformarla en un problema de optimización:\n",
    "\n",
    "$$\n",
    "\\underset{w,b}{\\mathrm{argmin}} - \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log (logit(wx_i+b)) + (1-y_i) log(1- logit(wx_i+b))\n",
    "$$\n",
    "\n",
    "Al que podemos añadirle regularización ($\\ell1$, $\\ell2$ o *elasticNet*) y un hiperparámetro $C$ para controlar su importancia:\n",
    "\n",
    "$$\n",
    "\\underset{w,b}{\\mathrm{argmin}}  \\frac{1}{2} (b^2 + w^2) - C \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log (logit(wx_i+b)) + (1-y_i) log(1- logit(wx_i+b))\n",
    "$$\n",
    "\n",
    "Problema que podemos resolver mediante el descenso de gradiente sabiendo que $logit'(x) = logit(x) \\cdot (1-logit(x))$. Las derivadas parciales respecto de $w_j$ y $b$ quedan definidas como:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w_j} = w_j + C \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j} ( logit(wx_i+b)-y_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b} = b + C \\frac{1}{n} \\sum_{i=1}^{n} logit(wx_i+b)-y_i\n",
    "$$\n",
    "\n",
    "Y, en consecuencia, las ecuaciones de actualización añadiendo un híper-parámetro de regularización pasan a ser:\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j - \\eta \\left( w_j + C \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j} ( logit(wx_i+b)-y_i) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b - \\eta \\left( b + C \\frac{1}{n} \\sum_{i=1}^{n} logit(wx_i+b)-y_i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxATamrWidQk"
   },
   "source": [
    "Gráficamente la regresión logística puede interpretarse como el problema de encontrar una división lineal para un conjunto de muestras etiquetadas.\n",
    "\n",
    "Por ejemplo, si disponemos del siguiente conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:09:12.485493Z",
     "start_time": "2021-05-17T10:09:12.379522Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-27T20:39:21.322Z",
     "iopub.status.busy": "2020-10-27T20:39:21.306Z",
     "iopub.status.idle": "2020-10-27T20:39:21.354Z",
     "shell.execute_reply": "2020-10-27T20:39:21.379Z"
    },
    "id": "vb6-18kSEqS6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=2, random_state=42)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZxj33AVitzA"
   },
   "source": [
    "Aplicar la regresión logística dará como resultado una línea recta (estamos trabajando con 2 dimensiones) que separa ambos conjuntos de puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:09:14.040680Z",
     "start_time": "2021-05-17T10:09:13.931962Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-27T20:39:25.977Z",
     "iopub.status.busy": "2020-10-27T20:39:25.966Z",
     "iopub.status.idle": "2020-10-27T20:39:26.360Z",
     "shell.execute_reply": "2020-10-27T20:39:26.377Z"
    },
    "id": "2KHdk_8OIG1R"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42).fit(X, y)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "line = np.linspace(min[0], max[0])\n",
    "plt.plot(line, -(line * clf.coef_[0][0] + clf.intercept_) / clf.coef_[0][1], c='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku_NOv8djK0X"
   },
   "source": [
    "Es decir, el espacio en el que viven las muestras de nuestro conjunto de datos quedará separado linealmente en dos regiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:09:29.813213Z",
     "start_time": "2021-05-17T10:09:29.699549Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-27T20:39:31.963Z",
     "iopub.status.busy": "2020-10-27T20:39:31.953Z",
     "iopub.status.idle": "2020-10-27T20:39:32.061Z",
     "shell.execute_reply": "2020-10-27T20:39:32.077Z"
    },
    "id": "I9ItP6cpjZRR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(min[0], max[0])\n",
    "plt.ylim(min[1], max[1])\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(min[0], max[0]), np.linspace(min[1], max[1]))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape) \n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.bwr_r)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_ks4i4ckG94"
   },
   "source": [
    "En las que el límite de decisión generará incertidumbre sobre cómo clasificar una muestra y genera como salida probabilidades inferiores intermedias que dificultarán la clasificación de las mismas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXsvR1k5a94W"
   },
   "source": [
    "Si extendemos el ejemplo anterior para trabajar con 3 características ($x_1, x_2, x_3$) podemos observar que en lugar de dividir el espacio mediante una linea, la regresión logística encuentra un plano de separación entre ambos conjuntos de puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:08:47.098412Z",
     "start_time": "2021-05-17T10:08:46.860069Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-27T20:39:50.771Z",
     "iopub.status.busy": "2020-10-27T20:39:50.759Z",
     "iopub.status.idle": "2020-10-27T20:39:50.919Z",
     "shell.execute_reply": "2020-10-27T20:39:50.930Z"
    },
    "id": "kQgExRYARuMM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 3d figure\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.view_init(12,15)\n",
    "\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('X3')\n",
    "\n",
    "# draw scatter points\n",
    "X, y = make_blobs(n_samples=300, n_features=3, centers=2, cluster_std=2, random_state=42)\n",
    "\n",
    "min = np.amin(X, axis=0)\n",
    "max = np.amax(X, axis=0)\n",
    "\n",
    "diff = max - min\n",
    "\n",
    "min = min - 0.1 * diff\n",
    "max = max + 0.1 * diff\n",
    "\n",
    "ax.set_xlim(min[0], max[0])\n",
    "ax.set_ylim(min[1], max[1])\n",
    "ax.set_zlim(min[2], max[2])\n",
    "\n",
    "ax.scatter(xs=X[:,0], ys=X[:,1], zs=X[:,2], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "# draw decision plane\n",
    "clf = LogisticRegression(random_state=42).fit(X, y)\n",
    "\n",
    "xx = np.arange(min[0], max[0], 0.25)\n",
    "yy = np.arange(min[1], max[1], 0.25)\n",
    "xx, yy = np.meshgrid(xx, yy)\n",
    "zz = -(clf.intercept_ + xx * clf.coef_[0][0] + yy * clf.coef_[0][1]) / clf.coef_[0][2]\n",
    "\n",
    "ax.plot_surface(xx, yy, zz, alpha=0.3, color='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9iwsoe7kdEz"
   },
   "source": [
    "Aunque la regresión logística funciona en términos generales de forma aceptable, se enfrenta al problema de que las muestras a clasificar deben ser separables linealmente. De lo contrario, la regresión logística no consigue aproximar bien el modelo y tiene *underfitting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INX91q7RcHNC"
   },
   "source": [
    "Está limitación puede ser subsanada si combinamos el concepto de regresión logística con la regresión polinómica. De este modo, en lugar de encontrar una recta que separe linealmente las clases, encontrado una curva, que dependerá del grado del polinomio a utilizar, que logrará dicha separación.\n",
    "\n",
    "La siguiente imagen muestra el resultado de aplicar regresión logística polinómica con polinomios de grado 1 (equivalente a la regresión logística lineal), 2 y 3 sobre los conjuntos de datos *blobs*, *moons* y *circles*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T10:10:35.048554Z",
     "start_time": "2021-05-17T10:10:33.699039Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-27T20:40:14.083Z",
     "iopub.status.busy": "2020-10-27T20:40:14.063Z",
     "iopub.status.idle": "2020-10-27T20:40:15.160Z",
     "shell.execute_reply": "2020-10-27T20:40:15.179Z"
    },
    "id": "EXdb56BHmCFb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def plot_clasification(X, y, axs):\n",
    "  min = np.amin(X, axis=0)\n",
    "  max = np.amax(X, axis=0)\n",
    "\n",
    "  diff = max - min\n",
    "\n",
    "  min = min - 0.1 * diff\n",
    "  max = max + 0.1 * diff\n",
    "\n",
    "  axs[0].set_title('Raw data')\n",
    "  axs[0].set_xlabel('X1')\n",
    "  axs[0].set_ylabel('X2')\n",
    "\n",
    "  axs[0].set_xlim(min[0], max[0])\n",
    "  axs[0].set_ylim(min[1], max[1])\n",
    "\n",
    "  axs[0].scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "\n",
    "  for degree in range(1,4,1):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    poly.fit(X)\n",
    "\n",
    "    poly_X = poly.transform(X)\n",
    "\n",
    "    clf = LogisticRegression(random_state=42).fit(poly_X, y)\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "    Z = clf.predict_proba(poly.transform(np.c_[xx.ravel(), yy.ravel()]))[:,0]\n",
    "    Z = Z.reshape(xx.shape) \n",
    "\n",
    "    if degree == 1:\n",
    "      axs[degree].set_title('Lineal logistic regression')\n",
    "    else:\n",
    "      axs[degree].set_title('Polynomial logistic regresion (degree=' + str(degree) + ')')\n",
    "    axs[degree].set_xlabel('X1')\n",
    "    axs[degree].set_ylabel('X2')\n",
    "\n",
    "    axs[degree].set_xlim(min[0], max[0])\n",
    "    axs[degree].set_ylim(min[1], max[1])\n",
    "\n",
    "    axs[degree].pcolormesh(xx, yy, Z, cmap=plt.cm.bwr_r, vmin=0, vmax=1)\n",
    "    axs[degree].scatter(X[:,0], X[:,1], c=y, edgecolor='black', cmap='bwr')\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(20,15))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=2, cluster_std=2, random_state=42)\n",
    "plot_clasification(X, y, axs[0])\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "plot_clasification(X, y, axs[1])\n",
    "\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "plot_clasification(X, y, axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEVeKp2rk4Pb"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Fernando Ortega** (fernando.ortega@upm.es), **Ángel González Prieto** (angel.gonzalez.prieto@upm.es) y **Raúl Lara Cabrera** (raul.lara@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regresión logística.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.26.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
