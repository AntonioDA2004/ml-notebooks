{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyIx4EzklFpL"
   },
   "source": [
    "# Aprendizaje basado en *Ensembles*\n",
    "\n",
    "\n",
    "El aprendizaje basado en [*ensembles*](https://scikit-learn.org/stable/modules/ensemble.html) intenta buscar una sinergia entre distintos modelos de aprendizaje computacional. El término proviene del [ámbito musical](https://en.wikipedia.org/wiki/Musical_ensemble), en el cual varias bandas o solistas se agrupan con un nuevo nombre para tocar juntos.\n",
    "\n",
    "La premisa es que un conjunto de expertos será capaz de ofrecer mejores predicciones que los expertos por separado. Esto se consigue complementando las carencias de un modelo con otros modelos.\n",
    "\n",
    "Los métodos *ensemble* se distribuyen en tres grandes familias:\n",
    "\n",
    "- **_Votting_**: diferentes clasificadores se juntan para determinar, mediante votación, qué etiqueta debe asignarse a una muestra.\n",
    "\n",
    "- **_Bagging_**: se entrenan múltiples instancias de un mismo clasificador eliminando aleatoriamente en cada entrenamiento muestras y características de los datos.\n",
    "\n",
    "- **_Boosting_**: los modelos del ensemble se construyen de manera iterativa, intentando reducir el sesgo de la combinación de todos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VoznrnEkmwc7"
   },
   "source": [
    "## *Voting Classifier*\n",
    "\n",
    "La idea detrás de un ([`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)) es combinar clasificadores conceptualmente diferentes y utilizar un voto mayoritario (*hard*) o las probabilidades promedio pronosticadas (*soft-voting*) para predecir las etiquetas. \n",
    "\n",
    "Este clasificador puede ser útil para un conjunto de modelos con un rendimiento igualmente bueno, a fin de equilibrar sus debilidades individuales.\n",
    "\n",
    "Veamos el resultado de clasificar un conjunto de datos sintético con los clasificadores `DecisionTreeClassifier`, `KNeighborsClassifier` y `SVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T08:19:40.018486Z",
     "start_time": "2021-06-07T08:19:36.687339Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.5, random_state=42)\n",
    "\n",
    "clf1 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='hard', weights=[2, 1, 2])\n",
    "\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X, y)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "def plot_clf(clf, axs, title): \n",
    "    axs.set_title(title)\n",
    "    \n",
    "    axs.set_xlabel('X1')\n",
    "    axs.set_ylabel('X2')\n",
    "    \n",
    "    axs.scatter(X[:,0], X[:,1], c=y, cmap='rainbow')\n",
    "    \n",
    "    xmin, xmax = axs.get_xlim()\n",
    "    ymin, ymax = axs.get_ylim()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 100), np.linspace(ymin, ymax, 100))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    grid_pred = clf.predict(grid).reshape(xx.shape)\n",
    "    axs.contourf(xx, yy, grid_pred, alpha=0.2, cmap='rainbow')\n",
    "    \n",
    "plot_clf(clf1, axs[0,0], 'DecisionTreeClassifier')\n",
    "plot_clf(clf2, axs[0,1], 'KNeighborsClassifier')\n",
    "plot_clf(clf3, axs[1,0], 'SVC')\n",
    "plot_clf(eclf, axs[1,1], 'VotingClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIv8IPnEtB9D"
   },
   "source": [
    "En el caso anterior hemos configurado el `VotingClassifier` especificando el parámetro `voting='hard'` por lo que la salida del clasificador será la opinión mayoritaria del resto de clasificadores.\n",
    "\n",
    "Y ahora probamos con una configuración `voting='soft'`, que trabaja con las probabilidades de que la muestra pertenezca a una clase en lugar de con la clase directamente. Además, es posible modificar el peso de la probabilidad de cada clasificador mediante el parámetro `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T08:19:54.166307Z",
     "start_time": "2021-06-07T08:19:52.202397Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.5, random_state=42)\n",
    "\n",
    "clf1 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 2])\n",
    "\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X, y)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "def plot_clf(clf, axs, title): \n",
    "    axs.set_title(title)\n",
    "    \n",
    "    axs.set_xlabel('X1')\n",
    "    axs.set_ylabel('X2')\n",
    "    \n",
    "    axs.scatter(X[:,0], X[:,1], c=y, edgecolor='black', s=20, cmap='rainbow')\n",
    "    \n",
    "    xmin, xmax = axs.get_xlim()\n",
    "    ymin, ymax = axs.get_ylim()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 100), np.linspace(ymin, ymax, 100))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    grid_pred = clf.predict(grid).reshape(xx.shape)\n",
    "    axs.contourf(xx, yy, grid_pred, alpha=0.2, cmap='rainbow')\n",
    "    \n",
    "plot_clf(clf1, axs[0,0], 'DecisionTreeClassifier')\n",
    "plot_clf(clf2, axs[0,1], 'KNeighborsClassifier')\n",
    "plot_clf(clf3, axs[1,0], 'SVC')\n",
    "plot_clf(eclf, axs[1,1], 'VotingClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKDtm8w3lFqA"
   },
   "source": [
    "## Bagging Classifier\n",
    "\n",
    "El funcionamiento de estos métodos consiste en entrenar varias instancias de un mismo modelo de aprendizaje computacional, pero usando un subconjunto distinto de muestras seleccionadas aleatoriamente para cada entrenamiento.\n",
    "\n",
    "En `sklearn` tenemos el meta-clasificador [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier). Destacamos algunos de sus parámetros:\n",
    "\n",
    "- `base_estimator`: estimador sobre el que realizar los múltiples entrenamientos.\n",
    "- `n_estimators`: permite definir el número de muestreos a ejecutar sobre el clasificador especificado.\n",
    "- `max_samples`: porcentaje (o número) de muestras a utilizar en cada muestreo.\n",
    "- `max_features`: porcentaje (o número) de features a utilizar en cada muestreo.\n",
    "\n",
    "Veamos su funcionamiento empelando `DecissionTreeClassifier` como estimador base y empleando el 30% de las muestras en cada entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T08:24:43.923374Z",
     "start_time": "2021-06-07T08:24:40.695675Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=8, max_samples=0.3, random_state=1)\n",
    "bag.fit(X, y)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15,15))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "min, max = np.amin(X, axis=0), np.amax(X, axis=0)\n",
    "diff = max - min\n",
    "min, max = min - 0.1 * diff, max + 0.1 * diff\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        \n",
    "        axs[i,j].set_xlabel('X1')\n",
    "        axs[i,j].set_ylabel('X2')\n",
    "\n",
    "        axs[i,j].set_xlim(min[0], max[0])\n",
    "        axs[i,j].set_ylim(min[1], max[1])\n",
    "                \n",
    "        xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "        \n",
    "        if i == 0 and j == 0:\n",
    "            axs[i,j].set_title('Bagging Classifier')\n",
    "            Z = bag.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            axs[i,j].contourf(xx, yy, Z, alpha=0.4, cmap='rainbow')\n",
    "        \n",
    "        else:\n",
    "            t = 3 * i + j - 1 # tree index\n",
    "    \n",
    "            axs[i,j].set_title('Decision Tree #' + str(t+1))\n",
    "            Z = bag.estimators_[t].predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            axs[i,j].contourf(xx, yy, Z, alpha=0.4, cmap='rainbow')\n",
    "            \n",
    "        axs[i,j].scatter(X[:,0], X[:,1], c=y, edgecolor='black', s=20, cmap='rainbow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxszxPAxjMXk"
   },
   "source": [
    "## Boosting\n",
    "\n",
    "[AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) es el máximo exponente de los algoritmos de boosting. El principio básico de AdaBoost es ajustar una secuencia de aprendizajes débiles (es decir, modelos que sólo son ligeramente mejores que una clasificación aleatoria) usando versiones modificadas de los datos. Las predicciones de todos ellos se combinan finalmente mediante una votación por mayoría ponderada (o suma) para producir la predicción final. \n",
    "\n",
    "Las modificaciones de los datos en cada una de las iteraciones consisten en aplicar pesos $w_1, w_2, \\dots, w_n$ a cada una de las $n$ muestras de entrenamiento. Inicialmente, estos pesos se fijan a $w_i=\\frac{1}{n}$, de tal modo que en la primera iteración se produce un entrenamiento \"débil\" de los datos originales. En cada iteración sucesiva, los pesos de la muestra se modifican individualmente y el algoritmo de aprendizaje se vuelve a aplicar a los datos re-ponderados. En un paso determinado, los ejemplos de entrenamiento que fueron incorrectamente predichos por el modelo del paso anterior aumentan sus pesos, mientras que los pesos se reducen para los que fueron predichos correctamente. A medida que avanzan las iteraciones, los ejemplos difíciles de predecir reciben una influencia cada vez mayor, logrando, de este modo, que los clasificadores se concentren en las muestras que no han sido correctamente clasificadas.\n",
    "\n",
    "`sklearn` nos proporciona la clase [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T09:22:27.587380Z",
     "start_time": "2021-06-07T09:22:23.948766Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.20, random_state=1337)\n",
    "\n",
    "min, max = np.amin(X, axis=0), np.amax(X, axis=0)\n",
    "diff = max - min\n",
    "min, max = min - 0.1 * diff, max + 0.1 * diff\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(min[0], max[0], 100), np.linspace(min[1], max[1], 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    row = int(i / 3)\n",
    "    col = i % 3\n",
    "        \n",
    "    axs[row,col].set_title('AdaBoost (paso ' + str(i+1) + ')')\n",
    "    \n",
    "    axs[row,col].set_xlabel('X1')\n",
    "    axs[row,col].set_ylabel('X2')\n",
    "\n",
    "    axs[row,col].set_xlim(min[0], max[0])\n",
    "    axs[row,col].set_ylim(min[1], max[1])\n",
    "        \n",
    "    n_estimators = i+1\n",
    "    \n",
    "    adaboost = AdaBoostClassifier(n_estimators=n_estimators, random_state=42).fit(X, y)\n",
    "    \n",
    "    grid_pred = adaboost.predict(grid).reshape(xx.shape)\n",
    "    axs[row,col].contourf(xx, yy, grid_pred, alpha=0.2, cmap='bwr')\n",
    "    \n",
    "    wrong = X[adaboost.predict(X) != y]\n",
    "    axs[row,col].scatter(wrong[:,0], wrong[:,1], c='yellow', s=500, alpha=0.2)\n",
    "\n",
    "    axs[row,col].scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='black')\n",
    "\n",
    "    if i < 8:\n",
    "        adaboost_next = AdaBoostClassifier(n_estimators=n_estimators+1, random_state=42).fit(X, y)\n",
    "\n",
    "        clf = adaboost_next.estimators_[n_estimators]\n",
    "        grid_pred_next = clf.predict(grid).reshape(xx.shape)    \n",
    "        axs[row,col].contour(xx, yy, grid_pred_next, linewidths=1, linestyles='dashed', colors='black')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqczAnNamFnZ"
   },
   "source": [
    "## Stacked generalization\n",
    "\n",
    "*Stacked generalization* es un método para combinar los estimadores para reducir sus sesgos. Concretamente, las predicciones de cada estimador individual se apilan y se usan como entrada a un estimador final para calcular la predicción. Este estimador final es entrenado a través de la validación cruzada.\n",
    "\n",
    "El estimador final generalmente se alimenta únicamente de la salida de los clasificadores previos:\n",
    "\n",
    "![Stacked generalization sin features](https://wolpert.readthedocs.io/en/latest/_images/stack_example_001.png)\n",
    "\n",
    "No obstante, también es posible añadir a dicho estimador las muestras del conjunto de datos:\n",
    "\n",
    "![Stacked generalization con features](https://wolpert.readthedocs.io/en/latest/_images/restack_graph.png)\n",
    "\n",
    "En `sklearn` tenemos este clasificador implementado en la clase [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier). Los estimadores a combinar se deben definir en el parámetro `estimators` mientras que el estimador final se define en el parametro `final_estimator`. El parámetro `passthrough` permite definir si el estimador final se entrena sólo con la salida de los estimadores (`passthrough=False`, valor por defecto) o se usan también las muestras de entrada (`passthrough=True`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T09:32:00.146837Z",
     "start_time": "2021-06-07T09:31:56.128883Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=4, random_state=42)\n",
    "\n",
    "clf1 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "estimators = [('dt', clf1), ('knn', clf2), ('svc', clf3)]\n",
    "\n",
    "sclf_pass_false = stack_clf = StackingClassifier(estimators=estimators, passthrough=False)\n",
    "sclf_pass_true = stack_clf = StackingClassifier(estimators=estimators, passthrough=True)\n",
    "\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "\n",
    "lr = lr.fit(X, y)\n",
    "\n",
    "sclf_pass_false = sclf_pass_false.fit(X, y)\n",
    "sclf_pass_true = sclf_pass_true.fit(X, y)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "def plot_clf(clf, axs, title): \n",
    "    axs.set_title(title)\n",
    "    \n",
    "    axs.set_xlabel('X1')\n",
    "    axs.set_ylabel('X2')\n",
    "    \n",
    "    axs.scatter(X[:,0], X[:,1], c=y, cmap='rainbow')\n",
    "    \n",
    "    xmin, xmax = axs.get_xlim()\n",
    "    ymin, ymax = axs.get_ylim()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 100), np.linspace(ymin, ymax, 100))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    grid_pred = clf.predict(grid).reshape(xx.shape)\n",
    "    axs.contourf(xx, yy, grid_pred, alpha=0.2, cmap='rainbow')\n",
    "    \n",
    "plot_clf(clf1, axs[0,0], 'DecisionTreeClassifier')\n",
    "plot_clf(clf2, axs[0,1], 'KNeighborsClassifier')\n",
    "plot_clf(clf3, axs[0,2], 'SVC')\n",
    "\n",
    "plot_clf(lr, axs[1,0], 'LogisticRegression (final_estimator)')\n",
    "plot_clf(sclf_pass_false, axs[1,1], 'StackingClassifier (sin passthrough)')\n",
    "plot_clf(sclf_pass_true, axs[1,2], 'StackingClassifier (con passthrough)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrEOVl7f8Gly"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Raúl Lara** (raul.lara@upm.es) y **Fernando Ortega** (fernando.ortega@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ensembles.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
