{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"nav_menu":{"height":"252px","width":"333px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"name":"Ensembles.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"qyIx4EzklFpL"},"source":["# Aprendizaje basado en Ensembles\n","\n","<img src =\"https://i.imgur.com/OxLKD6t.png\" width=\"700\">\n","\n","El aprendizaje basado en [Ensembles](https://scikit-learn.org/stable/modules/ensemble.html) intenta buscar una sinergia entre distintos modelos de aprendizaje computacional. El término proviene del [ámbito musical](https://en.wikipedia.org/wiki/Musical_ensemble), en el cual varias bandas o solistas se agrupan con un nuevo nombre para tocar juntos.\n","\n","La premisa es que un conjunto de expertos será capaz de ofrecer mejores predicciones que los expertos por separado. Esto se consigue complementando las carencias de un modelo con otros modelos.\n","\n","Los métodos Ensemble se distribuyen en dos grandes familias:\n","\n","- **Métodos de promediado** (*averaging methods*): cuyo funcionamiento consiste en entrenar de forma paralela y de manera independiente varios modelos de aprendizaje para obtener la predicción final mediante una combinación de la predicción de los modelos por separado. Los modelos que se entrenan en esta familia suelen ser modelos robustos y complejos, que obtienen buenos resultados por sí mismos.\n","\n","- **Métodos Boosting** (*boosting methods*): los modelos del ensemble se construyen de manera iterativa, intentando reducir el sesgo de la combinación de todos. A diferencia de los métodos de promediado, los modelos que se construyen en esta familia suelen ser modelos muy simples. Técnicamente nos basta con que el modelo obtenga precisiones superiores al 0.5"]},{"cell_type":"markdown","metadata":{"id":"JM6__Dp7lFpQ"},"source":["## Configuración y librerías"]},{"cell_type":"code","metadata":{"id":"WasXjbS7lFpV"},"source":["# Librerías comunes\n","import numpy as np\n","import os\n","import pandas as pd\n","\n","# Fijamos una semilla para el PRNG de Numpy\n","np.random.seed(1337)\n","\n","# Parámetros de Matplotlib\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['axes.labelsize'] = 14\n","plt.rcParams['xtick.labelsize'] = 12\n","plt.rcParams['ytick.labelsize'] = 12"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ulijnBq4lFpb"},"source":["## Clasificación por votación"]},{"cell_type":"markdown","metadata":{"id":"VoznrnEkmwc7"},"source":["La idea detrás de un **Clasificador por Votación** ([`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)) es combinar clasificadores conceptualmente diferentes y utilizar un voto mayoritario (*hard*) o las probabilidades promedio pronosticadas (*soft-voting*) para predecir las etiquetas. \n","\n","Este clasificador puede ser útil para un conjunto de modelos con un rendimiento igualmente bueno, a fin de equilibrar sus debilidades individuales.\n","\n","Vamos a generar un _dataset_ sintético con la función `make_moons` de sklearn que sirve para generar nubes de puntos con forma de lunas entrelazadas:"]},{"cell_type":"code","metadata":{"id":"KMAfqNN_morD"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples=1000, noise=0.30, random_state=1337)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1337)\n","\n","df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n","colors = {0:'red', 1:'blue'}\n","fig, ax = plt.subplots()\n","grouped = df.groupby('label')\n","for key, group in grouped:\n","    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g-m8j5lXcwsr"},"source":["Ahora, calcularemos la predicción uniendo los clasificadores *Logistic Regression*, *Naïve Bayes* y *SVC*:"]},{"cell_type":"code","metadata":{"id":"qHiW1lNNlFps"},"source":["from sklearn.ensemble import VotingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","\n","log_clf = LogisticRegression(random_state=42, solver='lbfgs')\n","nb_clf = GaussianNB()\n","svm_clf = SVC(random_state=42, gamma='auto')\n","\n","voting_clf = VotingClassifier(\n","    estimators=[('lr', log_clf), ('nb', nb_clf), ('svc', svm_clf)],\n","    voting='hard')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMmT8jcClFpv"},"source":["voting_clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVbZXQ_ElFp0"},"source":["from sklearn.metrics import accuracy_score\n","\n","for clf in (log_clf, nb_clf, svm_clf, voting_clf):\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIv8IPnEtB9D"},"source":["En el caso anterior hemos configurado el `VotingClassifier` especificando el parámetro `voting='hard'` por lo que la salida del clasificador será la opinión mayoritaria del resto de clasificadores.\n","\n","Y ahora probamos con una configuración `voting='soft'`, que trabaja con las probabilidades de que la muestra pertenezca a una clase en lugar de con la clase directamente. Además, es posible modificar el peso de la probabilidad de cada clasificador mediante el parámetro `weights`."]},{"cell_type":"code","metadata":{"id":"w3tLD1G_lFp3"},"source":["log_clf = LogisticRegression(random_state=42, solver='lbfgs')\n","nb_clf = GaussianNB()\n","svm_clf = SVC(random_state=42, gamma='auto', probability=True)\n","\n","voting_clf = VotingClassifier(\n","    estimators=[('lr', log_clf), ('nb', nb_clf), ('svc', svm_clf)],\n","    voting='soft',\n","    weights=[0.25, 0.25, 0.50])\n","voting_clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDoBWlSSlFp7"},"source":["from sklearn.metrics import accuracy_score\n","\n","for clf in (log_clf, nb_clf, svm_clf, voting_clf):\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DKDtm8w3lFqA"},"source":["# Bagging ensembles\n","\n","El funcionamiento de estos métodos consiste en entrenar varias instancias de un mismo modelo de aprendizaje computacional, pero usando un subconjunto distinto de muestras seleccionadas aleatoriamente para cada entrenamiento.\n","\n","En `sklearn` tenemos el meta-clasificador [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier). Destacamos algunos de sus parámetros:\n","\n","- `n_estimators`: permite definir el número de muestreos a ejecutar sobre el clasificador especificado.\n","- `max_samples`: porcentaje (o número) de muestras a utilizar en cada muestreo.\n","- `max_features`: porcentaje (o número`) de features a utilizar en cada muestreo."]},{"cell_type":"code","metadata":{"id":"UH57EBhQlFqB"},"source":["from sklearn.ensemble import BaggingClassifier\n","\n","bag_clf = BaggingClassifier(\n","    base_estimator=SVC(random_state=42, gamma='auto', probability=True), \n","    n_estimators=20,\n","    max_samples=150, \n","    max_features=2,\n","    n_jobs=-1, \n","    random_state=42)\n","\n","bag_clf.fit(X_train, y_train)\n","\n","y_pred = bag_clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fTw-PMZlFqF"},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HT7hVoolFqo"},"source":["La clase `BaggingClassifier` dispone de un atributo `oob_score_` (si se especifica el parámetro `oob_score=True`) que permite utilizar este meta-clasificador cuando se tiene un conjunto de datos pequeños y no queremos sacar una parte de él como conjunto de *test*. OOB son las siglas de *Out-of-bag* y representa una métrica que utiliza las muestras que no se han utilizado en el entrenamiento de cada uno de los árboles de decisión del ensemble como conjunto de test."]},{"cell_type":"code","metadata":{"id":"9Py4rG4TlFqp"},"source":["bag_clf = BaggingClassifier(\n","    SVC(random_state=42, gamma='auto', probability=True), \n","    n_estimators=20,\n","    max_samples=150, \n","    max_features=2,\n","    n_jobs=-1,\n","    oob_score=True, \n","    random_state=42)\n","\n","bag_clf.fit(X_train, y_train)\n","\n","bag_clf.oob_score_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxszxPAxjMXk"},"source":["## AdaBoost\n","\n","El principio básico de AdaBoost es ajustar una secuencia de aprendizajes débiles (es decir, modelos que sólo son ligeramente mejores que una clasificación aleatoria) usando versiones modificadas de los datos. Las predicciones de todos ellos se combinan finalmente mediante una votación por mayoría ponderada (o suma) para producir la predicción final. \n","\n","Las modificaciones de los datos en cada una de las iteraciones consisten en aplicar pesos $w_1, w_2, \\dots, w_N$ a cada una de las $N$ muestras de entrenamiento. Inicialmente, estos pesos se fijan a $w_i=\\frac{1}{N}$, de tal modo que en la primera iteración se produce un entrenamiento \"débil\" de los datos originales. En cada iteración sucesiva, los pesos de la muestra se modifican individualmente y el algoritmo de aprendizaje se vuelve a aplicar a los datos re-ponderados. En un paso determinado, los ejemplos de entrenamiento que fueron incorrectamente predichos por el modelod el paso anterior aumentan sus pesos, mientras que los pesos se reducen para los que fueron predichos correctamente. A medida que avanzan las iteraciones, los ejemplos difíciles de predecir reciben una influencia cada vez mayor, logrando, de este modo, que los clasificadores se concentren en las muestras que no han sido correctamente clasificadas.\n","\n","`sklearn` nos proporciona la clase [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)."]},{"cell_type":"code","metadata":{"id":"YUBib-silFq8"},"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","ada_clf = AdaBoostClassifier(\n","    base_estimator=DecisionTreeClassifier(max_depth=1), \n","    n_estimators=100, \n","    random_state=42)\n","\n","ada_clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"217DIDqblAbI"},"source":["from matplotlib.colors import ListedColormap\n","\n","def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n","    x1s = np.linspace(axes[0], axes[1], 100)\n","    x2s = np.linspace(axes[2], axes[3], 100)\n","    x1, x2 = np.meshgrid(x1s, x2s)\n","    X_new = np.c_[x1.ravel(), x2.ravel()]\n","    y_pred = clf.predict(X_new).reshape(x1.shape)\n","    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n","    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n","    if contour:\n","        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n","        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n","    plt.axis(axes)\n","    plt.xlabel(r\"$x_1$\", fontsize=18)\n","    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHLnW-o4lFq-"},"source":["plot_decision_boundary(ada_clf, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqczAnNamFnZ"},"source":["## Stacked generalization\n","\n","*Stacked generalization* es un método para combinar los estimadores para reducir sus sesgos. Concretamente, las predicciones de cada estimador individual se apilan y se usan como entrada a un estimador final para calcular la predicción. Este estimador final es entrenado a través de la validación cruzada.\n","\n","El estimador final generalmente se alimenta únicamente de la salida de los clasificadores previos:\n","\n","![Stacked generalization sin features](https://wolpert.readthedocs.io/en/latest/_images/stack_example_001.png)\n","\n","No obstante, también es posible añadir a dicho estimador las muestras del conjunto de datos:\n","\n","![Stacked generalization con features](https://wolpert.readthedocs.io/en/latest/_images/restack_graph.png)\n","\n","En `sklearn` tenemos este clasificador implementado en la clase [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier). Los estimadores a combinar se deben definir en el parámetro `estimators` mientras que el estimador final se define en el parametro `final_estimator`. El parámetro `passthrough` permite definir si el estimador final se entrena sólo con la salida de los estimadores (`passthrough=False`, valor por defecto) o se usan también las muestras de entrada (`passthrough=True`).\n"]},{"cell_type":"code","metadata":{"id":"boNGfuAboeBA"},"source":["from sklearn.ensemble import StackingClassifier\n","\n","log_clf = LogisticRegression(random_state=42, solver='lbfgs')\n","nb_clf = GaussianNB()\n","svm_clf = SVC(random_state=42, gamma='auto', probability=True)\n","\n","stack_clf = StackingClassifier(\n","    estimators=[('lr', log_clf), ('nb', nb_clf), ('svc', svm_clf)],\n","    final_estimator=LogisticRegression(),\n","    passthrough=False)\n","\n","stack_clf.fit(X_train, y_train)\n","\n","y_pred = stack_clf.predict(X_test)\n","\n","accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdXM04btpG52"},"source":["log_clf = LogisticRegression(random_state=42, solver='lbfgs')\n","nb_clf = GaussianNB()\n","svm_clf = SVC(random_state=42, gamma='auto', probability=True)\n","\n","stack_clf = StackingClassifier(\n","    estimators=[('lr', log_clf), ('nb', nb_clf), ('svc', svm_clf)],\n","    final_estimator=LogisticRegression(),\n","    passthrough=True)\n","\n","stack_clf.fit(X_train, y_train)\n","\n","y_pred = stack_clf.predict(X_test)\n","\n","accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrEOVl7f8Gly"},"source":["---\n","\n","Creado por **Raúl Lara** (raul.lara@upm.es) y **Fernando Ortega** (fernando.ortega@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]}]}