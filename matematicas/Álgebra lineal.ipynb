{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Álgebra lineal.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"XTGY5c4Bijnj","colab_type":"text"},"source":["## Espacios vectoriales\n","\n","**Definición.** Un espacio vectorial es un conjunto $V$ con dos operaciones, denotadas $+$ y $\\cdot$.\n","* La operación $+$, llamada suma, toma dos vectores $v,w \\in V$ y devuelve un vector $v+w$.\n","* La operación $\\cdot$, llamada producto por escalares, toma un número $\\lambda \\in \\mathbb{R}$ y un vector $v$ y devuelve un vector $\\lambda \\cdot v \\in V$.\n","\n","**Ejemplos**\n","* $\\mathbb{R}^n$ con la suma y producto componente a componente.\n","* Soluciones de sistemas de ecuaciones.\n","\n","**Observación.** También se pueden considerar espacios vectoriales sobre $\\mathbb{C}$ o, en general, sobre cualquier cuerpo.\n","\n","Consideremos el sistema\n","$$\n","\\left\\{\\begin{matrix}x+y=0 \\\\ 2x+2y=0\\end{matrix}\\right.\n","$$\n","\n","Este sistema tiene por soluciones el conjunto $S$ de los puntos $(x,y) \\in \\mathbb{R}^2$ de la forma $(x,y)=(a, -a)$. Este conjunto es un espacio vectorial: Si yo cojo dos vectores $v = (a, -a)$ y $w = (b, -b)$ entonces $v+w=(a+b,-(a+b)) \\in S$ y $\\lambda v=(\\lambda a, -\\lambda a) \\in S$.\n","\n","De hecho, se tiene que $S$ es un subconjunto del espacio vectorial $\\mathbb{R}^2$ y hereda sus operaciones. Este fenómeno se llama *subespacio vectorial*.\n","\n","**Ejercicios.**\n","* El espacio de polinomios, $\\mathbb{R}[x]$, con la suma de polinomios y el producto por números es un espacio vectorial.\n","* El espacio de polinomios de grado menor o igual que $n$, $\\mathbb{R}^{\\leq n}[x]$, es un espacio vectorial."]},{"cell_type":"markdown","metadata":{"id":"WyTJMRxeijnn","colab_type":"text"},"source":["**Definición** Sea $V$ un espacio vectorial. Dado un subconjunto finito $\\left\\{b_1, \\ldots, b_d\\right\\} \\subseteq V$, se dice que es\n","* *Sistema de generadores* si, para todo $v \\in V$, existen $\\lambda_1, \\ldots, \\lambda_d \\in \\mathbb{R}$ tales que\n","$$\n","    v = \\lambda_1 b_1 + \\ldots + \\lambda_d b_d.\n","$$\n","* *Linealmente independiente* si, para cualesquiera $\\lambda_1, \\ldots, \\lambda_d \\in \\mathbb{R}$ tales que\n","$$\n","    \\lambda_1 b_1 + \\ldots + \\lambda_d b_d = 0 \\Rightarrow \\lambda_1 = \\lambda_2 = \\ldots = \\lambda_d = 0.\n","$$\n","* *Base* si es un sistema de generadores linealmente independiente. En ese caso se dice que $d$ es la dimensión de $V$ y se escribe $d = \\mathrm{dim}\\,V$.\n","\n","**Ejemplos**\n","* $e_1 = (1,0,\\ldots), \\ldots, e_n = (0,\\ldots, 1)$ es base de $\\mathbb{R}^n$. Luego $\\mathrm{dim}\\,\\mathbb{R}^n =n$.\n","* Dar una base y calcular la dimensión del espacio de polinomios de grado menor que $n$. ¿Y de todo el espacio de polinomios?\n","* Dar una base de $S$ del ejemplo anterior."]},{"cell_type":"code","metadata":{"id":"LyU1ie3lijnq","colab_type":"code","colab":{}},"source":["from sympy import *\n","\n","x, y = symbols(\"x, y\")\n","print(solve([x+y, 2*x+2*y], x, y))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7aen0D0Lijn1","colab_type":"code","colab":{}},"source":["# Otra manera de resolverlo\n","\n","A = Matrix([[1,1],[2,2]])\n","print(A)\n","\n","linsolve((A, Matrix([0,0])), x, y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VpZZCJRFijn_","colab_type":"text"},"source":["* Dar una base del espacio de soluciones del sistema\n","\n","$$\n","\\left\\{\\begin{matrix}\n"," x   + 2y   + 3z & = 0\\\\\n","4x  +5y  +6z& = 0\\\\\n"," 7x   +8y   +9z& = 0\\\\\n","\\end{matrix}\\right.\n","$$"]},{"cell_type":"code","metadata":{"id":"wjFFebV1ijoA","colab_type":"code","colab":{}},"source":["x, y, z = symbols(\"x, y, z\")\n","print(solve([x + 2*y + 3*z, 4*x + 5*y + 6*z, 7*x + 8*y +9*z], x, y,z))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CScVk6AAijoJ","colab_type":"text"},"source":["* Dar una base del espacio de soluciones del sistema\n","\n","$$\n","\\left\\{\\begin{matrix}\n"," x_1   +2x_2   +2x_3  -5x_4   +6x_5 & = 0\\\\\n","-x_1  -2x_2  -x_3   +x_4  -x_5& = 0\\\\\n"," 4x_1   +8x_2   +5x_3  -8x_4   +9x_5& = 0\\\\\n"," 3x_1   +6x_2   +x_3   +5x_4  -7x_5& = 0\n","\\end{matrix}\\right.\n","$$"]},{"cell_type":"code","metadata":{"id":"QeI8r-bQijoK","colab_type":"code","colab":{}},"source":["x1, x2, x3, x4, x5 = symbols(\"x1, x2, x3, x4, x5\")\n","\n","A = Matrix([[1 ,  2 ,  2 , -5 ,  6], [-1, -2, -1,  1, -1], [4,  8,  5, -8,  9],[3,  6, 1,  5, -7]])\n","print(A)\n","\n","linsolve((A, Matrix([0,0,0,0])), x1, x2, x3, x4, x5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ECnIjLRkijoQ","colab_type":"text"},"source":["* **Modelo de Leontief.** Apple, Intel y Nvidia son tres empresas interrelacionadas, en el sentido de que cada una de ellas genera un bien específico (digamos Mac's, CPUs y tarjetas gráficas, respectivamente) que las demás requieren para continuar su proporción. Supongamos que las dependencias son como sigue: Cada unidad de Mac requiere 0.3 unidades de otros Mac's, 0.4 unidades CPUs y 0.3 unidades de tarjetas gráficas. Cada CPU producida por Intel necesita 0.3 unidades de Mac's, 0.2 CPUs y 0.5 tarjetas gráficas. Finalmente, cada tarjeta gráfica necesita 0.4 Mac's, 0.4 CPUs y 0.2 tarjetas gráficas para ser producidas. Determinar los niveles de proporción necesarios para que la economía se encuentre en equilibrio."]},{"cell_type":"code","metadata":{"id":"ETaSbqWiijoR","colab_type":"code","colab":{}},"source":["A = Matrix([[0.3-1,  0.4,  0.3], [0.3, 0.2-1, 0.5], [0.4,  0.4,  0.2-1]])\n","linsolve((A, Matrix([0,0,0])), x1, x2, x3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scfIKVLeijoY","colab_type":"text"},"source":["# Aplicaciones lineales\n","\n","**Definición** Sean $V, W$ dos espacio vectoriales. Una función $f: V \\to W$ se dice una *aplicación lineal* si\n","* $f(v+w)=f(v) + f(w)$.\n","* $f(\\lambda v) = \\lambda f(v)$.\n","\n","**Ejemplos **\n","* $f: \\mathbb{R} \\to \\mathbb{R}$ dada por $f(x)=5x$.\n","* $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$ dada por $f(x,y) = (3x+5y, x-y, x-2y)$.\n","* $f: \\mathbb{R} \\to \\mathbb{R}$ dada por $f(x)=x^2$ **no es** aplicación lineal.\n","* $f: \\mathbb{R}[x] \\to \\mathbb{R}$, $f(P)=P(5)$.\n","\n","**Definición** Si $f: V \\to W$ es una aplicación lineal, definimos:\n","* El *kernel* (o *núcleo*) es\n","$$\n","    \\mathrm{Ker}\\,f = \\left\\{v \\in V\\,|\\, f(v)=0\\right\\}.\n","$$\n","* La *imagen* es\n","$$\n","    \\mathrm{Im}\\,f = f(V) \\subseteq W.\n","$$\n","\n","La dimensión de la imagen se suele llamar el *rango* de $f$ y se denota $\\mathrm{rk}\\,f = \\mathrm{dim}\\,\\mathrm{Im}\\,f$.\n","\n","**Ejercicios**\n","* $\\mathrm{Ker}\\,f \\subseteq V$ es un subespacio vectorial.\n","* Calcular el kernel, la imagen y el rango de todas las aplicaciones anteriores.\n","\n","**Proposición (Principio de inclusión-exclusión).** Para toda aplicación lineal $f: V \\to W$, se tiene\n","\n","$$\n","    \\mathrm{dim}\\,\\mathrm{Ker}\\,f + \\mathrm{dim}\\,\\mathrm{Im}\\,f = \\mathrm{dim}\\,V\n","$$\n","\n","**Observación clave:** Una aplicación lineal $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ se puede describir como una matrix. Por ejemplo, la matrix de $f(x,y) = (3x+5y, x-y, x-2y)$ es\n","$$\n","    \\begin{pmatrix}\n","    3 & 5 \\\\\n","    1 & -1 \\\\\n","    1 & -2 \\\\\n","    \\end{pmatrix}\n","$$\n","de manera que\n","$$\n","f(x,y) = \\begin{pmatrix}\n","    3 & 5 \\\\\n","    1 & -1 \\\\\n","    1 & -2 \\\\\n","    \\end{pmatrix}\\begin{pmatrix}\n","    x \\\\\n","    y \\\\\n","    \\end{pmatrix}\n","$$"]},{"cell_type":"code","metadata":{"id":"aS4jfpCLijoc","colab_type":"code","colab":{}},"source":["A = Matrix([[3,5],[1,-1],[1,-2]])\n","A"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d0DVBsBxijok","colab_type":"code","colab":{}},"source":["# Se puede calcular el kernel\n","print(A.nullspace())\n","\n","# Y la imagen\n","print(A.columnspace())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gYcO3ZFfijoq","colab_type":"text"},"source":["**Ejercicio.** Calcular bases de la imagen y del kernel de las aplicaciones lineales dadas por las siguientes matrices:\n","\n","$$\n","A= \\begin{pmatrix}\n","1 & -1 & 0 \\\\ 0 & 1 & 2 \\\\\n","\\end{pmatrix} \\hspace{1cm}\n","A= \\begin{pmatrix}\n","1 &0 &-1 \\\\ 2 &-4 &1 \\\\ -1 & 0 &-1 \\\\ 2 & -2 &-2\\\\\n","\\end{pmatrix} \\hspace{1cm}\n","A= \\begin{pmatrix}\n","6 &-4 & 16 \\\\\n","-2 &-5 & 1 \\\\\n","1 &14 &-12 \\\\\n","\\end{pmatrix}\n","$$\n","\n","## Comprobación de Kernel nulo\n","\n","Sea $f: V \\to V$ una aplicación lineal y sea $A$ su matriz. ¿Cómo podemos comprobar si $\\mathrm{Ker}\\,f = 0$?\n","\n","$$\n","\\mathrm{Ker}\\,f = 0 \\Leftrightarrow Ax=0 \\textrm{ solo tiene la solución trivial} \\Leftrightarrow A \\textrm{ es invertible } \\Leftrightarrow \\det{A}\\neq 0.\n","$$\n","\n","**Proposición.** Sea $f: V \\to V$ una aplicación linea con matriz $A$. Entonces $\\mathrm{Ker}\\,f = 0$ si y solo si $\\det{A} \\neq 0$. \n","\n","*Demostración.* Sea $A$ la matriz de $f$. Entonces el kernel de $f$ está dado por los vectores $x = (x_1, \\ldots, x_n)$ tales que \n","\n","$$\n","    Ax = 0\n","$$\n","\n","Este sistema tiene la solución obvia $x = (0, \\ldots, 0)$. Ahora bien, esta será la única solución si y solo si $\\det{A} \\neq 0$, que es exactamente la condición buscada.\n","\n","**Observación.** Si $f: V \\to V$ tiene kernel trivial, entonces $ \\mathrm{dim}\\,V= \\mathrm{dim}\\,\\mathrm{Ker}\\,f + \\mathrm{dim}\\,\\mathrm{Im}\\,f=\\mathrm{dim}\\,\\mathrm{Im}\\,f$ y, por tanto $f(V)=V$ y $f$ es sobreyectiva."]},{"cell_type":"markdown","metadata":{"id":"DqTMneaEijos","colab_type":"text"},"source":["# Productos escalares\n","\n","\n","**Definición.** Sea $B: V \\times V \\to \\mathbb{R}$. Se dice que $B$ es un *producto escalar* si\n","* $B$ es una aplicación bilineal: $B(-,v), B(v,-): V \\to \\mathbb{R}$ son lineales para todo $v \\in B$\n","* $B$ es definido positivo: $B(v,v)\\geq 0$ para todo $v \\in V$ y $B(v,v)=0$ si y solo si $v=0$.\n","* $B$ es simétrico: $B(v,w)=B(w,v)$ para todo $v,w \\in V$.\n","En ese caso $(V,B)$ se llama un *espacio euclídeo* o *espacio de Hilbert*. Dado $v \\in V$, el número $||v||=\\sqrt{B(v,v)}$ se llama la *norma* de $v$.\n","\n","\n","**Observación.** El producto $B(v,w)$ se suele denotar por $\\langle v,w \\rangle$ o simplemente por $v \\cdot w$.\n","\n","**Ejemplos**\n","* $\\mathbb{R}^n$ con el producto escalar usual. Intrepretación geométrica: $v \\cdot w = ||v||\\,||w||\\cos{\\angle (u,v)}$.\n","* Espacio de polinomios con (Producto $L^2$)\n","$$\n","    \\langle P, Q\\rangle = \\int_{-1}^1 P(t)Q(t)\\,dt.\n","$$\n","\n","**Definición.** Una base $v_1, \\ldots, v_d$ de un espacio euclídeo $V$ se dice *ortonormal* si\n","* $\\langle v_i, v_j\\rangle = 0$ si $i \\neq j$.\n","* $||v_i||=1$ para todo $i$.\n","\n","**Ejemplo.** La base estándar de $\\mathbb{R}^n$ es ortonormal.\n","\n","**Proposición (Desigualdad de Cauchy-Schwarz).** En un espacio euclídeo se tiene que, para todos $v,w \\in V$\n","\n","$$\n","    \\langle v,w \\rangle \\leq ||u||\\,\\,||w||\n","$$\n","\n","Además, se tiene la igualdad si y solo si $v$ y $w$ son paralelos.\n","\n","**Corolario (Desigualdad triangular).** Para todos $v,w \\in V$ se tiene\n","\n","$$\n","    ||v+ w || \\leq ||v|| + ||w||.\n","$$\n","\n","*Demostración.* Expandiendo la normal y usando la desigualdad de Cauchy-Schwarz tenemos\n","\n","$$\n","    ||v + w||^2 = \\langle v + w, v + w\\rangle = ||v||^2 + ||w||^2 +2 \\langle v, w\\rangle \\leq ||v||^2 + ||w||^2 +2||v||\\, ||w|| = (||v|| + ||w||)^2.\n","$$\n","\n","Tomando la raiz cuadrada, se obtiene el resultado.\n","\n","## Proceso de Gram-Schmidt\n","\n","Supongamos que tenemos una base $v_1, \\ldots, v_n$ de un espacio euclídeo $V$ y queremos convertirla en una base ortogonal $v_1', \\ldots, v_n'$. Para el primer vector es muy fácil, porque podemos tomar\n","\n","$$\n","    v_1' = v_1\n","$$\n","\n","Siguiendo el proceso recursivamente, supongamos que hemos conseguido ortonormalizar $v_1', \\ldots, v_k'$. Entonces, si tomamos \n","\n","$$\n","    v_{k+1}' = v_{k+1} - \\sum_{i=1}^k \\frac{\\langle v_{k+1}, v_i'\\rangle}{||v_i'||^2} v_i',\n","$$\n","\n","tenemos que, para todo $v_i'$ con $1 \\leq i \\leq k$,\n","\n","$$\n","    \\langle v_{k+1}', v_i'\\rangle = \\langle v_{k+1}, v_i'\\rangle - \\frac{\\langle v_{k+1}, v_i'\\rangle}{||v_i'||^2} \\langle v_i',v_i'\\rangle = 0.\n","$$\n","\n","De este modo, obtenemos una base ortogonal $v_1', \\ldots, v_n'$. Si queremos normalizarla para que sea, de hecho, ortogonal, podemo tomar $v_i'' = \\frac{v_i'}{||v_i'||}$.\n","\n","**Ejercicios.**\n","* Aplicar el proceso de Gram-Schmidt a la base de $\\mathbb{R}^3$, $(1,-2,2), (0,0,1), (-1,0,1)$."]},{"cell_type":"code","metadata":{"id":"y3VPVdX8ijot","colab_type":"code","colab":{}},"source":["def GramSchmidt(base_original, B, base_ortogonal=[]):\n","    if len(base_ortogonal) == 0:\n","        base_ortogonal.append(base_original.pop(0))\n","        return GramSchmidt(base_original, B, base_ortogonal)\n","    \n","    if len(base_original) == 0:\n","        return [v/sqrt(B(v,v)) for v in base_ortogonal]\n","\n","    v_k = base_original.pop(0)\n","    \n","    v = base_ortogonal[0]\n","    residuo = v*B(v_k, v)/(B(v,v))\n","    for v in base_ortogonal[1:]:\n","        residuo = residuo + v*B(v_k, v)/(B(v,v))\n","    \n","    v_kp = v_k - residuo\n","    base_ortogonal.append(v_kp)\n","    \n","    \n","    return GramSchmidt(base_original, B, base_ortogonal)\n","\n","def B_euclideo(v, w):\n","    return sum([v[i]*w[i] for i in range(len(v))])\n","\n","base_original = [Matrix([1,-2,2]), Matrix([0,0,1]), Matrix([-1,0,1])]\n","print(GramSchmidt(base_original, B_euclideo))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMOJeJwpijoy","colab_type":"text"},"source":["* Aplicar el proceso de Gram-Schimidt a la base $1, t, t^2$ del espacio de polinomios de grado menor o igual que $2$. Esta base se llama polinomios de Legendre."]},{"cell_type":"code","metadata":{"id":"EQxlSR9Jijoy","colab_type":"code","colab":{}},"source":["x = symbols(\"x\")\n","\n","def B_polinomios(P, Q):\n","    return integrate(P*Q, (x, -1, 1))\n","\n","base_original_pols = [1+0*x, x, x**2]\n","print(base_original_pols)\n","\n","print(GramSchmidt(base_original_pols, B_polinomios, base_ortogonal=[]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"145zBC9mijo5","colab_type":"text"},"source":["# Aplicaciones ortogonales\n","\n","**Definición.** Una aplicación lineal $f: V \\to W$ entre espacios euclídeos se dice *ortogonal* si\n","$$\n","    \\langle f(v), f(w)\\rangle = \\langle v, w\\rangle\n","$$\n","para todo $v,w\\in V$.\n","\n","**Observación.**\n","* Si $f$ es ortogonal, entonces $||f(v)|| = ||v||$. De este modo, se suele decir que una aplicación ortogonal es aquella que preserva módulos y ángulos.\n","* $f$ es ortogonal si y solo si manda bases ortonormales en bases ortonomales.\n","* En particular, eso implica que, si $A$ es la matriz de $f$, entonces las columnas de $A$ forman una base ortonormal. De este modo $f$ es ortogonal si y solo si $AA^t=\\mathrm{I}$.\n","\n","**Ejercicio.** Comprobar si las siguientes aplicaciones lineales son ortogonales\n","\n","$$\n","A= \\begin{pmatrix}\n","1 & -1 & 0 \\\\ 0 & 1 & 2 \\\\ 2 & 3 & 1 \\\\\n","\\end{pmatrix} \\hspace{1cm}\n","A= \\begin{pmatrix}\n","1 &0 &-1 \\\\ 2 &-4 &1 \\\\ -1 & 0 &-1 \\\\\n","\\end{pmatrix} \\hspace{1cm}\n","A= \\begin{pmatrix}\n","0 & 0 & 1 \\\\\n","1 & 0 & 0 \\\\\n","0 & 1 & 0 \\\\\n","\\end{pmatrix}\n","$$"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ygj4VR8Zijo8","colab_type":"text"},"source":["# Autovalores, autovectores y diagonalización\n","\n","**Definición.** Sea $f: V \\to W$ una aplicación lineal. Se dice que $v \\in V$, $v \\neq 0$, es un *autovector* de $f$ con autovalor $\\lambda \\in k$ si $f(v)=\\lambda v$.\n","\n","**Forma de cálculo.** Sea $A$ la matriz de la aplicación $f$.\n","\n","$$\n","    v \\textrm{ autovector de autovalor } \\lambda \\Leftrightarrow Av = \\lambda v \\Leftrightarrow Av-\\lambda v=0 \\Leftrightarrow (A-\\lambda \\mathrm{I})v=0 \\Leftrightarrow v \\in \\mathrm{Ker}\\left(A-\\lambda \\mathrm{I}\\right)\n","$$\n","\n","De este modo, $A$ tiene autovectores no nulos de autovalor $\\lambda$ si y solo si $\\mathrm{det}\\left(A-\\lambda \\mathrm{I}\\right)=0$. Esto proporciona una receta para calcular autovalores y autovectores:\n","* Para calcular los autovalores: se forma el polinomio $p(\\lambda)=\\mathrm{det}\\left(A-\\lambda \\mathrm{I}\\right)$, llamado polinomio característico. Los ceros de $p(\\lambda)$ son los autovalores.\n","* Para calcular los autovectores de un autovalor: Una base de autovectores de autovalor $\\lambda$ está dada por una base de $\\mathrm{Ker}\\left(A-\\lambda \\mathrm{I}\\right)$\n","\n","** Definición. ** Una matriz $A$ se dice *diagonalizable* si existe una base de autovectores de $A$.\n","\n","** Ejemplo. ** Consideremos la matriz\n","$$\n","    A = \\begin{pmatrix}\n","6 &-4 & 16 \\\\\n","-2 &-5 & 1 \\\\\n","1 &14 &-12 \\\\\n","\\end{pmatrix}\n","$$\n","Determinar los autovalores de $A$, así como una base de autovectores para cada uno de ellos. ¿Es $A$ diagonalizable?"]},{"cell_type":"code","metadata":{"id":"UHmuNqabijo8","colab_type":"code","colab":{}},"source":["from sympy.solvers import solve\n","\n","A = Matrix([[6 ,-4 , 16],[-2 ,-5 , 1],[1 ,14 ,-12]])\n","\n","# Se forma la matrix A -lI\n","l = symbols(\"l\")\n","I = eye(3)\n","print(A-l*I)\n","\n","# Se forma el polinomio característico\n","p = (A-l*I).det()\n","print(p)\n","\n","# Se resuelve el polinomio característico\n","print(solve(p, l))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"omv3jDbcijpB","colab_type":"code","colab":{}},"source":["print((A-(-16)*I).nullspace())\n","print((A-(0)*I).nullspace())\n","print((A-(5)*I).nullspace())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDzW--ptijpJ","colab_type":"code","colab":{}},"source":["# Otra forma de hacerlo usando las bibliotecas de Python\n","\n","print(A.eigenvals())\n","print(A.eigenvects())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8nP1jZdWijpN","colab_type":"text"},"source":["Supongamos que $A$ es una matriz diagonalizable con una base de autovectores $v_1, \\ldots, v_n$, con autovalores $\\lambda_1, \\ldots, \\lambda_n$ respectivamente. Obsérvese que, en la base $v_1, \\ldots, v_n$, la matriz de $A$ es la matriz diagonal\n","$$\n","D = \\begin{pmatrix}\n","\\lambda_1 & 0 & \\ldots & & \\vdots\\\\\n","0 & \\lambda_2 & & \\\\\n","\\vdots & \\ddots & & \\\\\n","0 & & \\ldots & & \\lambda_n\\\\\n","\\end{pmatrix}\n","$$\n","De este modo, si consideramos la matriz\n","$$\n","    P = \\left(v_1 \\left|\\, v_2 \\left|\\, \\ldots \\left|\\, v_n\\right.\\right.\\right.\\right),\n","$$\n","entonces se tiene\n","$$\n","A = PDP^{-1}.\n","$$\n","Es decir *$A$ es diagonal, pero en la base equivocada*.\n","\n","*Demostración.*\n","La clave es que $Pe_i = v_i$ por construcción. Entonces $PDP^{-1}v_i=PDe_i=P\\lambda_i e_i = \\lambda_iPe_i=\\lambda_i v_i$. De este modo, $PDP^{-1}v_i = Av_i$ para todo $v_i$ y, como los $v_i$ son una base, se tiene que $A=PDP^{-1}$.\n","\n","**Ejercicio.** Dar las matrices $P$ y $D$ de la diagonalización de la matriz del ejercicio anterior."]},{"cell_type":"code","metadata":{"id":"n40cR0gSijpO","colab_type":"code","colab":{}},"source":["# Formamos la matriz de los autovectores obtenidos\n","\n","P = Matrix([[-10/13,-3/13,1], [-2,1, 1], [-26/3, 11/6,1]])\n","P = P.T\n","print(P)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zINElxO4ijpT","colab_type":"code","colab":{}},"source":["# Formamos la matriz diagonal con los autovalores\n","\n","D = Matrix([[-16,0,0], [0,0, 0], [0, 0, 5]])\n","print(D)\n","\n","print(P*D*P.inv())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jjqm3bJUijpW","colab_type":"text"},"source":["**Ejercicio.** Supongamos que Apple, Intel y Asus vuelven a estar en la situación del modelo de Leontief. Supongamos que, inicialmente, tenemos 3 Mac's, 5 CPU y 2 tarjetas gráficas. Comprobar cuánto stock tenemos tras 5 iteraciones de producción. ¿Y tras 100 iteraciones?"]},{"cell_type":"code","metadata":{"id":"cIv3Y6NpijpY","colab_type":"code","colab":{}},"source":["A = Matrix([[0.3,  0.4,  0.3], [0.3, 0.2, 0.5], [0.4,  0.4,  0.2]])\n","P, J = A.jordan_form(calc_transform=True) # Otra forma de calcular la diagonalizacion"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bsT_56T6ijpd","colab_type":"code","colab":{}},"source":["# Si A = PDP^(-1) entonces A^s = PD^(s)P^(-1)\n","\n","A5 = P*J**(5)*P.inv()\n","print(A5)\n","print(A5*Matrix([[3],[5],[2]]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImLBqzSHijpi","colab_type":"text"},"source":["**Teorema (espectral).**\n","* Si $A$ es una aplicación ortogonal, entonces es diagonalizable.\n","* Si $A$ es una matriz simétrica (i.e. $A=A^t$) entonces $A$ es diagonalizable."]},{"cell_type":"markdown","metadata":{"id":"xBqYmhSbijpj","colab_type":"text"},"source":["# Singular Value Decomposition\n","\n","Sea $A$ una matriz. Si $A$ no es cuadrada, no podemos diagonalizar $A$. Sin embargo, existe una 'versión débil' de diagonalización que será muy útil para nuestros propósitos.\n","\n","**Teorema (Descomposición en Valores Singulares, SVD).** Sea $A$ una matriz de tamaño $n \\times m$. Existen aplicaciones ortogonales $U$ de orden $n$ y $V$ de orden $m$ tales que\n","\n","$$\n","A = U\\Sigma V^t.\n","$$\n","\n","Aquí, $\\Sigma = \\textrm{diag}\\left(\\sigma_1, \\ldots, \\sigma_k\\right)$ con $k = \\min(n,m)$ y $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_k \\geq 0$, llamados los *valores singulares*.\n","\n","Para ser precisos, obsérvese que $\\Sigma$ es de la forma\n","$$\n","\\Sigma=\\begin{pmatrix}\n","\\sigma_1 & 0 & \\ldots & & & 0 & \\ldots & 0 \\\\\n","0 & \\sigma_2 & & & & \\vdots& &\\vdots\\\\\n","\\vdots & \\ddots & & \\\\\n","0 & & \\ldots & & \\sigma_n & 0 & \\ldots & 0\\\\\n","\\end{pmatrix} \\;\\;\\;\\;\\; \\textrm{ si }n\\leq m\n","$$\n","$$\n","\\Sigma=\\begin{pmatrix}\n","\\sigma_1 & 0 & \\ldots & \\\\\n","0 & \\sigma_2 & & & \\\\\n","\\vdots & \\ddots & & \\\\\n","0 & & \\ldots & & \\sigma_m \\\\\n","0 & & \\ldots & & 0 \\\\\n","\\vdots & & \\ldots & & \\vdots \\\\\n","0 & & \\ldots & & 0 \\\\\n","\\end{pmatrix} \\;\\;\\;\\;\\; \\textrm{ si }n\\geq m\n","$$"]},{"cell_type":"markdown","metadata":{"id":"DCQy3Z9yijpk","colab_type":"text"},"source":["## Detección de componentes principales\n","\n","Una de las utilidades prácticas más importantes de diagonalización y SVD es detectar los vectores 'más importantes' de un conjunto de datos. Para ilustrarlo, consideremos la base de datos MNIST."]},{"cell_type":"code","metadata":{"id":"Wc6WpfTVijpk","colab_type":"code","colab":{}},"source":["from sklearn import datasets, svm, metrics\n","import numpy as np\n","from numpy import linalg as la\n","import matplotlib.pyplot as plt\n","\n","mnist = datasets.load_digits()\n","digits = mnist['data']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"12mMQCsMijpn","colab_type":"code","colab":{}},"source":["def pintar_numero(d):\n","    pixels = d.reshape((8, 8))\n","    plt.imshow(pixels, cmap='gray')\n","    plt.show()\n","\n","pintar_numero(digits[23])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXDTb5c0ijpr","colab_type":"code","colab":{}},"source":["digits_mean = digits.mean(0)\n","\n","pintar_numero(digits_mean)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJwPBMi3ijpv","colab_type":"text"},"source":["### Usando diagonalización\n","\n","Consideremos un conjunto de datos $x_1, x_2, \\ldots, x_N$, de media $\\bar{x}$. La matrix\n","$$\n","    S = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})(x_i - \\bar{x})^t\n","$$\n","se llama *matriz de covarianzas* de los datos. Su entrada $(i,j)$ mide 'como varían' los atributos $i$-ésimo y $j$-ésimo respecto de su media.\n","\n","Los autovectores de $S$ pueden interpretarse como las 'direcciones de variación principal' y, cuanto mayor sea el autovalor asociado, mayor es la variación."]},{"cell_type":"code","metadata":{"id":"ttSSM557ijpw","colab_type":"code","colab":{}},"source":["# Calculo de la matriz de covarianzas\n","\n","cov = zeros(64,64)\n","\n","for d in digits:\n","    cov = cov + Matrix(d - digits_mean)*Matrix(d-digits_mean).T"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoznTI0Yijp0","colab_type":"code","colab":{}},"source":["# Otra manera mas rapida de calcular la\n","# matriz de covarianzas usando numpy\n","\n","digits_cent = [d - digits_mean for d in digits]\n","cov = sum([d.reshape(64,1)*d.reshape(64,1).T for d in digits_cent]).reshape(64,64)\n","\n","eigenvalues, eigenvectors = la.eig(cov)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h11C4Ygvijp5","colab_type":"code","colab":{}},"source":["eigenvalues"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J19WDqSTijp_","colab_type":"code","colab":{}},"source":["eigenvectors[:,0] # Devuelve los autovectores por columnas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFvbngu6ijqC","colab_type":"code","colab":{}},"source":["pintar_numero(eigenvectors[:,0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtyT3qjTijqE","colab_type":"text"},"source":["### Usando SVD\n","\n","Otra forma de realizar el mismo cálculo de forma más efectiva es la siguiente. Sea $X = \\left(x_1 - \\bar{x}\\,|\\, x_2- \\bar{x}\\,|\\,\\ldots\\,|\\, x_N- \\bar{x}\\right)$ la matriz de datos centrados en la media. Obsérvese que, en ese caso, $S = XX^t$.\n","\n","Apliquemos una descomposición SVD a $X$, de forma que\n","\n","$$\n","    X = U\\Sigma V^t\n","$$\n","con $\\Sigma = \\mathrm{diag}(\\sigma_1, \\ldots, \\sigma_N)$ con $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_N$ los valores principales ordenados. Entonces, tenemos que\n","\n","$$\n","    S = XX^t = U\\Sigma V^t V \\Sigma^t U^t = U\\Sigma\\Sigma^t U^t = UD U^t,\n","$$\n","\n","dónde $D = \\mathrm{diag}(\\sigma_1^2, \\ldots, \\sigma_N^2)$. De este modo, los autovalores de $S$ son $\\sigma_1^2, \\ldots, \\sigma_N^2$ y los autovectores vienen dados por $U$."]},{"cell_type":"code","metadata":{"id":"oVj5W-O1ijqG","colab_type":"code","colab":{}},"source":["# Formamos la matriz X\n","\n","X = np.matrix(digits_cent).T\n","\n","U, sigma, V = la.svd(X) # Calculamos la SVD de X"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8sUXwImijqJ","colab_type":"code","colab":{}},"source":["pintar_numero(U[:,2])\n","pintar_numero(eigenvectors[:,2])"],"execution_count":0,"outputs":[]}]}