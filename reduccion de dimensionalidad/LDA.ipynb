{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "LDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjrDf5cdk4Pn"
      },
      "source": [
        "# Latent Dirichlet Allocation\n",
        "\n",
        "Los modelos temáticos agrupan las palabras de un documento en diferentes temas. Por ejemplo, para la frase \n",
        "\n",
        "> A este chico le encanta jugar en el parque y le gusta el helado.\n",
        "\n",
        "se puede decir que consta de dos temas importantes: 1) La comida, debido al \"helado\", y 2) Las actividades, debido al \"juego\" y al \"parque\". \n",
        "\n",
        "**Latent Dirichlet Allocation (LDA)** es un algoritmo de aprendizaje no supervisado utilizado para descubrir diferentes temas y sus indicadores asociados (palabras relacionadas con el tema) en una colección de documentos. **LDA** se basa en la idea de que las palabras a menudo tienen fuertes relaciones semánticas con ciertos temas, por lo que los temas en un documento dado consistirán en un grupo de palabras similares.\n",
        "\n",
        "Al igual que en el algoritmo *K-means*, LDA requiere que elijamos el número de temas para que los descubra, y agrupa las palabras en el corpus de texto (un conjunto de documentos) que frecuentemente ocurren juntos dentro del tema. **LDA** asume que un documento es una mezcla de un conjunto de temas latentes (desconocidos), y cada tema es otra mezcla de palabras (colección de palabras que ayudan a identificar el tema)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyshzrfOmBCg"
      },
      "source": [
        "## El proceso de extracción de temas\n",
        "\n",
        "LDA es conocido por ser un modelo generativo, en el contexto del análisis de texto esto asume que los documentos son generados a través de algún proceso estadístico. \n",
        "\n",
        "Dado un documento $d$ en un corpus $D$ (un conjunto de documentos), entonces $d$ está generado por,\n",
        "\n",
        "1. Número de palabras en el documento $d$, representado por $N_{d}$, que sigue una distribución de Poisson $N_{d} \\sim Poisson(\\eta).$\n",
        "\n",
        "2. La mezcla de temas en el documento $d$, representado por $\\theta_{d},$ sigue una distribución $\\theta_{d} \\sim Dirichlet(\\alpha)$\n",
        "\n",
        "3. Se asigna a cada palabra $w_{i}, i = 1, \\ldots, N_{d},$ un tema, $z_{i},$ de forma que sea consistente con la distribución tema-documento definida en 2. Es decir, $z_{i} \\sim Multinomial(\\theta_{d})$\n",
        "\n",
        "4. Una vez que se conoce el tema $z_{i}$ de cada palabra $w_{i},$ podemos extraer palabras $w_{i}$ de la distribución tema-palabra $\\phi(\\beta)$. Es decir, se elige $w_{i}$ con probabilidad $Pr(w_{i}|z_{i},\\beta)$\n",
        "\n",
        "Para resumir, LDA asume que un documento es una mezcla de temas, donde los temas se extraen de la distribución de temas-documento, y los temas consisten en palabras, donde las palabras se extraen de la distribución de temas-palabras. En la práctica ya tenemos un corpus de texto, un conjunto de documentos. Por lo tanto, normalmente no estamos interesados en generar nuevos documentos, sino más bien en hacer inferencias sobre cómo se genera el documento a partir de diversos temas y palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D0iFRZ5aQQj"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import poisson\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(0, 100, 2)    \n",
        "y = poisson.pmf(x, mu=42.7)\n",
        "\n",
        "plt.plot(x, y, 'bo', ms=8)\n",
        "plt.vlines(x, 0, y, colors='b', lw=1, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYbRBrHrmeYc"
      },
      "source": [
        "## El algoritmo\n",
        "\n",
        "En LDA se supone que la distribución *a priori* sobre los temas de un corpus de texto sigue una distribución de Dirichlet. Esta distribución *a priori* proporciona la idea de que los documentos incluyen una mezcla de temas, y cada tema tiene una distribución sobre palabras que son similares. Hemos asumido un proceso estadístico en el que se generan documentos, por lo que ahora podemos combinar esta suposición con datos reales para encontrar la composición de los temas que aparecen en cada uno de nuestros documentos.\n",
        "\n",
        "El parámetro clave que debemos encontrar es la probabilidad de asignar una palabra de un documento dado a un tema en particular, dadas nuestras suposiciones previas y el vocabulario de nuestro *corpus* (palabras únicas en todos los documentos).\n",
        "\n",
        "De acuerdo a todo esto, el algoritmo para la detección de temas en documentos quedaría de la siguiente forma:\n",
        "\n",
        "1. Se asigna de manera aleatoria uno de los $K$ temas a cada una de las palabras de los documentos. Esta es la asignación inicial palabra-tema, que se irá actualizando durante la ejecución del algoritmo.\n",
        "2. Para cada documento $d$:\n",
        "3. Para cada palabra $w$ del documento $d$ se le asigna un tema de acuerdo a la asignación inicial tema-palabra.\n",
        "4. Ahora, se reasigna cada palabra $w$ del documento $d$ al tema $k$ con probabilidad\n",
        "$$Pr(z_{(d,n)} = k | z_{-(d,n)},W;\\alpha,\\beta) \\propto (n_{d,(.)}^{k,-(d,n)} + \\alpha)\\frac{n_{(.),w}^{k,-(d,n)} + \\beta}{\\sum_{w=1}^{V} n_{(.),w}^{k,-(d,n)} + \\beta V},$$\n",
        "\n",
        "donde $z_{(d,n)}$ es el topic asignado a la palabra *n-esima* del documento $d$, $z_{-(d,n)}$ son los temas asignados al resto de palabras del documento $d$, $V$ es el conjunto de todas las palabras encontradas en el corpus, $n_{d,(.)}^{k,-(d,n)}$ es el número de palabras del documento $d$ asignadas al tema $k$, excluyendo $z_{(d,n)}$ y, de manera análoga, $n_{(.),w}^{k,-(d,n)}$ es el número de documentos en los cuales la palabra $w$ se asigna al tema $k$, excluyendo $z_{(d,n)}.$\n",
        "5. Actualizar la asignación inicial tema-palabra con la nueva asignación obtenida del paso 4.\n",
        "6. Repetir los pasos 2-5 el número de iteraciones deseado.\n",
        "\n",
        "En cada iteración nuestro modelo generativo se hará más preciso y comenzará a captar tendencias en nuestros datos. Una vez que el algoritmo termina, tendremos la distribución de temas-documentos, y también la distribución de temas-trabajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3KQbtltjxcS"
      },
      "source": [
        "\n",
        "## Aplicando Latent Dirichlet Allocation para reducir la dimensionalidad de un Bag of Words\n",
        "\n",
        "\n",
        "En primer lugar, como es usual, importamos las librerías que se van a utilizar, algunos parámetros y funciones auxiliares:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJIoChmntAQR"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "n_samples = 20000\n",
        "n_features = 10000\n",
        "n_components = 20\n",
        "n_top_words = 20\n",
        "\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By9_JHl_tDPL"
      },
      "source": [
        "Cargamos el conjunto de datos de los 20 grupos de noticias y lo vectorizamos. Utilizamos algunas heurísticas para filtrar los términos inútiles desde el principio: los mensajes se despojan de encabezados, pies de página y respuestas citadas, y se eliminan palabras comunes en inglés, palabras que aparecen en un solo documento o en al menos el 95% de los documentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9zTL0NWtlIA"
      },
      "source": [
        "data = fetch_20newsgroups(shuffle=True, random_state=1337,\n",
        "                             remove=('headers', 'footers', 'quotes'))\n",
        "data_samples = data.data[:n_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7gnx7Vztum3"
      },
      "source": [
        "Vamos a usar la frecuencia de las palabras con `CountVectorizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDmBPeRJuMAe"
      },
      "source": [
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(data_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tmT2Et3uZFe"
      },
      "source": [
        "Y por último usamos las frecuencias para lanzar **LDA** y obtener el conjunto de temas que encontramos en los documentos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC_cB26cjxcT"
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=1337)\n",
        "lda.fit(tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7O3BW7Wvlvn"
      },
      "source": [
        "Veamos cuáles son los temas identificados por el algoritmo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlq2RqwgulZV"
      },
      "source": [
        "print(\"Temas identificados:\")\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN-H-o7_vfWr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57HkotEaHd5K"
      },
      "source": [
        "---\n",
        "\n",
        "Creado por **Raúl Lara** (raul.lara@upm.es)\n",
        "\n",
        "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
      ]
    }
  ]
}