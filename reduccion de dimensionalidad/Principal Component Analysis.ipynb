{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Principal Component Analysis.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOO0sS2CKNgPIzqBYGbB6qg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lKK-4KTVfbds"},"source":["# Análisis de Componentes Principales\n","\n","El algoritmo de Análisis de Componente Principales, en inglés *Principal Component Analysis (PCA)* es una técnica de aprendizaje no supervisado (i.e. el conjunto de datos no está etiquetado) que busca reducir la dimensionalidad de un conjunto de datos. Expliquemos su funcionamiento de forma gráfica.\n","\n","Observa los siguientes conjuntos de datos:"]},{"cell_type":"code","metadata":{"id":"1twg7JxmgGd8"},"source":["import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JalstQwLFP_S"},"source":["rng = np.random.RandomState(42)\n","\n","fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n","fig.tight_layout(pad=4.0)\n","\n","X = rng.randn(200, 2)\n","axs[0].scatter(X[:,0], X[:,1])\n","axs[0].set_xlabel('x1')\n","axs[0].set_ylabel('x2')\n","\n","X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n","axs[1].scatter(X[:,0], X[:,1])\n","axs[1].set_xlabel('x1')\n","axs[1].set_ylabel('x2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFQV4Yq8e6jf"},"source":["¿Qué observamos? ¿Son iguales ambos conjuntos de datos? ¿Cuántas características (dimensiones) definen cada uno de los conjuntos de datos?\n","\n","Efectivamente, el conjunto de datos de la izquierda dispone de dos características, `x1` y `x2`, que no presentan ningún tipo de relación entre ellas. Por contra, el conjunto de datos de la derecha, dispone de las mismas características pero, en este caso, existe cierta correlación entre `x1` y `x2`.\n","\n","Este hecho que observamos gráficamente puede demostrarse mediante la extracción de las **componentes principales** de cada conjunto de datos como se observa en las siguientes imágenes:\n","\n"]},{"cell_type":"code","metadata":{"id":"inHRZR5ZfbAp"},"source":["from sklearn.decomposition import PCA\n","\n","rng = np.random.RandomState(42)\n","\n","scale = 2\n","width = 0.03\n","\n","fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n","fig.tight_layout(pad=4.0)\n","\n","# left dataset\n","\n","X = rng.randn(200, 2)\n","pca = PCA(n_components=2, random_state=42).fit(X)\n","\n","axs[0].scatter(X[:,0], X[:,1])\n","axs[0].set_xlabel('x1')\n","axs[0].set_ylabel('x2')\n","\n","origin = pca.mean_\n","\n","pc1 = scale * np.sqrt(pca.explained_variance_)[0] * pca.components_[0] \n","pc2 = scale * np.sqrt(pca.explained_variance_)[1] * pca.components_[1] \n","\n","axs[0].arrow(*origin, *pc1, color='r', width=width)\n","axs[0].arrow(*origin, *pc2, color='g', width=width)\n","\n","# right dataset\n","\n","X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n","pca = PCA(n_components=2, random_state=42).fit(X)\n","\n","axs[1].scatter(X[:,0], X[:,1])\n","axs[1].set_xlabel('x1')\n","axs[1].set_ylabel('x2')\n","\n","origin = pca.mean_\n","\n","pc1 = scale * np.sqrt(pca.explained_variance_)[0] * pca.components_[0] \n","pc2 = scale * np.sqrt(pca.explained_variance_)[1] * pca.components_[1] \n","\n","axs[1].arrow(*origin, *pc1, color='r', width=width)\n","axs[1].arrow(*origin, *pc2, color='g', width=width)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qGE5nbfBi7vF"},"source":["Las componentes principales describen la varianza de los datos: su dirección indica hacia dónde varía la varianza y su magnitud denota la magnitud de dicha varianza. Es por ello que en el conjunto de datos izquierdo de la figura anterior ambas componentes principales tienen diferente sentido (los datos dependen de dos características) y similar magnitud (ambas características son igualmente relevantes para describir los datos), mientras que en el conjunto de datos derecho ambas componentes principales tienen diferente sentido (los datos dependen de dos características) pero diferente magnitud (la componente principal roja contiene mucha más varianza que la verde).\n","\n","El objetivo de PCA es encontrar estas componentes principales y reducir la dimensionalidad del conjunto de datos eliminando aquellas con escasa magnitud. Observemos el resultado de aplicar PCA a los conjuntos de datos anteriores:"]},{"cell_type":"code","metadata":{"id":"OxYXdhE_lOQB"},"source":["from sklearn.decomposition import PCA\n","\n","rng = np.random.RandomState(42)\n","\n","fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n","fig.tight_layout(pad=4.0)\n","\n","# left dataset\n","\n","X = rng.randn(200, 2)\n","X_trans = PCA(n_components=1, random_state=42).fit_transform(X)\n","\n","axs[0].scatter(X[:,0], X[:,1], alpha=0.3, label='original')\n","axs[0].scatter(X_trans[:,0], X_trans[:,0], label='reducido')\n","axs[0].set_xlabel('x1')\n","axs[0].set_ylabel('x2')\n","axs[0].legend()\n","\n","# right dataset\n","\n","X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n","X_trans = PCA(n_components=1, random_state=42).fit_transform(X)\n","\n","axs[1].scatter(X[:,0], X[:,1], alpha=0.3, label='original')\n","axs[1].scatter(X_trans[:,0], X_trans[:,0], label='reducido')\n","axs[1].set_xlabel('x1')\n","axs[1].set_ylabel('x2')\n","axs[1].legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83Il-7unuRrj"},"source":["Como vemos, en ambos casos, la información del conjunto de datos se ha condensado para pasar a una única dimensión. En el conjunto de datos de la izquierda esta compresión es excesiva, puesto que muestras muy distantes en el conjunto original se han acercado en exceso en el conjunto de datos reducido. Sin embargo, en el conjunto de datos de la derecha, la compresión es adecuada, al representar el conjunto de datos reducido prácticamente la misma información que el conjunto de datos original.\n","\n","Como dijimos, la magnitud de las componentes principales denota la cantidad de varianza explicada por cada componente principal. PCA nos permite recuperar el ratio de varianza explicada por cada componente principal (atributo `explained_variance_ratio`) para justificar si debemos o no eliminar una dimensión. Observemos la varianza explicada por cada componente principal del ejemplo anterior:"]},{"cell_type":"code","metadata":{"id":"M-jaRhL6vuuZ"},"source":["from sklearn.decomposition import PCA\n","\n","rng = np.random.RandomState(42)\n","\n","fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n","fig.tight_layout(pad=4.0)\n","\n","# left dataset\n","\n","X = rng.randn(200, 2)\n","pca = PCA(n_components=2, random_state=42).fit(X)\n","\n","axs[0].pie(pca.explained_variance_ratio_, labels=['Componente Principal 1', 'Componente Principal 2'], colors=['r', 'g'], autopct='%1.1f%%')\n","\n","\n","\n","# right dataset\n","\n","X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n","pca = PCA(n_components=2, random_state=42).fit(X)\n","\n","axs[1].pie(pca.explained_variance_ratio_, labels=['Componente Principal 1', 'Componente Principal 2'], colors=['r', 'g'], autopct='%1.1f%%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2BGAqHkYw3ob"},"source":["Estos gráficos demuestran por qué el conjunto de datos de la izquierda no puede reducirse a una dimensión mientras que el de la derecha sí.\n","\n","La cantidad de varianza explicada por las componentes principales permite determinar el número de dimensiones a las que debe reducirse un conjunto de datos. Nuestro objetivo será seleccionar suficientes dimensiones de tal forma que éstas expliquen una varianza superior a un umbra (habitualmente el 90%). El número ideal de componentes principales podemos extraerlo fácilmente analizando la varianza acumulada por cada componente principal. En el ejemplo visto hasta ahora no tiene sentido, ya que el conjunto de datos parte de dos dimensiones, pero si aplicamos PCA al conjunto de datos de [`boston houses-prices`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston) que dispone de 13 dimensiones obtenemos la siguiente curva de varianza explicada:"]},{"cell_type":"code","metadata":{"id":"xYXEYqVUyP2R"},"source":["from sklearn.datasets import load_boston\n","X, y = load_boston(return_X_y=True)\n","\n","pca = PCA(n_components=13).fit(X)\n","\n","plt.figure(figsize=(6,4))\n","\n","xx = np.arange(1, 14, step=1)\n","yy = np.cumsum(pca.explained_variance_ratio_)\n","\n","plt.plot(xx, yy)\n","plt.xlabel('Número de componentes principales')\n","plt.ylabel('Varianza explicada')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0KNJuTFyv0A"},"source":["Observamos que, con únicamente 2 componentes principales, somos capaces de explicar más del 95% de la varianza."]},{"cell_type":"markdown","metadata":{"id":"5pEhKajRy9VF"},"source":["## Cálculo de las componentes principales\n","\n","Una vez entendido cómo funciona PCA, vamos a determinar cómo podemos calcular las componentes principales. Para ello vamos a hacer uso de la descomposición de valores sigulares, en inglés *Singular Value Decomposition (SVD)*. Supongamos que tenemos un conjunto de datos $M$ con $n$ características y $m$ muestras. SVD descompondrá la matriz $M$ de tal modo que $M = U \\Sigma V^t$:\n","\n","<img src=\"https://imgur.com/download/TEfmfuS/\">\n","\n","$\\Sigma$ contiene los valores singulares ($\\sigma_1, \\sigma_2, \\dots, \\sigma_m$) como elementos diagonales ordenados de mayor a menor ($\\sigma_1 > \\sigma_2 > \\dots > \\sigma_m$):\n","\n","$\\Sigma =\n"," \\begin{pmatrix}\n","  \\sigma_1 & 0        & \\cdots & 0        \\\\\n","  0        & \\sigma_2 & \\cdots & 0        \\\\\n","  \\vdots   & \\vdots   & \\ddots & \\vdots   \\\\\n","  0        & 0        & \\cdots & \\sigma_n \\\\\n","  \\vdots   & \\vdots   & \\vdots & \\vdots   \\\\\n","  0        & 0        & 0      & 0   \n"," \\end{pmatrix}$\n","\n","Las columnas de $U$ son los *vectores sigulares izquierdos* y las columnas de $V$ son los *vectores sigulares derechos*:\n","\n","$U =\n"," \\begin{pmatrix}\n","  \\uparrow   & \\uparrow   & \\cdots & \\uparrow   \\\\\n","  u_1        & u_2        & \\cdots & u_m        \\\\\n","  \\downarrow & \\downarrow & \\cdots & \\downarrow \n"," \\end{pmatrix}$\n","\n","$V =\n"," \\begin{pmatrix}\n","  \\uparrow   & \\uparrow   & \\cdots & \\uparrow   \\\\\n","  v_1        & v_2        & \\cdots & v_n        \\\\\n","  \\downarrow & \\downarrow & \\cdots & \\downarrow \n"," \\end{pmatrix}$\n","\n","Se verifica la ortogonalidad de los vectores singulares de $U$ y $V$, por lo que $U\\cdot U^t = I$ y $V \\cdot V^t = I$.\n","\n","Tras realiza la descomposición, [proceso que puede realizarse de manera analítica](https://es.wikipedia.org/wiki/Descomposici%C3%B3n_en_valores_singulares), las **columnas de la matriz $V$ se corresponden con las componentes principales** y **los cuadrados de los elementos diagonales de $\\Sigma$ contienen la varianza explicada por cada componente principal**.\n","\n","Es importante destacar que PCA requiere **estandarizar el conjunto de datos de entrada** como paso previo para su funcionamiento. Es decir, la columnas de la matriz $M$ deberá tener media 0 y desviación típica 1."]},{"cell_type":"markdown","metadata":{"id":"kie7dcFH6OjA"},"source":["---\n","\n","Creado por **Fernando Ortega** (fernando.ortega@upm.es)\n","\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"]}]}